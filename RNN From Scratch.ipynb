{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YzvIJS1EZMpG",
        "WszkdFnhZcl7",
        "pxG9-G8iZ8mN",
        "hxr1hkpqaQfP",
        "dx1-wyJKalIv",
        "T1IKRzwjOfbe",
        "_lDuXQZ5OYBW",
        "_wBnVXyVazwL",
        "qds-_KSjbOZ_",
        "XcCs90g5bx47",
        "AC53Db67b8e1",
        "GwjcbzX8VwDD",
        "W4AS6HwAXRG3",
        "rJ93egf4XcIU",
        "gBjzHN4_Yh9o",
        "c3cca74d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question#1 - Single Neuron**"
      ],
      "metadata": {
        "id": "YzvIJS1EZMpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Character Prediction in a Word"
      ],
      "metadata": {
        "id": "WszkdFnhZcl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-1: HELLO\n",
        "\n"
      ],
      "metadata": {
        "id": "pxG9-G8iZ8mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding**"
      ],
      "metadata": {
        "id": "dnM85tjb8Mz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"hello\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "encoded_word = [char_to_int[ch] for ch in word]\n",
        "\n",
        "# Print the encoded word\n",
        "print(\"Encoded Word using One-Hot Encoding:\", encoded_word, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F-GEKP6ZDKB",
        "outputId": "599430ac-a5af-41b6-e6fa-f9d4b6f6bc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using One-Hot Encoding: [1, 0, 2, 2, 3] \n",
            "\n",
            "Epoch 0, Loss: [5.5985341]\n",
            "Epoch 200, Loss: [3.73582889]\n",
            "Epoch 400, Loss: [0.73169815]\n",
            "Epoch 600, Loss: [0.60448336]\n",
            "Epoch 800, Loss: [0.46434777]\n",
            "Epoch 1000, Loss: [0.54941086]\n",
            "Epoch 1200, Loss: [0.34806325]\n",
            "Epoch 1400, Loss: [0.33578572]\n",
            "Epoch 1600, Loss: [0.35104384]\n",
            "Epoch 1800, Loss: [0.40441667]\n",
            "\n",
            "Predictions:\n",
            "Input: h, Predicted: e\n",
            "Input: e, Predicted: l\n",
            "Input: l, Predicted: o\n",
            "Input: l, Predicted: o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding**"
      ],
      "metadata": {
        "id": "s0ZXQduY8Rha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"hello\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Word Embedding: Random embedding initialization (size 2 for simplicity)\n",
        "embedding_size = 2\n",
        "embedding_matrix = np.random.randn(len(char_to_int), embedding_size)\n",
        "encoded_word = [embedding_matrix[char_to_int[ch]] for ch in word]\n",
        "\n",
        "print(\"Encoded Word using Word Embedding:\\n\", encoded_word, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, embedding_size) * 0.01  # Adjusted to match embedding size\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        x_t = np.array(encoded_word[t]).reshape(-1, 1)  # Convert embedding to column vector\n",
        "        y_true = char_to_int[word[t + 1]]  # Use index from the original word\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.array(encoded_word[t]).reshape(-1, 1)  # Convert embedding to column vector\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[char_to_int[word[t]]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2MilG_66z4m",
        "outputId": "8e55937b-ee7e-43ab-b927-e1917a908718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Word Embedding:\n",
            " [array([ 0.55979795, -1.04837189]), array([ 0.86685017, -1.48646659]), array([1.50208173, 0.48843163]), array([1.50208173, 0.48843163]), array([ 0.97422545, -0.29840624])] \n",
            "\n",
            "Epoch 0, Loss: [5.59830852]\n",
            "Epoch 200, Loss: [1.93608268]\n",
            "Epoch 400, Loss: [1.68127556]\n",
            "Epoch 600, Loss: [1.61010648]\n",
            "Epoch 800, Loss: [1.57747913]\n",
            "Epoch 1000, Loss: [1.5589571]\n",
            "Epoch 1200, Loss: [1.54708198]\n",
            "Epoch 1400, Loss: [1.5388437]\n",
            "Epoch 1600, Loss: [1.53280378]\n",
            "Epoch 1800, Loss: [1.52819099]\n",
            "\n",
            "Predictions:\n",
            "Input: h, Predicted: l\n",
            "Input: e, Predicted: l\n",
            "Input: l, Predicted: l\n",
            "Input: l, Predicted: o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "jyHyowauB2If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"hello\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Bag of Words Representation\n",
        "bag_of_words = np.zeros((len(char_to_int),), dtype=int)\n",
        "for ch in word:\n",
        "    bag_of_words[char_to_int[ch]] += 1\n",
        "\n",
        "print(\"Encoded Word using Bag of Words:\\n\", bag_of_words, \"\\n\")\n",
        "\n",
        "# Define Hyperparameters and Model Parameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Example: Input to RNN\n",
        "x_input = bag_of_words.reshape(-1, 1)  # The Bag of Words vector as input\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "h_next, y_pred = rnn_step_forward(x_input, h_prev)\n",
        "\n",
        "# Training loop (conceptual only for Bag of Words)\n",
        "# Bag of Words represents the entire word as a single input, so there's no sequential prediction\n",
        "print(\"RNN Output using Bag of Words Encoding:\\n\", y_pred)\n",
        "\n",
        "# Prediction\n",
        "predicted_index = np.argmax(y_pred)  # Choose the index with the highest probability\n",
        "predicted_char = int_to_char[predicted_index]\n",
        "print(f\"Predicted next character based on Bag of Words encoding: {predicted_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdls4fHZB4yD",
        "outputId": "cd884165-8d70-4918-bb94-d53a345405b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Bag of Words:\n",
            " [1 1 2 1] \n",
            "\n",
            "RNN Output using Bag of Words Encoding:\n",
            " [[ 6.43432841e-05]\n",
            " [-1.52126512e-04]\n",
            " [ 4.66170685e-05]\n",
            " [-1.86400995e-04]]\n",
            "Predicted next character based on Bag of Words encoding: e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words - Using Sliding Window**"
      ],
      "metadata": {
        "id": "HftRQqReDfNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"hello\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Bag of Words with Sliding Windows\n",
        "window_size = 2  # Define the size of the sliding window\n",
        "encoded_windows = []\n",
        "\n",
        "for i in range(len(word) - window_size + 1):\n",
        "    window = word[i : i + window_size]  # Extract a window\n",
        "    bag_of_words = np.zeros((len(char_to_int),), dtype=int)\n",
        "    for ch in window:\n",
        "        bag_of_words[char_to_int[ch]] += 1\n",
        "    encoded_windows.append(bag_of_words)\n",
        "\n",
        "print(\"Sliding Window Encoded Words (Bag of Words):\")\n",
        "for i, bow in enumerate(encoded_windows):\n",
        "    print(f\"Window {i + 1}: {bow}\")\n",
        "\n",
        "# Feed these sliding windows to an RNN\n",
        "hidden_size = 1\n",
        "Wx = np.random.randn(hidden_size, len(char_to_int)) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(len(char_to_int), hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((len(char_to_int), 1))\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Sequentially process the sliding windows\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "for t, bow in enumerate(encoded_windows):\n",
        "    x_t = bow.reshape(-1, 1)  # Reshape for RNN input\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Window {t + 1}: Predicted next character: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1XoxNMTDkKz",
        "outputId": "36395d90-c874-446f-b00e-c5c6d14508e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sliding Window Encoded Words (Bag of Words):\n",
            "Window 1: [1 1 0 0]\n",
            "Window 2: [1 0 1 0]\n",
            "Window 3: [0 0 2 0]\n",
            "Window 4: [0 0 1 1]\n",
            "Window 1: Predicted next character: l\n",
            "Window 2: Predicted next character: o\n",
            "Window 3: Predicted next character: o\n",
            "Window 4: Predicted next character: o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hashing Encoding**"
      ],
      "metadata": {
        "id": "Kd67HvoLEhpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "\n",
        "# Define the data\n",
        "word = \"hello\"\n",
        "\n",
        "# Define Hashing Function\n",
        "def hash_function(value, num_buckets):\n",
        "    hashed = int(hashlib.md5(value.encode()).hexdigest(), 16)\n",
        "    return hashed % num_buckets\n",
        "\n",
        "# Hashing Encoding (character-by-character for RNN compatibility)\n",
        "num_buckets = 5  # Define the number of buckets for hashing\n",
        "hash_vectors = []\n",
        "\n",
        "for ch in word:\n",
        "    hash_vector = np.zeros((num_buckets,), dtype=int)\n",
        "    bucket = hash_function(ch, num_buckets)\n",
        "    hash_vector[bucket] += 1\n",
        "    hash_vectors.append(hash_vector)\n",
        "\n",
        "print(\"Encoded Word using Hashing Encoding (character-by-character):\")\n",
        "for i, vec in enumerate(hash_vectors):\n",
        "    print(f\"Character '{word[i]}': {vec}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Define Hyperparameters and Model Parameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = num_buckets\n",
        "output_size = num_buckets\n",
        "learning_rate = 0.1\n",
        "\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Predict next characters using RNN\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"Predictions:\")\n",
        "\n",
        "for t in range(len(hash_vectors) - 1):  # Predict for all but the last character\n",
        "    x_t = hash_vectors[t].reshape(-1, 1)\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "\n",
        "    # Decode prediction (find the bucket with the highest value)\n",
        "    predicted_bucket = np.argmax(y_pred)\n",
        "    # Match the bucket back to a character\n",
        "    predicted_char = None\n",
        "    for ch in word:  # Iterate through word to find matching hash bucket\n",
        "        if hash_function(ch, num_buckets) == predicted_bucket:\n",
        "            predicted_char = ch\n",
        "            break\n",
        "\n",
        "    print(f\"Input: '{word[t]}', Predicted next character: '{predicted_char}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJmQ9iTDDn9C",
        "outputId": "c52d545d-2f32-4a1f-9ad1-99b869028341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Hashing Encoding (character-by-character):\n",
            "Character 'h': [0 0 0 1 0]\n",
            "Character 'e': [0 1 0 0 0]\n",
            "Character 'l': [0 0 1 0 0]\n",
            "Character 'l': [0 0 1 0 0]\n",
            "Character 'o': [0 0 0 1 0]\n",
            "\n",
            "\n",
            "Predictions:\n",
            "Input: 'h', Predicted next character: 'h'\n",
            "Input: 'e', Predicted next character: 'h'\n",
            "Input: 'l', Predicted next character: 'None'\n",
            "Input: 'l', Predicted next character: 'None'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-2: HAPPY\n",
        "\n"
      ],
      "metadata": {
        "id": "M4UAv7p_E1gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding**"
      ],
      "metadata": {
        "id": "ZRB_QF_8E1gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"happy\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "encoded_word = [char_to_int[ch] for ch in word]\n",
        "\n",
        "# Print the encoded word\n",
        "print(\"Encoded Word using One-Hot Encoding:\", encoded_word, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608895ab-0806-4c8c-e8db-eb39f4461faf",
        "id": "RkqhRxNHE1gc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using One-Hot Encoding: [1, 0, 2, 2, 3] \n",
            "\n",
            "Epoch 0, Loss: [5.59826591]\n",
            "Epoch 200, Loss: [1.11149072]\n",
            "Epoch 400, Loss: [0.64693971]\n",
            "Epoch 600, Loss: [1.24950606]\n",
            "Epoch 800, Loss: [1.7811296]\n",
            "Epoch 1000, Loss: [0.7302327]\n",
            "Epoch 1200, Loss: [0.37019844]\n",
            "Epoch 1400, Loss: [0.33125695]\n",
            "Epoch 1600, Loss: [0.28336462]\n",
            "Epoch 1800, Loss: [0.31947961]\n",
            "\n",
            "Predictions:\n",
            "Input: h, Predicted: a\n",
            "Input: a, Predicted: p\n",
            "Input: p, Predicted: p\n",
            "Input: p, Predicted: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding**"
      ],
      "metadata": {
        "id": "Xh1SPbsLE1gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"happy\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Word Embedding: Random embedding initialization (size 2 for simplicity)\n",
        "embedding_size = 2\n",
        "embedding_matrix = np.random.randn(len(char_to_int), embedding_size)\n",
        "encoded_word = [embedding_matrix[char_to_int[ch]] for ch in word]\n",
        "\n",
        "print(\"Encoded Word using Word Embedding:\\n\", encoded_word, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, embedding_size) * 0.01  # Adjusted to match embedding size\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        x_t = np.array(encoded_word[t]).reshape(-1, 1)  # Convert embedding to column vector\n",
        "        y_true = char_to_int[word[t + 1]]  # Use index from the original word\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.array(encoded_word[t]).reshape(-1, 1)  # Convert embedding to column vector\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[char_to_int[word[t]]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856e73eb-3f80-4115-91aa-45ef7ecdcffa",
        "id": "5djRT3kHE1gd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Word Embedding:\n",
            " [array([0.0016641 , 1.47165403]), array([0.3374475 , 0.42373716]), array([0.09433769, 1.38656867]), array([0.09433769, 1.38656867]), array([-1.18585439,  0.08112651])] \n",
            "\n",
            "Epoch 0, Loss: [5.59840657]\n",
            "Epoch 200, Loss: [3.77824674]\n",
            "Epoch 400, Loss: [2.16512331]\n",
            "Epoch 600, Loss: [5.06827428]\n",
            "Epoch 800, Loss: [2.95859308]\n",
            "Epoch 1000, Loss: [1.98271182]\n",
            "Epoch 1200, Loss: [2.14352732]\n",
            "Epoch 1400, Loss: [2.03112976]\n",
            "Epoch 1600, Loss: [2.2042331]\n",
            "Epoch 1800, Loss: [2.16471457]\n",
            "\n",
            "Predictions:\n",
            "Input: h, Predicted: a\n",
            "Input: a, Predicted: p\n",
            "Input: p, Predicted: p\n",
            "Input: p, Predicted: p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "voFsehwtE1ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"happy\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Bag of Words Representation\n",
        "bag_of_words = np.zeros((len(char_to_int),), dtype=int)\n",
        "for ch in word:\n",
        "    bag_of_words[char_to_int[ch]] += 1\n",
        "\n",
        "print(\"Encoded Word using Bag of Words:\\n\", bag_of_words, \"\\n\")\n",
        "\n",
        "# Define Hyperparameters and Model Parameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Example: Input to RNN\n",
        "x_input = bag_of_words.reshape(-1, 1)  # The Bag of Words vector as input\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "h_next, y_pred = rnn_step_forward(x_input, h_prev)\n",
        "\n",
        "# Training loop (conceptual only for Bag of Words)\n",
        "# Bag of Words represents the entire word as a single input, so there's no sequential prediction\n",
        "print(\"RNN Output using Bag of Words Encoding:\\n\", y_pred)\n",
        "\n",
        "# Prediction\n",
        "predicted_index = np.argmax(y_pred)  # Choose the index with the highest probability\n",
        "predicted_char = int_to_char[predicted_index]\n",
        "print(f\"Predicted next character based on Bag of Words encoding: {predicted_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ae7bd4-5faf-4cd3-a138-fd7b26833f53",
        "id": "I10md_ggE1ge"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Bag of Words:\n",
            " [1 1 2 1] \n",
            "\n",
            "RNN Output using Bag of Words Encoding:\n",
            " [[ 2.35353406e-04]\n",
            " [ 7.45161395e-05]\n",
            " [-8.00167329e-05]\n",
            " [-1.27538369e-04]]\n",
            "Predicted next character based on Bag of Words encoding: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words - Using Sliding Window**"
      ],
      "metadata": {
        "id": "HHvISh0OE1gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"happy\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Bag of Words with Sliding Windows\n",
        "window_size = 2  # Define the size of the sliding window\n",
        "encoded_windows = []\n",
        "\n",
        "for i in range(len(word) - window_size + 1):\n",
        "    window = word[i : i + window_size]  # Extract a window\n",
        "    bag_of_words = np.zeros((len(char_to_int),), dtype=int)\n",
        "    for ch in window:\n",
        "        bag_of_words[char_to_int[ch]] += 1\n",
        "    encoded_windows.append(bag_of_words)\n",
        "\n",
        "print(\"Sliding Window Encoded Words (Bag of Words):\")\n",
        "for i, bow in enumerate(encoded_windows):\n",
        "    print(f\"Window {i + 1}: {bow}\")\n",
        "\n",
        "# Feed these sliding windows to an RNN\n",
        "hidden_size = 1\n",
        "Wx = np.random.randn(hidden_size, len(char_to_int)) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(len(char_to_int), hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((len(char_to_int), 1))\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Sequentially process the sliding windows\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "for t, bow in enumerate(encoded_windows):\n",
        "    x_t = bow.reshape(-1, 1)  # Reshape for RNN input\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Window {t + 1}: Predicted next character: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11128cad-5cd2-44f2-908e-deb1e03b4301",
        "id": "zkiT2OMYE1gg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sliding Window Encoded Words (Bag of Words):\n",
            "Window 1: [1 1 0 0]\n",
            "Window 2: [1 0 1 0]\n",
            "Window 3: [0 0 2 0]\n",
            "Window 4: [0 0 1 1]\n",
            "Window 1: Predicted next character: p\n",
            "Window 2: Predicted next character: p\n",
            "Window 3: Predicted next character: p\n",
            "Window 4: Predicted next character: p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hashing Encoding**"
      ],
      "metadata": {
        "id": "sisf6jMGE1gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "\n",
        "# Define the data\n",
        "word = \"happy\"\n",
        "\n",
        "# Define Hashing Function\n",
        "def hash_function(value, num_buckets):\n",
        "    hashed = int(hashlib.md5(value.encode()).hexdigest(), 16)\n",
        "    return hashed % num_buckets\n",
        "\n",
        "# Hashing Encoding (character-by-character for RNN compatibility)\n",
        "num_buckets = 5  # Define the number of buckets for hashing\n",
        "hash_vectors = []\n",
        "\n",
        "for ch in word:\n",
        "    hash_vector = np.zeros((num_buckets,), dtype=int)\n",
        "    bucket = hash_function(ch, num_buckets)\n",
        "    hash_vector[bucket] += 1\n",
        "    hash_vectors.append(hash_vector)\n",
        "\n",
        "print(\"Encoded Word using Hashing Encoding (character-by-character):\")\n",
        "for i, vec in enumerate(hash_vectors):\n",
        "    print(f\"Character '{word[i]}': {vec}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Define Hyperparameters and Model Parameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = num_buckets\n",
        "output_size = num_buckets\n",
        "learning_rate = 0.1\n",
        "\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# RNN function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Predict next characters using RNN\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"Predictions:\")\n",
        "\n",
        "for t in range(len(hash_vectors) - 1):  # Predict for all but the last character\n",
        "    x_t = hash_vectors[t].reshape(-1, 1)\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "\n",
        "    # Decode prediction (find the bucket with the highest value)\n",
        "    predicted_bucket = np.argmax(y_pred)\n",
        "    # Match the bucket back to a character\n",
        "    predicted_char = None\n",
        "    for ch in word:  # Iterate through word to find matching hash bucket\n",
        "        if hash_function(ch, num_buckets) == predicted_bucket:\n",
        "            predicted_char = ch\n",
        "            break\n",
        "\n",
        "    print(f\"Input: '{word[t]}', Predicted next character: '{predicted_char}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cacbed7-048e-4494-ffb1-1f4b7d4cf894",
        "id": "o66i8GdWE1gg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Hashing Encoding (character-by-character):\n",
            "Character 'h': [0 0 0 1 0]\n",
            "Character 'a': [0 0 1 0 0]\n",
            "Character 'p': [0 1 0 0 0]\n",
            "Character 'p': [0 1 0 0 0]\n",
            "Character 'y': [0 0 0 0 1]\n",
            "\n",
            "\n",
            "Predictions:\n",
            "Input: 'h', Predicted next character: 'a'\n",
            "Input: 'a', Predicted next character: 'h'\n",
            "Input: 'p', Predicted next character: 'h'\n",
            "Input: 'p', Predicted next character: 'h'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Word Prediction in a Sentence"
      ],
      "metadata": {
        "id": "Ne-fcw8uZkx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-1 : I Love Coding"
      ],
      "metadata": {
        "id": "hxr1hkpqaQfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "pgMwD1rJIqGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"I love coding\"\n",
        "words = sentence.split()\n",
        "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "# Display tokens, vocabulary, and encodings\n",
        "print(\"### One-Hot Encoding ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
        "print(\"Index to Word Mapping:\", int_to_word)\n",
        "print(\"Encoded Sentence:\", encoded_sentence , \"\\n\")\n",
        "\n",
        "\n",
        "# Model parameters\n",
        "input_size = len(word_to_int)  # Number of unique words\n",
        "output_size = len(word_to_int)\n",
        "hidden_size = 1  # Single neuron\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize weights and biases\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN forward step\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "\n",
        "    for t in range(len(encoded_sentence) - 1):\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_sentence[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass (gradient calculation and parameter update)\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next words\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_sentence) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_word = int_to_word[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9a-6N30ZUfW",
        "outputId": "86cea3dc-1bc7-4260-a113-d5bb7fcacff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### One-Hot Encoding ###\n",
            "Sentence: I love coding\n",
            "Tokens (Words): ['I', 'love', 'coding']\n",
            "Vocabulary (Word to Index): {'I': 0, 'coding': 1, 'love': 2}\n",
            "Index to Word Mapping: {0: 'I', 1: 'coding', 2: 'love'}\n",
            "Encoded Sentence: [0, 2, 1] \n",
            "\n",
            "Epoch 0, Loss: [2.23168791]\n",
            "Epoch 200, Loss: [0.05924359]\n",
            "Epoch 400, Loss: [0.02322638]\n",
            "Epoch 600, Loss: [0.01433453]\n",
            "Epoch 800, Loss: [0.0103422]\n",
            "Epoch 1000, Loss: [0.00808059]\n",
            "Epoch 1200, Loss: [0.00662659]\n",
            "Epoch 1400, Loss: [0.00561388]\n",
            "Epoch 1600, Loss: [0.00486837]\n",
            "Epoch 1800, Loss: [0.00429683]\n",
            "\n",
            "Predictions:\n",
            "Input: I, Predicted: love\n",
            "Input: love, Predicted: coding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding**"
      ],
      "metadata": {
        "id": "KnOZMHXPIt3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"I love coding\"\n",
        "words = sentence.split()\n",
        "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "# Display tokens, vocabulary, and encodings\n",
        "print(\"### Word Embedding ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
        "print(\"Index to Word Mapping:\", int_to_word)\n",
        "print(\"Encoded Sentence:\", encoded_sentence, \"\\n\")\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(word_to_int)  # Number of unique words\n",
        "embedding_dim = 3  # Size of word embeddings\n",
        "hidden_size = 1  # Single neuron\n",
        "output_size = vocab_size\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize weights and biases\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01  # Word embeddings\n",
        "Wx = np.random.randn(hidden_size, embedding_dim) * 0.01  # Embedding to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN forward step\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "\n",
        "    for t in range(len(encoded_sentence) - 1):\n",
        "        word_idx = encoded_sentence[t]\n",
        "        x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
        "        y_true = encoded_sentence[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass (gradient calculation and parameter update)\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next words\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_sentence) - 1):\n",
        "    word_idx = encoded_sentence[t]\n",
        "    x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_word = int_to_word[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EpNA3epI1CX",
        "outputId": "5b6898a9-ef6a-4421-b77e-73b022885c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Word Embedding ###\n",
            "Sentence: I love coding\n",
            "Tokens (Words): ['I', 'love', 'coding']\n",
            "Vocabulary (Word to Index): {'I': 0, 'coding': 1, 'love': 2}\n",
            "Index to Word Mapping: {0: 'I', 1: 'coding', 2: 'love'}\n",
            "Encoded Sentence: [0, 2, 1] \n",
            "\n",
            "Epoch 0, Loss: [2.23167933]\n",
            "Epoch 200, Loss: [1.47191492]\n",
            "Epoch 400, Loss: [1.45512751]\n",
            "Epoch 600, Loss: [1.44949047]\n",
            "Epoch 800, Loss: [1.44666605]\n",
            "Epoch 1000, Loss: [1.44497151]\n",
            "Epoch 1200, Loss: [1.4438428]\n",
            "Epoch 1400, Loss: [1.44303742]\n",
            "Epoch 1600, Loss: [1.44243398]\n",
            "Epoch 1800, Loss: [1.44196505]\n",
            "\n",
            "Predictions:\n",
            "Input: I, Predicted: coding\n",
            "Input: love, Predicted: coding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "6cr4JwB2J3RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"I love coding\"\n",
        "words = sentence.split()\n",
        "vocab = sorted(set(words))\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "\n",
        "# Bag of Words Encoding\n",
        "bow_vector = np.zeros(len(vocab), dtype=int)\n",
        "for word in words:\n",
        "    bow_vector[word_to_int[word]] += 1\n",
        "\n",
        "# Display tokens, vocabulary, and encoding\n",
        "print(\"### Bag of Words (BoW) ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
        "print(\"Index to Word Mapping:\", int_to_word)\n",
        "print(\"BoW Vector (Encoded Sentence):\", bow_vector)\n",
        "\n",
        "# Example: Use BoW encoding as input to a model\n",
        "input_vector = bow_vector.reshape(-1, 1)  # Reshape for compatibility\n",
        "hidden_size = 2  # Number of neurons\n",
        "output_size = len(vocab)\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize model parameters\n",
        "Wx = np.random.randn(hidden_size, len(vocab)) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Simple forward pass\n",
        "hidden_layer = np.tanh(np.dot(Wx, input_vector) + bh)\n",
        "output_layer = np.dot(Wy, hidden_layer) + by\n",
        "\n",
        "# Apply Softmax to output_layer for predictions\n",
        "output_probs = np.exp(output_layer) / np.sum(np.exp(output_layer))  # Softmax\n",
        "predicted_word_idx = np.argmax(output_probs)  # Get the index of the predicted word\n",
        "predicted_word = int_to_word[predicted_word_idx]\n",
        "\n",
        "print(\"\\nModel Output (Raw Scores):\\n\", output_layer)\n",
        "print(\"\\nSoftmax Probabilities (Predictions):\", output_probs.flatten())\n",
        "print(\"Predicted Word:\", predicted_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igAfL99RJ5yZ",
        "outputId": "9e4d74b9-76ed-4699-863f-2c363b2a4720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Bag of Words (BoW) ###\n",
            "Sentence: I love coding\n",
            "Tokens (Words): ['I', 'love', 'coding']\n",
            "Vocabulary (Word to Index): {'I': 0, 'coding': 1, 'love': 2}\n",
            "Index to Word Mapping: {0: 'I', 1: 'coding', 2: 'love'}\n",
            "BoW Vector (Encoded Sentence): [1 1 1]\n",
            "\n",
            "Model Output (Raw Scores):\n",
            " [[-2.20668581e-05]\n",
            " [ 8.27444754e-05]\n",
            " [-2.39934914e-04]]\n",
            "\n",
            "Softmax Probabilities (Predictions): [0.33334589 0.33338083 0.33327327]\n",
            "Predicted Word: coding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hashing Encoding**"
      ],
      "metadata": {
        "id": "YlQA61C_KBD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "\n",
        "# Define the data\n",
        "sentence = \"I love coding\"\n",
        "words = sentence.split()\n",
        "\n",
        "# Hashing function\n",
        "def hash_function(value, num_buckets):\n",
        "    hashed = int(hashlib.md5(value.encode()).hexdigest(), 16)\n",
        "    return hashed % num_buckets\n",
        "\n",
        "# Hashing Encoding\n",
        "num_buckets = 3 # Define the number of buckets\n",
        "hash_vector = np.zeros(num_buckets, dtype=int)\n",
        "for word in words:\n",
        "    bucket = hash_function(word, num_buckets)\n",
        "    hash_vector[bucket] += 1\n",
        "\n",
        "# Display tokens and hashed encoding\n",
        "print(\"\\n### Hashing Encoding ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Number of Buckets:\", num_buckets)\n",
        "print(\"Hash Vector (Encoded Sentence):\", hash_vector)\n",
        "\n",
        "# Example: Use Hashing encoding as input to a model\n",
        "input_vector = hash_vector.reshape(-1, 1)  # Reshape for compatibility\n",
        "hidden_size = 2  # Number of neurons\n",
        "output_size = num_buckets\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize model parameters\n",
        "Wx = np.random.randn(hidden_size, num_buckets) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Simple forward pass\n",
        "hidden_layer = np.tanh(np.dot(Wx, input_vector) + bh)\n",
        "output_layer = np.dot(Wy, hidden_layer) + by\n",
        "\n",
        "# Apply Softmax to output_layer for predictions\n",
        "output_probs = np.exp(output_layer) / np.sum(np.exp(output_layer))  # Softmax\n",
        "predicted_bucket = np.argmax(output_probs)  # Get the predicted bucket index\n",
        "\n",
        "print(\"\\nModel Output (Raw Scores):\\n\", output_layer)\n",
        "print(\"\\nSoftmax Probabilities (Predictions):\", output_probs.flatten())\n",
        "print(\"Predicted Bucket:\", predicted_bucket)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ASX031KFEk",
        "outputId": "a321fa6e-10a9-41c8-c244-a636639e4644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Hashing Encoding ###\n",
            "Sentence: I love coding\n",
            "Tokens (Words): ['I', 'love', 'coding']\n",
            "Number of Buckets: 3\n",
            "Hash Vector (Encoded Sentence): [2 1 0]\n",
            "\n",
            "Model Output (Raw Scores):\n",
            " [[ 0.00012358]\n",
            " [-0.00075279]\n",
            " [ 0.00033973]]\n",
            "\n",
            "Softmax Probabilities (Predictions): [0.33340666 0.3331146  0.33347873]\n",
            "Predicted Bucket: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-2 : Our national language is Urdu"
      ],
      "metadata": {
        "id": "dx1-wyJKalIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "pzfkJfZ_LPEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"Our national language is Urdu\"\n",
        "words = sentence.split()\n",
        "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "# Display tokens, vocabulary, and encodings\n",
        "print(\"### One-Hot Encoding ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
        "print(\"Index to Word Mapping:\", int_to_word)\n",
        "print(\"Encoded Sentence:\", encoded_sentence , \"\\n\")\n",
        "\n",
        "\n",
        "# Model parameters\n",
        "input_size = len(word_to_int)  # Number of unique words\n",
        "output_size = len(word_to_int)\n",
        "hidden_size = 1  # Single neuron\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize weights and biases\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN forward step\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "\n",
        "    for t in range(len(encoded_sentence) - 1):\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_sentence[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass (gradient calculation and parameter update)\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next words\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_sentence) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_word = int_to_word[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26c7c22-caad-496f-a7d8-19f9ae1d6189",
        "id": "47XkgRBfLPEX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### One-Hot Encoding ###\n",
            "Sentence: Our national language is Urdu\n",
            "Tokens (Words): ['Our', 'national', 'language', 'is', 'Urdu']\n",
            "Vocabulary (Word to Index): {'Our': 0, 'Urdu': 1, 'is': 2, 'language': 3, 'national': 4}\n",
            "Index to Word Mapping: {0: 'Our', 1: 'Urdu', 2: 'is', 3: 'language', 4: 'national'}\n",
            "Encoded Sentence: [0, 4, 3, 2, 1] \n",
            "\n",
            "Epoch 0, Loss: [6.55916618]\n",
            "Epoch 200, Loss: [2.25840477]\n",
            "Epoch 400, Loss: [1.85519033]\n",
            "Epoch 600, Loss: [1.72324284]\n",
            "Epoch 800, Loss: [1.65770307]\n",
            "Epoch 1000, Loss: [1.60609088]\n",
            "Epoch 1200, Loss: [1.05597886]\n",
            "Epoch 1400, Loss: [0.64920434]\n",
            "Epoch 1600, Loss: [0.51583237]\n",
            "Epoch 1800, Loss: [0.43260939]\n",
            "\n",
            "Predictions:\n",
            "Input: Our, Predicted: national\n",
            "Input: national, Predicted: language\n",
            "Input: language, Predicted: is\n",
            "Input: is, Predicted: Urdu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding**"
      ],
      "metadata": {
        "id": "ZIw_nuOyLPEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"Our national language is Urdu\"\n",
        "words = sentence.split()\n",
        "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "# Display tokens, vocabulary, and encodings\n",
        "print(\"### Word Embedding ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
        "print(\"Index to Word Mapping:\", int_to_word)\n",
        "print(\"Encoded Sentence:\", encoded_sentence, \"\\n\")\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(word_to_int)  # Number of unique words\n",
        "embedding_dim = 3  # Size of word embeddings\n",
        "hidden_size = 1  # Single neuron\n",
        "output_size = vocab_size\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize weights and biases\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01  # Word embeddings\n",
        "Wx = np.random.randn(hidden_size, embedding_dim) * 0.01  # Embedding to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN forward step\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden neuron\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):  # Training for 2000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "\n",
        "    for t in range(len(encoded_sentence) - 1):\n",
        "        word_idx = encoded_sentence[t]\n",
        "        x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
        "        y_true = encoded_sentence[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass (gradient calculation and parameter update)\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next words\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_sentence) - 1):\n",
        "    word_idx = encoded_sentence[t]\n",
        "    x_t = embedding_matrix[word_idx].reshape(-1, 1)  # Word embedding vector\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_word = int_to_word[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3765ae5d-9234-4c56-9bb6-604e9271049e",
        "id": "HGErBFbtLPEY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Word Embedding ###\n",
            "Sentence: Our national language is Urdu\n",
            "Tokens (Words): ['Our', 'national', 'language', 'is', 'Urdu']\n",
            "Vocabulary (Word to Index): {'Our': 0, 'Urdu': 1, 'is': 2, 'language': 3, 'national': 4}\n",
            "Index to Word Mapping: {0: 'Our', 1: 'Urdu', 2: 'is', 3: 'language', 4: 'national'}\n",
            "Encoded Sentence: [0, 4, 3, 2, 1] \n",
            "\n",
            "Epoch 0, Loss: [6.55928099]\n",
            "Epoch 200, Loss: [5.73676323]\n",
            "Epoch 400, Loss: [5.71727751]\n",
            "Epoch 600, Loss: [5.7106698]\n",
            "Epoch 800, Loss: [5.70734927]\n",
            "Epoch 1000, Loss: [5.70535234]\n",
            "Epoch 1200, Loss: [5.70401925]\n",
            "Epoch 1400, Loss: [5.70306602]\n",
            "Epoch 1600, Loss: [5.70235022]\n",
            "Epoch 1800, Loss: [5.70179248]\n",
            "\n",
            "Predictions:\n",
            "Input: Our, Predicted: Urdu\n",
            "Input: national, Predicted: Urdu\n",
            "Input: language, Predicted: Urdu\n",
            "Input: is, Predicted: Urdu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "2rMFAndlLPEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"Our national language is Urdu\"\n",
        "words = sentence.split()\n",
        "vocab = sorted(set(words))\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "\n",
        "# Bag of Words Encoding\n",
        "bow_vector = np.zeros(len(vocab), dtype=int)\n",
        "for word in words:\n",
        "    bow_vector[word_to_int[word]] += 1\n",
        "\n",
        "# Display tokens, vocabulary, and encoding\n",
        "print(\"### Bag of Words (BoW) ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Vocabulary (Word to Index):\", word_to_int)\n",
        "print(\"Index to Word Mapping:\", int_to_word)\n",
        "print(\"BoW Vector (Encoded Sentence):\", bow_vector)\n",
        "\n",
        "# Example: Use BoW encoding as input to a model\n",
        "input_vector = bow_vector.reshape(-1, 1)  # Reshape for compatibility\n",
        "hidden_size = 2  # Number of neurons\n",
        "output_size = len(vocab)\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize model parameters\n",
        "Wx = np.random.randn(hidden_size, len(vocab)) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Simple forward pass\n",
        "hidden_layer = np.tanh(np.dot(Wx, input_vector) + bh)\n",
        "output_layer = np.dot(Wy, hidden_layer) + by\n",
        "\n",
        "# Apply Softmax to output_layer for predictions\n",
        "output_probs = np.exp(output_layer) / np.sum(np.exp(output_layer))  # Softmax\n",
        "predicted_word_idx = np.argmax(output_probs)  # Get the index of the predicted word\n",
        "predicted_word = int_to_word[predicted_word_idx]\n",
        "\n",
        "print(\"\\nModel Output (Raw Scores):\\n\", output_layer)\n",
        "print(\"\\nSoftmax Probabilities (Predictions):\", output_probs.flatten())\n",
        "print(\"Predicted Word:\", predicted_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00496d0-a676-468b-e725-34cd838f174d",
        "id": "eEBFdSvwLPEZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Bag of Words (BoW) ###\n",
            "Sentence: Our national language is Urdu\n",
            "Tokens (Words): ['Our', 'national', 'language', 'is', 'Urdu']\n",
            "Vocabulary (Word to Index): {'Our': 0, 'Urdu': 1, 'is': 2, 'language': 3, 'national': 4}\n",
            "Index to Word Mapping: {0: 'Our', 1: 'Urdu', 2: 'is', 3: 'language', 4: 'national'}\n",
            "BoW Vector (Encoded Sentence): [1 1 1 1 1]\n",
            "\n",
            "Model Output (Raw Scores):\n",
            " [[-0.00016566]\n",
            " [ 0.00015024]\n",
            " [ 0.00017704]\n",
            " [ 0.00039723]\n",
            " [ 0.00045202]]\n",
            "\n",
            "Softmax Probabilities (Predictions): [0.19992644 0.19998961 0.19999497 0.20003901 0.20004997]\n",
            "Predicted Word: national\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hashing Encoding**"
      ],
      "metadata": {
        "id": "4ILk8-jGLPEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "\n",
        "# Define the data\n",
        "sentence = \"Our national language is Urdu\"\n",
        "words = sentence.split()\n",
        "\n",
        "# Hashing function\n",
        "def hash_function(value, num_buckets):\n",
        "    hashed = int(hashlib.md5(value.encode()).hexdigest(), 16)\n",
        "    return hashed % num_buckets\n",
        "\n",
        "# Hashing Encoding\n",
        "num_buckets = 5 # Define the number of buckets\n",
        "hash_vector = np.zeros(num_buckets, dtype=int)\n",
        "for word in words:\n",
        "    bucket = hash_function(word, num_buckets)\n",
        "    hash_vector[bucket] += 1\n",
        "\n",
        "# Display tokens and hashed encoding\n",
        "print(\"\\n### Hashing Encoding ###\")\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens (Words):\", words)\n",
        "print(\"Number of Buckets:\", num_buckets)\n",
        "print(\"Hash Vector (Encoded Sentence):\", hash_vector)\n",
        "\n",
        "# Example: Use Hashing encoding as input to a model\n",
        "input_vector = hash_vector.reshape(-1, 1)  # Reshape for compatibility\n",
        "hidden_size = 2  # Number of neurons\n",
        "output_size = num_buckets\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize model parameters\n",
        "Wx = np.random.randn(hidden_size, num_buckets) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Simple forward pass\n",
        "hidden_layer = np.tanh(np.dot(Wx, input_vector) + bh)\n",
        "output_layer = np.dot(Wy, hidden_layer) + by\n",
        "\n",
        "# Apply Softmax to output_layer for predictions\n",
        "output_probs = np.exp(output_layer) / np.sum(np.exp(output_layer))  # Softmax\n",
        "predicted_bucket = np.argmax(output_probs)  # Get the predicted bucket index\n",
        "\n",
        "print(\"\\nModel Output (Raw Scores):\\n\", output_layer)\n",
        "print(\"\\nSoftmax Probabilities (Predictions):\", output_probs.flatten())\n",
        "print(\"Predicted Bucket:\", predicted_bucket)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b97f6c0-3cfd-450b-a373-53172594f14a",
        "id": "7TWMaV2rLPEZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Hashing Encoding ###\n",
            "Sentence: Our national language is Urdu\n",
            "Tokens (Words): ['Our', 'national', 'language', 'is', 'Urdu']\n",
            "Number of Buckets: 5\n",
            "Hash Vector (Encoded Sentence): [0 1 2 0 2]\n",
            "\n",
            "Model Output (Raw Scores):\n",
            " [[-0.00040377]\n",
            " [ 0.00029077]\n",
            " [ 0.00037115]\n",
            " [-0.00023484]\n",
            " [ 0.0002637 ]]\n",
            "\n",
            "Softmax Probabilities (Predictions): [0.19990778 0.20004667 0.20006275 0.19994155 0.20004125]\n",
            "Predicted Bucket: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question#2 - Multiple Neurons**"
      ],
      "metadata": {
        "id": "T1IKRzwjOfbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Character Prediction in a Word\n",
        "\n"
      ],
      "metadata": {
        "id": "_lDuXQZ5OYBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-1 : TELEPHONE"
      ],
      "metadata": {
        "id": "_wBnVXyVazwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "LKwk4Eh9Mz2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipPwC1frJKfc",
        "outputId": "9a256a02-f93c-427b-a073-bc1985bc9b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using One-Hot Encoding: [6, 0, 2, 0, 5, 1, 4, 3, 0] \n",
            "\n",
            "Epoch 0, Loss: [1.72974144]\n",
            "Epoch 200, Loss: [0.01802542]\n",
            "Epoch 400, Loss: [0.00700745]\n",
            "Epoch 600, Loss: [0.00446998]\n",
            "Epoch 800, Loss: [0.00544615]\n",
            "Epoch 1000, Loss: [0.00342847]\n",
            "Epoch 1200, Loss: [0.00260297]\n",
            "Epoch 1400, Loss: [0.00211741]\n",
            "Epoch 1600, Loss: [0.00178581]\n",
            "Epoch 1800, Loss: [0.00154262]\n",
            "\n",
            "Predictions:\n",
            "Input: t, Predicted: e\n",
            "Input: e, Predicted: l\n",
            "Input: l, Predicted: e\n",
            "Input: e, Predicted: p\n",
            "Input: p, Predicted: h\n",
            "Input: h, Predicted: o\n",
            "Input: o, Predicted: n\n",
            "Input: n, Predicted: e\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"telephone\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "encoded_word = [char_to_int[ch] for ch in word]\n",
        "\n",
        "print(\"Encoded Word using One-Hot Encoding:\", encoded_word, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(encoded_word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding**"
      ],
      "metadata": {
        "id": "oX5ehZx6N_u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"telephone\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "# Embedding parameters\n",
        "embedding_dim = 4  # Dimensionality of the embeddings\n",
        "vocab_size = len(char_to_int)\n",
        "\n",
        "# Randomly initialized embedding matrix\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim)\n",
        "\n",
        "# Encode the word using embeddings\n",
        "embedded_word = [embedding_matrix[char_to_int[ch]] for ch in word]\n",
        "print(\"Encoded Word using Word Embedding:\")\n",
        "for ch, emb in zip(word, embedded_word):\n",
        "    print(f\"Character: {ch}, Embedding: {emb}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(encoded_word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOGzTOXgNiiX",
        "outputId": "e95facc0-9868-498e-a7fe-9831076f7719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Word Embedding:\n",
            "Character: t, Embedding: [-1.2454758  -0.85674954 -0.68525055  1.0762732 ]\n",
            "Character: e, Embedding: [-0.76680977 -0.38920485 -0.10529827 -0.23005368]\n",
            "Character: l, Embedding: [-0.30054043 -0.19943389  0.07753008 -2.25822463]\n",
            "Character: e, Embedding: [-0.76680977 -0.38920485 -0.10529827 -0.23005368]\n",
            "Character: p, Embedding: [-0.89982608 -0.48267382 -1.76788283  1.14888282]\n",
            "Character: h, Embedding: [-0.00956843 -0.21126562 -0.21673139 -0.1999813 ]\n",
            "Character: o, Embedding: [0.53656774 0.59248285 0.94343379 0.55614605]\n",
            "Character: n, Embedding: [ 1.69375973  0.57413375 -1.74953246  0.25221324]\n",
            "Character: e, Embedding: [-0.76680977 -0.38920485 -0.10529827 -0.23005368]\n",
            "\n",
            "\n",
            "Epoch 0, Loss: [1.72964785]\n",
            "Epoch 200, Loss: [0.01956246]\n",
            "Epoch 400, Loss: [0.00809698]\n",
            "Epoch 600, Loss: [0.00497693]\n",
            "Epoch 800, Loss: [0.00356651]\n",
            "Epoch 1000, Loss: [0.00319341]\n",
            "Epoch 1200, Loss: [0.02740825]\n",
            "Epoch 1400, Loss: [0.00843575]\n",
            "Epoch 1600, Loss: [0.0050867]\n",
            "Epoch 1800, Loss: [0.00364764]\n",
            "\n",
            "Predictions:\n",
            "Input: t, Predicted: e\n",
            "Input: e, Predicted: l\n",
            "Input: l, Predicted: e\n",
            "Input: e, Predicted: p\n",
            "Input: p, Predicted: h\n",
            "Input: h, Predicted: o\n",
            "Input: o, Predicted: n\n",
            "Input: n, Predicted: e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "4fD5cxhpNXW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Define the data\n",
        "word = \"telephone\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "\n",
        "# Count character occurrences\n",
        "char_counts = Counter(word)\n",
        "\n",
        "# Encode word using BoW\n",
        "bow_vector = np.zeros(len(char_to_int))  # Correct initialization\n",
        "for ch, count in char_counts.items():\n",
        "    bow_vector[char_to_int[ch]] = count\n",
        "\n",
        "print(\"\\nEncoded Word using Bag of Words (BoW):\")\n",
        "print(f\"Characters: {sorted(set(word))}\")\n",
        "print(f\"BoW Vector: {bow_vector}\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Convert BoW vector to sequences for training\n",
        "# For simplicity, assume the BoW vector is used as input at every time step\n",
        "encoded_word = bow_vector.astype(int)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(encoded_word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQP4W4nUOLP7",
        "outputId": "aa38e82a-55b1-4d15-c458-460ff82c6712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded Word using Bag of Words (BoW):\n",
            "Characters: ['e', 'h', 'l', 'n', 'o', 'p', 't']\n",
            "BoW Vector: [3. 1. 1. 1. 1. 1. 1.]\n",
            "Epoch 0, Loss: [1.66810609]\n",
            "Epoch 200, Loss: [0.00115437]\n",
            "Epoch 400, Loss: [0.00051743]\n",
            "Epoch 600, Loss: [0.00032838]\n",
            "Epoch 800, Loss: [0.00023892]\n",
            "Epoch 1000, Loss: [0.00018708]\n",
            "Epoch 1200, Loss: [0.00015338]\n",
            "Epoch 1400, Loss: [0.00012977]\n",
            "Epoch 1600, Loss: [0.00011234]\n",
            "Epoch 1800, Loss: [9.89572919e-05]\n",
            "\n",
            "Predictions:\n",
            "Input: n, Predicted: h\n",
            "Input: h, Predicted: h\n",
            "Input: h, Predicted: h\n",
            "Input: h, Predicted: h\n",
            "Input: h, Predicted: h\n",
            "Input: h, Predicted: h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hashing Embedding**"
      ],
      "metadata": {
        "id": "41PyqbLCPApk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "\n",
        "# Define the data\n",
        "word = \"telephone\"\n",
        "\n",
        "# Character-to-integer mapping\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Hashing parameters\n",
        "hash_size = 8  # Size of the hash space (embedding dimension)\n",
        "\n",
        "# Hash function\n",
        "def hash_function(ch, hash_size):\n",
        "    return int(hashlib.md5(ch.encode()).hexdigest(), 16) % hash_size\n",
        "\n",
        "# Encode each character using hashing\n",
        "hashed_vectors = np.zeros((len(word), hash_size))\n",
        "for i, ch in enumerate(word):\n",
        "    hash_idx = hash_function(ch, hash_size)\n",
        "    hashed_vectors[i, hash_idx] = 1  # One-hot vector in the hashed space\n",
        "\n",
        "print(\"\\nEncoded Word using Hashing Embedding:\")\n",
        "for i, ch in enumerate(word):\n",
        "    print(f\"Character: {ch}, Hashed Vector: {hashed_vectors[i]}\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = hash_size  # Input size now matches hash embedding size\n",
        "output_size = len(char_to_int)  # Output matches the unique character count\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(hashed_vectors) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = hashed_vectors[t].reshape(-1, 1)\n",
        "        y_true = char_to_int[word[t + 1]]  # Use char_to_int for consistent indexing\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(hashed_vectors) - 1):\n",
        "    x_t = hashed_vectors[t].reshape(-1, 1)\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {word[t]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbe2GSdMPDCN",
        "outputId": "d7ce2076-c3d2-484b-f653-1b5d0cf22f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded Word using Hashing Embedding:\n",
            "Character: t, Hashed Vector: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Character: e, Hashed Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Character: l, Hashed Vector: [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Character: e, Hashed Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Character: p, Hashed Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Character: h, Hashed Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Character: o, Hashed Vector: [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Character: n, Hashed Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Character: e, Hashed Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Epoch 0, Loss: [1.72977452]\n",
            "Epoch 200, Loss: [0.01671459]\n",
            "Epoch 400, Loss: [0.00694615]\n",
            "Epoch 600, Loss: [0.00446653]\n",
            "Epoch 800, Loss: [0.00331141]\n",
            "Epoch 1000, Loss: [0.00264625]\n",
            "Epoch 1200, Loss: [0.00221407]\n",
            "Epoch 1400, Loss: [0.00190886]\n",
            "Epoch 1600, Loss: [0.00168028]\n",
            "Epoch 1800, Loss: [0.00150203]\n",
            "\n",
            "Predictions:\n",
            "Input: t, Predicted: e\n",
            "Input: e, Predicted: l\n",
            "Input: l, Predicted: e\n",
            "Input: e, Predicted: p\n",
            "Input: p, Predicted: h\n",
            "Input: h, Predicted: o\n",
            "Input: o, Predicted: n\n",
            "Input: n, Predicted: e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-2 : OPERATION"
      ],
      "metadata": {
        "id": "qds-_KSjbOZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "J_Wc4_u5Tb5r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "413fc845-85cc-4286-e037-28d505e4b308",
        "id": "n3FpXLJrTb5s"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using One-Hot Encoding: [4, 5, 1, 6, 0, 7, 2, 4, 3] \n",
            "\n",
            "Epoch 0, Loss: [1.84822721]\n",
            "Epoch 200, Loss: [0.03247513]\n",
            "Epoch 400, Loss: [0.00822547]\n",
            "Epoch 600, Loss: [0.00507107]\n",
            "Epoch 800, Loss: [0.00351097]\n",
            "Epoch 1000, Loss: [0.0034943]\n",
            "Epoch 1200, Loss: [0.00512022]\n",
            "Epoch 1400, Loss: [0.00256907]\n",
            "Epoch 1600, Loss: [0.00250509]\n",
            "Epoch 1800, Loss: [0.01118589]\n",
            "\n",
            "Predictions:\n",
            "Input: o, Predicted: p\n",
            "Input: p, Predicted: e\n",
            "Input: e, Predicted: r\n",
            "Input: r, Predicted: a\n",
            "Input: a, Predicted: t\n",
            "Input: t, Predicted: i\n",
            "Input: i, Predicted: o\n",
            "Input: o, Predicted: n\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"operation\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "encoded_word = [char_to_int[ch] for ch in word]\n",
        "\n",
        "print(\"Encoded Word using One-Hot Encoding:\", encoded_word, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(encoded_word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding**"
      ],
      "metadata": {
        "id": "aOFG2UmPTb5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"operation\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "# Embedding parameters\n",
        "embedding_dim = 4  # Dimensionality of the embeddings\n",
        "vocab_size = len(char_to_int)\n",
        "\n",
        "# Randomly initialized embedding matrix\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim)\n",
        "\n",
        "# Encode the word using embeddings\n",
        "embedded_word = [embedding_matrix[char_to_int[ch]] for ch in word]\n",
        "print(\"Encoded Word using Word Embedding:\")\n",
        "for ch, emb in zip(word, embedded_word):\n",
        "    print(f\"Character: {ch}, Embedding: {emb}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(encoded_word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773cd9c9-f2d3-4cc4-f4ef-3329c01c2d40",
        "id": "GuoGT1BxTb5u"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using Word Embedding:\n",
            "Character: o, Embedding: [ 0.11617933  0.05182357  0.4108245  -0.41417887]\n",
            "Character: p, Embedding: [ 0.73905628 -1.76463     0.8566777  -1.08343584]\n",
            "Character: e, Embedding: [ 0.13711358  1.02871666 -0.67525888  0.22507392]\n",
            "Character: r, Embedding: [-0.14797962 -3.59627136  0.34262165  0.13214909]\n",
            "Character: a, Embedding: [-0.10480013  0.64304829  0.82731614 -0.20202049]\n",
            "Character: t, Embedding: [ 0.18836633 -0.62056445  0.60702552  0.97698812]\n",
            "Character: i, Embedding: [-1.33047587 -1.61224687 -0.88722738  0.81691221]\n",
            "Character: o, Embedding: [ 0.11617933  0.05182357  0.4108245  -0.41417887]\n",
            "Character: n, Embedding: [-0.07361069  1.05132346  2.10553455  0.01170187]\n",
            "\n",
            "\n",
            "Epoch 0, Loss: [1.8484461]\n",
            "Epoch 200, Loss: [0.0179796]\n",
            "Epoch 400, Loss: [0.00482728]\n",
            "Epoch 600, Loss: [0.00285725]\n",
            "Epoch 800, Loss: [0.00208428]\n",
            "Epoch 1000, Loss: [0.00176208]\n",
            "Epoch 1200, Loss: [0.00146921]\n",
            "Epoch 1400, Loss: [0.00113212]\n",
            "Epoch 1600, Loss: [0.00093785]\n",
            "Epoch 1800, Loss: [0.00082381]\n",
            "\n",
            "Predictions:\n",
            "Input: o, Predicted: p\n",
            "Input: p, Predicted: e\n",
            "Input: e, Predicted: r\n",
            "Input: r, Predicted: a\n",
            "Input: a, Predicted: t\n",
            "Input: t, Predicted: i\n",
            "Input: i, Predicted: o\n",
            "Input: o, Predicted: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "Om7o-aG6Tb5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Define the data\n",
        "word = \"operation\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "\n",
        "# Count character occurrences\n",
        "char_counts = Counter(word)\n",
        "\n",
        "# Encode word using BoW\n",
        "bow_vector = np.zeros(len(char_to_int))  # Correct initialization\n",
        "for ch, count in char_counts.items():\n",
        "    bow_vector[char_to_int[ch]] = count\n",
        "\n",
        "print(\"\\nEncoded Word using Bag of Words (BoW):\")\n",
        "print(f\"Characters: {sorted(set(word))}\")\n",
        "print(f\"BoW Vector: {bow_vector}\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Convert BoW vector to sequences for training\n",
        "# For simplicity, assume the BoW vector is used as input at every time step\n",
        "encoded_word = bow_vector.astype(int)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1\n",
        "        y_true = encoded_word[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(encoded_word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218f4884-d672-4dc6-cbd7-7b2267046834",
        "id": "T8JxgY0qTb5v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded Word using Bag of Words (BoW):\n",
            "Characters: ['a', 'e', 'i', 'n', 'o', 'p', 'r', 't']\n",
            "BoW Vector: [1. 1. 1. 1. 2. 1. 1. 1.]\n",
            "Epoch 0, Loss: [1.81937094]\n",
            "Epoch 200, Loss: [0.31825305]\n",
            "Epoch 400, Loss: [0.32262148]\n",
            "Epoch 600, Loss: [0.26164613]\n",
            "Epoch 800, Loss: [0.31116175]\n",
            "Epoch 1000, Loss: [0.18394353]\n",
            "Epoch 1200, Loss: [0.01760416]\n",
            "Epoch 1400, Loss: [0.27928677]\n",
            "Epoch 1600, Loss: [0.2826133]\n",
            "Epoch 1800, Loss: [0.2820995]\n",
            "\n",
            "Predictions:\n",
            "Input: e, Predicted: e\n",
            "Input: e, Predicted: e\n",
            "Input: e, Predicted: e\n",
            "Input: e, Predicted: e\n",
            "Input: i, Predicted: e\n",
            "Input: e, Predicted: e\n",
            "Input: e, Predicted: e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hashing Embedding**"
      ],
      "metadata": {
        "id": "y7DLU7tOTb5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import hashlib\n",
        "\n",
        "# Define the data\n",
        "word = \"operation\"\n",
        "\n",
        "\n",
        "# Character-to-integer mapping\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "\n",
        "# Hashing parameters\n",
        "hash_size = 8  # Size of the hash space (embedding dimension)\n",
        "\n",
        "# Hash function\n",
        "def hash_function(ch, hash_size):\n",
        "    return int(hashlib.md5(ch.encode()).hexdigest(), 16) % hash_size\n",
        "\n",
        "# Encode each character using hashing\n",
        "hashed_vectors = np.zeros((len(word), hash_size))\n",
        "for i, ch in enumerate(word):\n",
        "    hash_idx = hash_function(ch, hash_size)\n",
        "    hashed_vectors[i, hash_idx] = 1  # One-hot vector in the hashed space\n",
        "\n",
        "print(\"\\nEncoded Word using Hashing Embedding:\")\n",
        "for i, ch in enumerate(word):\n",
        "    print(f\"Character: {ch}, Hashed Vector: {hashed_vectors[i]}\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 8\n",
        "input_size = hash_size  # Input size now matches hash embedding size\n",
        "output_size = len(char_to_int)  # Output matches the unique character count\n",
        "learning_rate = 0.1\n",
        "epochs = 2000\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "\n",
        "# Helper functions\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, softmax(y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss = 0\n",
        "    dWx, dWh, dWy = np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(Wy)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "    for t in range(len(hashed_vectors) - 1):\n",
        "        # Prepare input and target\n",
        "        x_t = hashed_vectors[t].reshape(-1, 1)\n",
        "        y_true = char_to_int[word[t + 1]]  # Use char_to_int for consistent indexing\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        loss += -np.log(y_pred[y_true])\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_pred\n",
        "        dy[y_true] -= 1  # Cross-entropy gradient\n",
        "\n",
        "        dWy += np.dot(dy, h_prev.T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)\n",
        "        dWx += np.dot(dh, x_t.T)\n",
        "        dWh += np.dot(dh, h_prev.T)\n",
        "        dbh += dh\n",
        "\n",
        "    # Update parameters (with gradient clipping)\n",
        "    for param, dparam in zip([Wx, Wh, Wy, bh, by], [dWx, dWh, dWy, dbh, dby]):\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip gradients\n",
        "        param -= learning_rate * dparam\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss / len(word)}\")\n",
        "\n",
        "# Predict next characters\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(hashed_vectors) - 1):\n",
        "    x_t = hashed_vectors[t].reshape(-1, 1)\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {word[t]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ebb4aa-e6e2-45ca-9db7-bec5104e17f6",
        "id": "8289zf7iTb5w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded Word using Hashing Embedding:\n",
            "Character: o, Hashed Vector: [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Character: p, Hashed Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Character: e, Hashed Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Character: r, Hashed Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Character: a, Hashed Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Character: t, Hashed Vector: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Character: i, Hashed Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Character: o, Hashed Vector: [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Character: n, Hashed Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 0, Loss: [1.84831175]\n",
            "Epoch 200, Loss: [0.02249502]\n",
            "Epoch 400, Loss: [0.00909796]\n",
            "Epoch 600, Loss: [0.00580962]\n",
            "Epoch 800, Loss: [0.00434665]\n",
            "Epoch 1000, Loss: [0.00351166]\n",
            "Epoch 1200, Loss: [0.00295531]\n",
            "Epoch 1400, Loss: [0.00254436]\n",
            "Epoch 1600, Loss: [0.00222167]\n",
            "Epoch 1800, Loss: [0.00196016]\n",
            "\n",
            "Predictions:\n",
            "Input: o, Predicted: p\n",
            "Input: p, Predicted: e\n",
            "Input: e, Predicted: r\n",
            "Input: r, Predicted: a\n",
            "Input: a, Predicted: t\n",
            "Input: t, Predicted: i\n",
            "Input: i, Predicted: o\n",
            "Input: o, Predicted: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Word Prediction in a Sentence"
      ],
      "metadata": {
        "id": "og7gKg7hObcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-1 : I am CS Student"
      ],
      "metadata": {
        "id": "XcCs90g5bx47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"I am CS student\"\n",
        "words = sentence.split()\n",
        "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "# Model parameters\n",
        "input_size = len(word_to_int)  # Number of unique words\n",
        "output_size = len(word_to_int)\n",
        "hidden_size = 4  # Arbitrary hidden state size\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize weights and biases\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN forward step\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):  # Training for 1000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "\n",
        "    for t in range(len(encoded_sentence) - 1):\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_sentence[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass (gradient calculation and parameter update)\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next words\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_sentence) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_word = int_to_word[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhbeAN8CNdYo",
        "outputId": "51efa0cc-c3a8-4fb1-b41a-75c56874c9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: [4.16598495]\n",
            "Epoch 100, Loss: [3.74484097]\n",
            "Epoch 200, Loss: [3.56650005]\n",
            "Epoch 300, Loss: [3.32936517]\n",
            "Epoch 400, Loss: [2.66141231]\n",
            "Epoch 500, Loss: [1.60595847]\n",
            "Epoch 600, Loss: [0.8719616]\n",
            "Epoch 700, Loss: [0.52982219]\n",
            "Epoch 800, Loss: [0.36752919]\n",
            "Epoch 900, Loss: [0.27982389]\n",
            "\n",
            "Predictions:\n",
            "Input: I, Predicted: am\n",
            "Input: am, Predicted: CS\n",
            "Input: CS, Predicted: student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-2: France is where i Grew up, but I now live in Boston. I speak fluent French"
      ],
      "metadata": {
        "id": "AC53Db67b8e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"France is where i Grew up, but I now live in Boston. I speak fluent French\"\n",
        "words = sentence.split()\n",
        "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "# Model parameters\n",
        "input_size = len(word_to_int)  # Number of unique words\n",
        "output_size = len(word_to_int)\n",
        "hidden_size = 4  # Arbitrary hidden state size\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize weights and biases\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN forward step\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):  # Training for 1000 epochs\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "\n",
        "    for t in range(len(encoded_sentence) - 1):\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_sentence[t + 1]\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass (gradient calculation and parameter update)\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Predict next words\n",
        "h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "print(\"\\nPredictions:\")\n",
        "for t in range(len(encoded_sentence) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_word = int_to_word[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_word[encoded_sentence[t]]}, Predicted: {next_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANn2Utj9bitM",
        "outputId": "c57f92b1-6d3d-4978-d3de-8e98bb53fcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: [40.68110442]\n",
            "Epoch 100, Loss: [39.78991664]\n",
            "Epoch 200, Loss: [39.38713047]\n",
            "Epoch 300, Loss: [37.40914653]\n",
            "Epoch 400, Loss: [32.61490015]\n",
            "Epoch 500, Loss: [23.62491818]\n",
            "Epoch 600, Loss: [20.74614278]\n",
            "Epoch 700, Loss: [15.62007448]\n",
            "Epoch 800, Loss: [15.00181898]\n",
            "Epoch 900, Loss: [16.89805145]\n",
            "\n",
            "Predictions:\n",
            "Input: France, Predicted: is\n",
            "Input: is, Predicted: where\n",
            "Input: where, Predicted: i\n",
            "Input: i, Predicted: Grew\n",
            "Input: Grew, Predicted: up,\n",
            "Input: up,, Predicted: but\n",
            "Input: but, Predicted: I\n",
            "Input: I, Predicted: now\n",
            "Input: now, Predicted: live\n",
            "Input: live, Predicted: in\n",
            "Input: in, Predicted: Boston.\n",
            "Input: Boston., Predicted: I\n",
            "Input: I, Predicted: Boston.\n",
            "Input: speak, Predicted: fluent\n",
            "Input: fluent, Predicted: French\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question#3 - Back Propogation**"
      ],
      "metadata": {
        "id": "GwjcbzX8VwDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Neuron - Single Character\n",
        "\n"
      ],
      "metadata": {
        "id": "W4AS6HwAXRG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "word = \"photosynthesis\"\n",
        "char_to_int = {ch: i for i, ch in enumerate(sorted(set(word)))}\n",
        "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
        "encoded_word = [char_to_int[ch] for ch in word]\n",
        "\n",
        "print(\"Encoded Word using One-Hot Encoding:\", encoded_word, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 1  # Single neuron\n",
        "input_size = len(char_to_int)\n",
        "output_size = len(char_to_int)\n",
        "learning_rate = 0.1\n",
        "epochs = 10  # Fewer epochs for demonstration\n",
        "\n",
        "# Model parameters\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN step function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)  # Single hidden unit\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "\n",
        "    for t in range(len(encoded_word) - 1):\n",
        "        print(f\"\\nTime Step {t + 1}/{len(encoded_word) - 1}\")\n",
        "\n",
        "        # Input preparation\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_word[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_word[t + 1]  # Target character index\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        # Print forward propagation details\n",
        "        print(\"Forward Propagation:\")\n",
        "        print(f\"  Input (x_t): {x_t.T}\")\n",
        "        print(f\"  Hidden State (h_prev): {h_prev.T}\")\n",
        "        print(f\"  Raw Prediction (y_pred): {y_pred.T}\")\n",
        "        print(f\"  Softmax Prediction (y_pred_softmax): {y_pred_softmax.T}\")\n",
        "\n",
        "        # Loss calculation\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop through tanh activation\n",
        "        dWx = np.dot(dh, x_t.T)\n",
        "        dWh = np.dot(dh, h_prev.T)\n",
        "        dbh = dh\n",
        "\n",
        "        # Print backward propagation details\n",
        "        print(\"Backward Propagation:\")\n",
        "        print(f\"  Gradient of Loss wrt Output (dy): {dy.T}\")\n",
        "        print(f\"  Gradient Wy (dWy): {dWy}\")\n",
        "        print(f\"  Gradient Wh (dWh): {dWh}\")\n",
        "        print(f\"  Gradient Wx (dWx): {dWx}\")\n",
        "        print(f\"  Gradient bh (dbh): {dbh.T}\")\n",
        "        print(f\"  Gradient by (dby): {dby.T}\")\n",
        "\n",
        "        # Update parameters\n",
        "        Wy -= learning_rate * dWy\n",
        "        by -= learning_rate * dby\n",
        "        Wx -= learning_rate * dWx\n",
        "        Wh -= learning_rate * dWh\n",
        "        bh -= learning_rate * dbh\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1} Loss: {loss}\")\n",
        "\n",
        "# Testing (prediction after training)\n",
        "print(\"\\nTesting the RNN Model:\")\n",
        "h_prev = np.zeros((hidden_size, 1))  # Single hidden unit\n",
        "for t in range(len(encoded_word) - 1):\n",
        "    x_t = np.zeros((input_size, 1))\n",
        "    x_t[encoded_word[t]] = 1  # One-hot encoding\n",
        "    h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "    next_char = int_to_char[np.argmax(y_pred)]\n",
        "    print(f\"Input: {int_to_char[encoded_word[t]]}, Predicted: {next_char}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv106pk4V1bx",
        "outputId": "30276cdf-5501-4807-eb91-787e92f6f636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Word using One-Hot Encoding: [5, 1, 4, 7, 4, 6, 8, 3, 7, 1, 0, 6, 2, 6] \n",
            "\n",
            "Epoch 1/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.00940775]]\n",
            "  Raw Prediction (y_pred): [[-2.60556853e-04  3.26682597e-05 -1.29606134e-04  4.16751678e-06\n",
            "   9.34522460e-06  5.60869129e-05 -7.82832643e-05 -9.11400438e-06\n",
            "   2.17966570e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.11108653 0.1111191  0.11110107 0.11111594 0.11111651 0.11112171\n",
            "  0.11110678 0.11111446 0.1111179 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.11108653 -0.8888809   0.11110107  0.11111594  0.11111651  0.11112171\n",
            "   0.11110678  0.11111446  0.1111179 ]]\n",
            "  Gradient Wy (dWy): [[ 0.00104507]\n",
            " [-0.00836237]\n",
            " [ 0.00104521]\n",
            " [ 0.00104535]\n",
            " [ 0.00104536]\n",
            " [ 0.00104541]\n",
            " [ 0.00104527]\n",
            " [ 0.00104534]\n",
            " [ 0.00104537]]\n",
            "  Gradient Wh (dWh): [[-7.19301021e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.00764583\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00764583]]\n",
            "  Gradient by (dby): [[ 0.11108653 -0.8888809   0.11110107  0.11111594  0.11111651  0.11112171\n",
            "   0.11110678  0.11111446  0.1111179 ]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00168591]]\n",
            "  Raw Prediction (y_pred): [[-0.01106178  0.08888083 -0.01108671 -0.01111216 -0.01111315 -0.01112205\n",
            "  -0.01109647 -0.01110964 -0.01111552]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10983237 0.12137657 0.10982963 0.10982683 0.10982672 0.10982575\n",
            "  0.10982856 0.10982711 0.10982646]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10983237  0.12137657  0.10982963  0.10982683 -0.89017328  0.10982575\n",
            "   0.10982856  0.10982711  0.10982646]]\n",
            "  Gradient Wy (dWy): [[-0.00018517]\n",
            " [-0.00020463]\n",
            " [-0.00018516]\n",
            " [-0.00018516]\n",
            " [ 0.00150075]\n",
            " [-0.00018516]\n",
            " [-0.00018516]\n",
            " [-0.00018516]\n",
            " [-0.00018516]]\n",
            "  Gradient Wh (dWh): [[8.37219929e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.00496599  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00496599]]\n",
            "  Gradient by (dby): [[ 0.10983237  0.12137657  0.10982963  0.10982683 -0.89017328  0.10982575\n",
            "   0.10982856  0.10982711  0.10982646]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00460218]]\n",
            "  Raw Prediction (y_pred): [[-0.02196403  0.07673051 -0.02202927 -0.02209592  0.07790228 -0.02212179\n",
            "  -0.02205484 -0.0220893  -0.0221047 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10860074 0.11986579 0.10859365 0.10858641 0.12000633 0.10858361\n",
            "  0.10859087 0.10858713 0.10858546]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10860074  0.11986579  0.10859365  0.10858641  0.12000633  0.10858361\n",
            "   0.10859087 -0.89141287  0.10858546]]\n",
            "  Gradient Wy (dWy): [[-0.0004998 ]\n",
            " [-0.00055164]\n",
            " [-0.00049977]\n",
            " [-0.00049973]\n",
            " [-0.00055229]\n",
            " [-0.00049972]\n",
            " [-0.00049975]\n",
            " [ 0.00410244]\n",
            " [-0.00049973]]\n",
            "  Gradient Wh (dWh): [[1.36618467e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00296856  0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00296856]]\n",
            "  Gradient by (dby): [[ 0.10860074  0.11986579  0.10859365  0.10858641  0.12000633  0.10858361\n",
            "   0.10859087 -0.89141287  0.10858546]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.00939438]]\n",
            "  Raw Prediction (y_pred): [[-0.03321249  0.06480504 -0.0330822  -0.0329491   0.0659125  -0.03289744\n",
            "  -0.03303113  0.06703337 -0.03293156]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10736756 0.1184245  0.10738155 0.10739584 0.11855572 0.10740139\n",
            "  0.10738703 0.11868868 0.10739773]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10736756  0.1184245   0.10738155  0.10739584 -0.88144428  0.10740139\n",
            "   0.10738703  0.11868868  0.10739773]]\n",
            "  Gradient Wy (dWy): [[ 0.00100865]\n",
            " [ 0.00111252]\n",
            " [ 0.00100878]\n",
            " [ 0.00100892]\n",
            " [-0.00828062]\n",
            " [ 0.00100897]\n",
            " [ 0.00100883]\n",
            " [ 0.00111501]\n",
            " [ 0.00100893]]\n",
            "  Gradient Wh (dWh): [[-4.49725605e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00478718  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00478718]]\n",
            "  Gradient by (dby): [[ 0.10736756  0.1184245   0.10738155  0.10739584 -0.88144428  0.10740139\n",
            "   0.10738703  0.11868868  0.10739773]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00356677]]\n",
            "  Raw Prediction (y_pred): [[-0.04358945  0.05290616 -0.04364096 -0.04369359  0.15404369 -0.04371402\n",
            "  -0.04366116  0.05518388 -0.04370053]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10612137 0.11687196 0.1061159  0.10611031 0.12931051 0.10610815\n",
            "  0.10611376 0.11713847 0.10610958]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10612137  0.11687196  0.1061159   0.10611031  0.12931051  0.10610815\n",
            "  -0.89388624  0.11713847  0.10610958]]\n",
            "  Gradient Wy (dWy): [[-0.00037851]\n",
            " [-0.00041686]\n",
            " [-0.00037849]\n",
            " [-0.00037847]\n",
            " [-0.00046122]\n",
            " [-0.00037846]\n",
            " [ 0.00318829]\n",
            " [-0.00041781]\n",
            " [-0.00037847]]\n",
            "  Gradient Wh (dWh): [[-1.61814455e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00453672 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00453672]]\n",
            "  Gradient by (dby): [[ 0.10612137  0.11687196  0.1061159   0.10611031  0.12931051  0.10610815\n",
            "  -0.89388624  0.11713847  0.10610958]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00104009]]\n",
            "  Raw Prediction (y_pred): [[-0.05427195  0.04122972 -0.05428775 -0.05430389  0.14111669 -0.05431016\n",
            "   0.04570643  0.04346601 -0.05430602]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10500126 0.11552351 0.10499961 0.10499791 0.12765879 0.10499725\n",
            "  0.11604183 0.11578214 0.10499769]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10500126  0.11552351  0.10499961  0.10499791  0.12765879  0.10499725\n",
            "   0.11604183  0.11578214 -0.89500231]]\n",
            "  Gradient Wy (dWy): [[-0.00010921]\n",
            " [-0.00012015]\n",
            " [-0.00010921]\n",
            " [-0.00010921]\n",
            " [-0.00013278]\n",
            " [-0.00010921]\n",
            " [-0.00012069]\n",
            " [-0.00012042]\n",
            " [ 0.00093088]]\n",
            "  Gradient Wh (dWh): [[6.44173712e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00619346  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00619346]]\n",
            "  Gradient by (dby): [[ 0.10500126  0.11552351  0.10499961  0.10499791  0.12765879  0.10499725\n",
            "   0.11604183  0.11578214 -0.89500231]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[0.00053264]]\n",
            "  Raw Prediction (y_pred): [[-0.06481578  0.02968416 -0.06480953 -0.06480314  0.12835344 -0.06480066\n",
            "   0.03408845  0.03188539  0.03519765]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10391868 0.11421797 0.10391933 0.10392    0.12606251 0.10392026\n",
            "  0.11472213 0.11446967 0.11484945]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10391868  0.11421797  0.10391933 -0.89608     0.12606251  0.10392026\n",
            "   0.11472213  0.11446967  0.11484945]]\n",
            "  Gradient Wy (dWy): [[ 5.53510622e-05]\n",
            " [ 6.08368561e-05]\n",
            " [ 5.53514084e-05]\n",
            " [-4.77286454e-04]\n",
            " [ 6.71457122e-05]\n",
            " [ 5.53518994e-05]\n",
            " [ 6.11053900e-05]\n",
            " [ 6.09709194e-05]\n",
            " [ 6.11732059e-05]]\n",
            "  Gradient Wh (dWh): [[-2.27185561e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00426529]]\n",
            "  Gradient bh (dbh): [[-0.00426529]]\n",
            "  Gradient by (dby): [[ 0.10391868  0.11421797  0.10391933 -0.89608     0.12606251  0.10392026\n",
            "   0.11472213  0.11446967  0.11484945]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00793227]]\n",
            "  Raw Prediction (y_pred): [[-0.07497242  0.01822579 -0.07508405  0.02480148  0.11573301 -0.07524236\n",
            "   0.02269048  0.02045136  0.02369477]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10289403 0.11294465 0.10288255 0.11368978 0.12451237 0.10286626\n",
            "  0.11345004 0.11319629 0.11356403]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10289403  0.11294465  0.10288255  0.11368978  0.12451237  0.10286626\n",
            "   0.11345004 -0.88680371  0.11356403]]\n",
            "  Gradient Wy (dWy): [[-0.00081618]\n",
            " [-0.00089591]\n",
            " [-0.00081609]\n",
            " [-0.00090182]\n",
            " [-0.00098767]\n",
            " [-0.00081596]\n",
            " [-0.00089992]\n",
            " [ 0.00703436]\n",
            " [-0.00090082]]\n",
            "  Gradient Wh (dWh): [[1.85538392e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.00233903  0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00233903]]\n",
            "  Gradient by (dby): [[ 0.10289403  0.11294465  0.10288255  0.11368978  0.12451237  0.10286626\n",
            "   0.11345004 -0.88680371  0.11356403]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01118883]]\n",
            "  Raw Prediction (y_pred): [[-0.08579227  0.00701494 -0.08563661  0.01344121  0.1033149  -0.08541587\n",
            "   0.01117877  0.10909463  0.01237988]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10172493 0.1116177  0.10174077 0.1123373  0.12290106 0.10176323\n",
            "  0.11208343 0.12361345 0.11221813]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10172493 -0.8883823   0.10174077  0.1123373   0.12290106  0.10176323\n",
            "   0.11208343  0.12361345  0.11221813]]\n",
            "  Gradient Wy (dWy): [[ 0.00113818]\n",
            " [-0.00993996]\n",
            " [ 0.00113836]\n",
            " [ 0.00125692]\n",
            " [ 0.00137512]\n",
            " [ 0.00113861]\n",
            " [ 0.00125408]\n",
            " [ 0.00138309]\n",
            " [ 0.00125559]]\n",
            "  Gradient Wh (dWh): [[-9.24282337e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00826076  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00826076]]\n",
            "  Gradient by (dby): [[ 0.10172493 -0.8883823   0.10174077  0.1123373   0.12290106  0.10176323\n",
            "   0.11208343  0.12361345  0.11221813]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.00172921]]\n",
            "  Raw Prediction (y_pred): [[-9.57028567e-02  9.58131692e-02 -9.56804460e-02  2.20260168e-03\n",
            "   9.10077762e-02 -9.56486683e-02  5.23294488e-05  9.67541601e-02\n",
            "   1.13695939e-03]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10066747 0.12191678 0.10066972 0.11102197 0.12133233 0.10067292\n",
            "  0.1107835  0.12203156 0.11090373]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.89933253  0.12191678  0.10066972  0.11102197  0.12133233  0.10067292\n",
            "   0.1107835   0.12203156  0.11090373]]\n",
            "  Gradient Wy (dWy): [[-0.00155514]\n",
            " [ 0.00021082]\n",
            " [ 0.00017408]\n",
            " [ 0.00019198]\n",
            " [ 0.00020981]\n",
            " [ 0.00017408]\n",
            " [ 0.00019157]\n",
            " [ 0.00021102]\n",
            " [ 0.00019178]]\n",
            "  Gradient Wh (dWh): [[4.16276852e-05]]\n",
            "  Gradient Wx (dWx): [[0.        0.0240732 0.        0.        0.        0.        0.\n",
            "  0.        0.       ]]\n",
            "  Gradient bh (dbh): [[0.0240732]]\n",
            "  Gradient by (dby): [[-0.89933253  0.12191678  0.10066972  0.11102197  0.12133233  0.10067292\n",
            "   0.1107835   0.12203156  0.11090373]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00475269]]\n",
            "  Raw Prediction (y_pred): [[-0.00559     0.08358656 -0.10565722 -0.00890188  0.07886404 -0.1057537\n",
            "  -0.01096885  0.08456647 -0.00996682]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.11022721 0.12050851 0.099731   0.10986276 0.11994075 0.09972138\n",
            "  0.10963591 0.12062666 0.10974582]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.11022721  0.12050851  0.099731    0.10986276  0.11994075  0.09972138\n",
            "  -0.89036409  0.12062666  0.10974582]]\n",
            "  Gradient Wy (dWy): [[-0.00052388]\n",
            " [-0.00057274]\n",
            " [-0.00047399]\n",
            " [-0.00052214]\n",
            " [-0.00057004]\n",
            " [-0.00047394]\n",
            " [ 0.00423162]\n",
            " [-0.0005733 ]\n",
            " [-0.00052159]]\n",
            "  Gradient Wh (dWh): [[-2.29025226e-05]]\n",
            "  Gradient Wx (dWx): [[0.00481885 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00481885]]\n",
            "  Gradient by (dby): [[ 0.11022721  0.12050851  0.099731    0.10986276  0.11994075  0.09972138\n",
            "  -0.89036409  0.12062666  0.10974582]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00120016]]\n",
            "  Raw Prediction (y_pred): [[-0.01671107  0.07155477 -0.11567983 -0.01988699  0.06687563 -0.11570523\n",
            "   0.07803672  0.07249524 -0.02093414]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10897663 0.11903282 0.09870787 0.10863108 0.11847715 0.09870536\n",
            "  0.11980689 0.11914482 0.10851738]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10897663  0.11903282 -0.90129213  0.10863108  0.11847715  0.09870536\n",
            "   0.11980689  0.11914482  0.10851738]]\n",
            "  Gradient Wy (dWy): [[-0.00013079]\n",
            " [-0.00014286]\n",
            " [ 0.00108169]\n",
            " [-0.00013037]\n",
            " [-0.00014219]\n",
            " [-0.00011846]\n",
            " [-0.00014379]\n",
            " [-0.00014299]\n",
            " [-0.00013024]]\n",
            "  Gradient Wh (dWh): [[-1.1768068e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.         0.\n",
            "  0.00980541 0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00980541]]\n",
            "  Gradient by (dby): [[ 0.10897663  0.11903282 -0.90129213  0.10863108  0.11847715  0.09870536\n",
            "   0.11980689  0.11914482  0.10851738]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00313556]]\n",
            "  Raw Prediction (y_pred): [[-0.02755533  0.05964091 -0.02552343 -0.03075091  0.05502464 -0.12558716\n",
            "   0.06607388  0.06058523 -0.03179001]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10788967 0.11771958 0.10810911 0.10754545 0.11717741 0.09781494\n",
            "  0.11847931 0.1178308  0.10743375]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10788967  0.11771958  0.10810911  0.10754545  0.11717741  0.09781494\n",
            "  -0.88152069  0.1178308   0.10743375]]\n",
            "  Gradient Wy (dWy): [[-0.00033829]\n",
            " [-0.00036912]\n",
            " [-0.00033898]\n",
            " [-0.00033722]\n",
            " [-0.00036742]\n",
            " [-0.0003067 ]\n",
            " [ 0.00276406]\n",
            " [-0.00036947]\n",
            " [-0.00033687]]\n",
            "  Gradient Wh (dWh): [[-1.58826364e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.00506533 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00506533]]\n",
            "  Gradient by (dby): [[ 0.10788967  0.11771958  0.10810911  0.10754545  0.11717741  0.09781494\n",
            "  -0.88152069  0.1178308   0.10743375]]\n",
            "\n",
            "Epoch 1 Loss: [28.84621103]\n",
            "Epoch 2/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.00948499]]\n",
            "  Raw Prediction (y_pred): [[-0.03869232  0.04793814 -0.03651046 -0.04149992  0.04332853 -0.13529418\n",
            "   0.15410679  0.04877322 -0.0425062 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10656996 0.11621386 0.10680274 0.10627117 0.11567939 0.09675672\n",
            "  0.12923091 0.11631095 0.10616429]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10656996 -0.88378614  0.10680274  0.10627117  0.11567939  0.09675672\n",
            "   0.12923091  0.11631095  0.10616429]]\n",
            "  Gradient Wy (dWy): [[ 0.00101081]\n",
            " [-0.0083827 ]\n",
            " [ 0.00101302]\n",
            " [ 0.00100798]\n",
            " [ 0.00109722]\n",
            " [ 0.00091774]\n",
            " [ 0.00122575]\n",
            " [ 0.00110321]\n",
            " [ 0.00100697]]\n",
            "  Gradient Wh (dWh): [[-9.22558608e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.00972651\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00972651]]\n",
            "  Gradient by (dby): [[ 0.10656996 -0.88378614  0.10680274  0.10627117  0.11567939  0.09675672\n",
            "   0.12923091  0.11631095  0.10616429]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00407594]]\n",
            "  Raw Prediction (y_pred): [[-0.04897507  0.13623887 -0.04700119 -0.05213269  0.03173767 -0.1450496\n",
            "   0.14131317  0.03717354 -0.05315155]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10537237 0.12681311 0.10558057 0.10504017 0.11422992 0.09571987\n",
            "  0.12745823 0.11485255 0.10493321]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10537237  0.12681311  0.10558057  0.10504017 -0.88577008  0.09571987\n",
            "   0.12745823  0.11485255  0.10493321]]\n",
            "  Gradient Wy (dWy): [[-0.00042949]\n",
            " [-0.00051688]\n",
            " [-0.00043034]\n",
            " [-0.00042814]\n",
            " [ 0.00361035]\n",
            " [-0.00039015]\n",
            " [-0.00051951]\n",
            " [-0.00046813]\n",
            " [-0.0004277 ]]\n",
            "  Gradient Wh (dWh): [[2.33063519e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.00571803  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00571803]]\n",
            "  Gradient by (dby): [[ 0.10537237  0.12681311  0.10558057  0.10504017 -0.88577008  0.09571987\n",
            "   0.12745823  0.11485255  0.10493321]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00515515]]\n",
            "  Raw Prediction (y_pred): [[-0.05948267  0.12355046 -0.05754431 -0.0626373   0.1203148  -0.15462807\n",
            "   0.12857748  0.02569062 -0.06364731]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10419362 0.12512135 0.10439578 0.10386545 0.12471715 0.09473709\n",
            "  0.12575192 0.11345704 0.1037606 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10419362  0.12512135  0.10439578  0.10386545  0.12471715  0.09473709\n",
            "   0.12575192 -0.88654296  0.1037606 ]]\n",
            "  Gradient Wy (dWy): [[-0.00053713]\n",
            " [-0.00064502]\n",
            " [-0.00053818]\n",
            " [-0.00053544]\n",
            " [-0.00064294]\n",
            " [-0.00048838]\n",
            " [-0.00064827]\n",
            " [ 0.00457026]\n",
            " [-0.0005349 ]]\n",
            "  Gradient Wh (dWh): [[8.74681987e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00169672  0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00169672]]\n",
            "  Gradient by (dby): [[ 0.10419362  0.12512135  0.10439578  0.10386545  0.12471715  0.09473709\n",
            "   0.12575192 -0.88654296  0.1037606 ]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01016959]]\n",
            "  Raw Prediction (y_pred): [[-0.07032483  0.11113678 -0.06819796 -0.07301734  0.10786293 -0.16401154\n",
            "   0.11585611  0.11430431 -0.07399057]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10301016 0.12350594 0.10322949 0.10273318 0.12310226 0.09379776\n",
            "  0.12409019 0.12389777 0.10263325]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10301016  0.12350594  0.10322949  0.10273318 -0.87689774  0.09379776\n",
            "   0.12409019  0.12389777  0.10263325]]\n",
            "  Gradient Wy (dWy): [[ 0.00104757]\n",
            " [ 0.001256  ]\n",
            " [ 0.0010498 ]\n",
            " [ 0.00104475]\n",
            " [-0.00891769]\n",
            " [ 0.00095388]\n",
            " [ 0.00126195]\n",
            " [ 0.00125999]\n",
            " [ 0.00104374]]\n",
            "  Gradient Wh (dWh): [[-5.4361838e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00534553  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00534553]]\n",
            "  Gradient by (dby): [[ 0.10301016  0.12350594  0.10322949  0.10273318 -0.87689774  0.09379776\n",
            "   0.12409019  0.12389777  0.10263325]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00432856]]\n",
            "  Raw Prediction (y_pred): [[-0.08022566  0.09869327 -0.07831819 -0.08329662  0.19552975 -0.17347651\n",
            "   0.10358561  0.10195573 -0.08428474]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10182442 0.12177424 0.10201883 0.1015122  0.13415627 0.09275847\n",
            "  0.12237146 0.12217217 0.10141194]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10182442  0.12177424  0.10201883  0.1015122   0.13415627  0.09275847\n",
            "  -0.87762854  0.12217217  0.10141194]]\n",
            "  Gradient Wy (dWy): [[-0.00044075]\n",
            " [-0.00052711]\n",
            " [-0.00044159]\n",
            " [-0.0004394 ]\n",
            " [-0.0005807 ]\n",
            " [-0.00040151]\n",
            " [ 0.00379886]\n",
            " [-0.00052883]\n",
            " [-0.00043897]]\n",
            "  Gradient Wh (dWh): [[-2.46724954e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00569994 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00569994]]\n",
            "  Gradient by (dby): [[ 0.10182442  0.12177424  0.10201883  0.1015122   0.13415627  0.09275847\n",
            "  -0.87762854  0.12217217  0.10141194]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00199057]]\n",
            "  Raw Prediction (y_pred): [[-0.09047289  0.08653052 -0.08855303 -0.09344714  0.18211917 -0.18273885\n",
            "   0.19132668  0.08973155 -0.09442122]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10063553 0.1201221  0.10082892 0.10033666 0.13217112 0.09176577\n",
            "  0.1333937  0.12050724 0.10023897]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10063553  0.1201221   0.10082892  0.10033666  0.13217112  0.09176577\n",
            "   0.1333937   0.12050724 -0.89976103]]\n",
            "  Gradient Wy (dWy): [[-0.00020032]\n",
            " [-0.00023911]\n",
            " [-0.00020071]\n",
            " [-0.00019973]\n",
            " [-0.0002631 ]\n",
            " [-0.00018267]\n",
            " [-0.00026553]\n",
            " [-0.00023988]\n",
            " [ 0.00179104]]\n",
            "  Gradient Wh (dWh): [[1.21914516e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00612459  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00612459]]\n",
            "  Gradient by (dby): [[ 0.10063553  0.1201221   0.10082892  0.10033666  0.13217112  0.09176577\n",
            "   0.1333937   0.12050724 -0.89976103]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[0.00036367]]\n",
            "  Raw Prediction (y_pred): [[-0.10060149  0.07453332 -0.0986689  -0.1034799   0.16890741 -0.19190165\n",
            "   0.17796372  0.07767407 -0.00444024]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09970233 0.11878604 0.0998952  0.09941576 0.13054238 0.09100267\n",
            "  0.13172998 0.1191597  0.10976594]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09970233  0.11878604  0.0998952  -0.90058424  0.13054238  0.09100267\n",
            "   0.13172998  0.1191597   0.10976594]]\n",
            "  Gradient Wy (dWy): [[ 3.62588002e-05]\n",
            " [ 4.31989819e-05]\n",
            " [ 3.63289415e-05]\n",
            " [-3.27515960e-04]\n",
            " [ 4.74744188e-05]\n",
            " [ 3.30949914e-05]\n",
            " [ 4.79063148e-05]\n",
            " [ 4.33348723e-05]\n",
            " [ 3.99186395e-05]]\n",
            "  Gradient Wh (dWh): [[-1.58761576e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00436553]]\n",
            "  Gradient bh (dbh): [[-0.00436553]]\n",
            "  Gradient by (dby): [[ 0.09970233  0.11878604  0.0998952  -0.90058424  0.13054238  0.09100267\n",
            "   0.13172998  0.1191597   0.10976594]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00828637]]\n",
            "  Raw Prediction (y_pred): [[-0.11033285  0.06259942 -0.10853736 -0.01342522  0.15583338 -0.20105265\n",
            "   0.16487723  0.06578281 -0.0154334 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09881913 0.11747475 0.09899672 0.10887482 0.12895421 0.0902489\n",
            "  0.13012575 0.11784932 0.1086564 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09881913  0.11747475  0.09899672  0.10887482  0.12895421  0.0902489\n",
            "   0.13012575 -0.88215068  0.1086564 ]]\n",
            "  Gradient Wy (dWy): [[-0.00081885]\n",
            " [-0.00097344]\n",
            " [-0.00082032]\n",
            " [-0.00090218]\n",
            " [-0.00106856]\n",
            " [-0.00074784]\n",
            " [-0.00107827]\n",
            " [ 0.00730983]\n",
            " [-0.00090037]]\n",
            "  Gradient Wh (dWh): [[8.82298778e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.         -0.00106476  0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00106476]]\n",
            "  Gradient by (dby): [[ 0.09881913  0.11747475  0.09899672  0.10887482  0.12895421  0.0902489\n",
            "   0.13012575 -0.88215068  0.1086564 ]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01183435]]\n",
            "  Raw Prediction (y_pred): [[-0.12076944  0.05098172 -0.11871766 -0.02430289  0.14298525 -0.20995866\n",
            "   0.15166471  0.15393174 -0.02625944]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09769818 0.1160051  0.09789885 0.10759235 0.12718436 0.08936184\n",
            "  0.12829305 0.12858423 0.10738204]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09769818 -0.8839949   0.09789885  0.10759235  0.12718436  0.08936184\n",
            "   0.12829305  0.12858423  0.10738204]]\n",
            "  Gradient Wy (dWy): [[ 0.00115619]\n",
            " [-0.0104615 ]\n",
            " [ 0.00115857]\n",
            " [ 0.00127328]\n",
            " [ 0.00150514]\n",
            " [ 0.00105754]\n",
            " [ 0.00151826]\n",
            " [ 0.00152171]\n",
            " [ 0.0012708 ]]\n",
            "  Gradient Wh (dWh): [[-0.00012321]]\n",
            "  Gradient Wx (dWx): [[ 0.         0.         0.         0.         0.         0.\n",
            "   0.        -0.0104115  0.       ]]\n",
            "  Gradient bh (dbh): [[-0.0104115]]\n",
            "  Gradient by (dby): [[ 0.09769818 -0.8839949   0.09789885  0.10759235  0.12718436  0.08936184\n",
            "   0.12829305  0.12858423  0.10738204]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00060911]]\n",
            "  Raw Prediction (y_pred): [[-0.13019657  0.13929981 -0.12833434 -0.03506858  0.13023711 -0.21896868\n",
            "   0.1389586   0.14111806 -0.03702252]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09669577 0.12660425 0.09687601 0.10634597 0.12546205 0.08848186\n",
            "  0.12656105 0.12683465 0.10613838]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.90330423  0.12660425  0.09687601  0.10634597  0.12546205  0.08848186\n",
            "   0.12656105  0.12683465  0.10613838]]\n",
            "  Gradient Wy (dWy): [[ 5.50211827e-04]\n",
            " [-7.71159392e-05]\n",
            " [-5.90081659e-05]\n",
            " [-6.47764170e-05]\n",
            " [-7.64202175e-05]\n",
            " [-5.38952041e-05]\n",
            " [-7.70896302e-05]\n",
            " [-7.72562823e-05]\n",
            " [-6.46499712e-05]]\n",
            "  Gradient Wh (dWh): [[-1.4536078e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.02386445 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02386445]]\n",
            "  Gradient by (dby): [[-0.90330423  0.12660425  0.09687601  0.10634597  0.12546205  0.08848186\n",
            "   0.12656105  0.12683465  0.10613838]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00570685]]\n",
            "  Raw Prediction (y_pred): [[-0.03972489  0.12660093 -0.13795046 -0.04570524  0.11767942 -0.22784664\n",
            "   0.12635365  0.12845362 -0.04764597]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10597322 0.1251499  0.09605883 0.10534136 0.12403834 0.08780028\n",
            "  0.12511896 0.12538198 0.10513712]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10597322  0.1251499   0.09605883  0.10534136  0.12403834  0.08780028\n",
            "  -0.87488104  0.12538198  0.10513712]]\n",
            "  Gradient Wy (dWy): [[-0.00060477]\n",
            " [-0.00071421]\n",
            " [-0.00054819]\n",
            " [-0.00060117]\n",
            " [-0.00070787]\n",
            " [-0.00050106]\n",
            " [ 0.00499282]\n",
            " [-0.00071554]\n",
            " [-0.0006    ]]\n",
            "  Gradient Wh (dWh): [[-3.42799637e-05]]\n",
            "  Gradient Wx (dWx): [[0.00600681 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00600681]]\n",
            "  Gradient by (dby): [[ 0.10597322  0.1251499   0.09605883  0.10534136  0.12403834  0.08780028\n",
            "  -0.87488104  0.12538198  0.10513712]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00216412]]\n",
            "  Raw Prediction (y_pred): [[-0.05042049  0.11411251 -0.14760614 -0.05623807  0.10528341 -0.23660609\n",
            "   0.21380728  0.11590204 -0.05815313]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10466048 0.12337822 0.09496761 0.10405338 0.12229369 0.08688071\n",
            "  0.13631241 0.12359921 0.1038543 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10466048  0.12337822 -0.90503239  0.10405338  0.12229369  0.08688071\n",
            "   0.13631241  0.12359921  0.1038543 ]]\n",
            "  Gradient Wy (dWy): [[-0.0002265 ]\n",
            " [-0.00026701]\n",
            " [ 0.0019586 ]\n",
            " [-0.00022518]\n",
            " [-0.00026466]\n",
            " [-0.00018802]\n",
            " [-0.000295  ]\n",
            " [-0.00026748]\n",
            " [-0.00022475]]\n",
            "  Gradient Wh (dWh): [[-2.13068308e-05]]\n",
            "  Gradient Wx (dWx): [[0.        0.        0.        0.        0.        0.        0.0098455\n",
            "  0.        0.       ]]\n",
            "  Gradient bh (dbh): [[0.0098455]]\n",
            "  Gradient by (dby): [[ 0.10466048  0.12337822 -0.90503239  0.10405338  0.12229369  0.08688071\n",
            "   0.13631241  0.12359921  0.1038543 ]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00424182]]\n",
            "  Raw Prediction (y_pred): [[-0.0608292   0.10175875 -0.05707305 -0.06664447  0.0930491  -0.24530647\n",
            "   0.2001978   0.10354961 -0.0685427 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10371675 0.12202813 0.10410706 0.10311536 0.12096992 0.08624451\n",
            "  0.13465159 0.12224686 0.10291981]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10371675  0.12202813  0.10410706  0.10311536  0.12096992  0.08624451\n",
            "  -0.86534841  0.12224686  0.10291981]]\n",
            "  Gradient Wy (dWy): [[-0.00043995]\n",
            " [-0.00051762]\n",
            " [-0.0004416 ]\n",
            " [-0.0004374 ]\n",
            " [-0.00051313]\n",
            " [-0.00036583]\n",
            " [ 0.00367065]\n",
            " [-0.00051855]\n",
            " [-0.00043657]]\n",
            "  Gradient Wh (dWh): [[-2.66688239e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.00628711 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00628711]]\n",
            "  Gradient by (dby): [[ 0.10371675  0.12202813  0.10410706  0.10311536  0.12096992  0.08624451\n",
            "  -0.86534841  0.12224686  0.10291981]]\n",
            "\n",
            "Epoch 2 Loss: [28.36870256]\n",
            "Epoch 3/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.00973255]]\n",
            "  Raw Prediction (y_pred): [[-0.07158644  0.08966324 -0.06768126 -0.07694878  0.08098544 -0.25384801\n",
            "   0.28658224  0.09127464 -0.07880677]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10233175 0.12023758 0.10273215 0.10178448 0.1191987  0.08528157\n",
            "  0.14640674 0.12043149 0.10159554]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10233175 -0.87976242  0.10273215  0.10178448  0.1191987   0.08528157\n",
            "   0.14640674  0.12043149  0.10159554]]\n",
            "  Gradient Wy (dWy): [[ 0.00099595]\n",
            " [-0.00856233]\n",
            " [ 0.00099985]\n",
            " [ 0.00099062]\n",
            " [ 0.00116011]\n",
            " [ 0.00083001]\n",
            " [ 0.00142491]\n",
            " [ 0.00117211]\n",
            " [ 0.00098878]]\n",
            "  Gradient Wh (dWh): [[-0.00011691]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.01201253\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01201253]]\n",
            "  Gradient by (dby): [[ 0.10233175 -0.87976242  0.10273215  0.10178448  0.1191987   0.08528157\n",
            "   0.14640674  0.12043149  0.10159554]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00638745]]\n",
            "  Raw Prediction (y_pred): [[-0.08137443  0.17750998 -0.07772622 -0.08713514  0.06902761 -0.26247146\n",
            "   0.27211777  0.07928999 -0.0889981 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.1012066  0.13111153 0.1015765  0.10062525 0.11763258 0.08444217\n",
            "  0.14412142 0.11884598 0.10043797]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.1012066   0.13111153  0.1015765   0.10062525 -0.88236742  0.08444217\n",
            "   0.14412142  0.11884598  0.10043797]]\n",
            "  Gradient Wy (dWy): [[-0.00064645]\n",
            " [-0.00083747]\n",
            " [-0.00064881]\n",
            " [-0.00064274]\n",
            " [ 0.00563607]\n",
            " [-0.00053937]\n",
            " [-0.00092057]\n",
            " [-0.00075912]\n",
            " [-0.00064154]]\n",
            "  Gradient Wh (dWh): [[4.11163123e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.00643705  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00643705]]\n",
            "  Gradient by (dby): [[ 0.1012066   0.13111153  0.1015765   0.10062525 -0.88236742  0.08444217\n",
            "   0.14412142  0.11884598  0.10043797]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00597254]]\n",
            "  Raw Prediction (y_pred): [[-0.09150696  0.16440188 -0.08789016 -0.09719787  0.15726867 -0.27091357\n",
            "   0.25770051  0.06740341 -0.09904149]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10009535 0.12928666 0.10045803 0.09952734 0.12836771 0.08365629\n",
            "  0.14192954 0.11733506 0.09934402]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10009535  0.12928666  0.10045803  0.09952734  0.12836771  0.08365629\n",
            "   0.14192954 -0.88266494  0.09934402]]\n",
            "  Gradient Wy (dWy): [[-0.00059782]\n",
            " [-0.00077217]\n",
            " [-0.00059999]\n",
            " [-0.00059443]\n",
            " [-0.00076668]\n",
            " [-0.00049964]\n",
            " [-0.00084768]\n",
            " [ 0.00527175]\n",
            " [-0.00059334]]\n",
            "  Gradient Wh (dWh): [[2.74941615e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.         -0.00046034  0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00046034]]\n",
            "  Gradient by (dby): [[ 0.10009535  0.12928666  0.10045803  0.09952734  0.12836771  0.08365629\n",
            "   0.14192954 -0.88266494  0.09934402]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01119948]]\n",
            "  Raw Prediction (y_pred): [[-0.10198998  0.15162234 -0.09817837 -0.10714143  0.14446231 -0.27917706\n",
            "   0.24332091  0.15560177 -0.1089413 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09897299 0.12754374 0.09935096 0.09846445 0.12663378 0.08290206\n",
            "  0.13979232 0.1280523  0.09828739]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09897299  0.12754374  0.09935096  0.09846445 -0.87336622  0.08290206\n",
            "   0.13979232  0.1280523   0.09828739]]\n",
            "  Gradient Wy (dWy): [[ 0.00110845]\n",
            " [ 0.00142842]\n",
            " [ 0.00111268]\n",
            " [ 0.00110275]\n",
            " [-0.00978125]\n",
            " [ 0.00092846]\n",
            " [ 0.0015656 ]\n",
            " [ 0.00143412]\n",
            " [ 0.00110077]]\n",
            "  Gradient Wh (dWh): [[-6.58876007e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00588309  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00588309]]\n",
            "  Gradient by (dby): [[ 0.09897299  0.12754374  0.09935096  0.09846445 -0.87336622  0.08290206\n",
            "   0.13979232  0.1280523   0.09828739]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00534994]]\n",
            "  Raw Prediction (y_pred): [[-0.11143071  0.13872456 -0.1078796  -0.11699647  0.23176395 -0.28756549\n",
            "   0.2295219   0.142866   -0.11880312]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09786004 0.12567429 0.09820817 0.09731689 0.13792816 0.08205613\n",
            "  0.13761926 0.12619584 0.09714123]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09786004  0.12567429  0.09820817  0.09731689  0.13792816  0.08205613\n",
            "  -0.86238074  0.12619584  0.09714123]]\n",
            "  Gradient Wy (dWy): [[-0.00052355]\n",
            " [-0.00067235]\n",
            " [-0.00052541]\n",
            " [-0.00052064]\n",
            " [-0.00073791]\n",
            " [-0.000439  ]\n",
            " [ 0.00461369]\n",
            " [-0.00067514]\n",
            " [-0.0005197 ]]\n",
            "  Gradient Wh (dWh): [[-3.7504013e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00701017 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00701017]]\n",
            "  Gradient by (dby): [[ 0.09786004  0.12567429  0.09820817  0.09731689  0.13792816  0.08205613\n",
            "  -0.86238074  0.12619584  0.09714123]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00298494]]\n",
            "  Raw Prediction (y_pred): [[-0.12128229  0.1261772  -0.11773417 -0.12672726  0.21797748 -0.29575735\n",
            "   0.31573535  0.13023606 -0.12851285]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09662253 0.123751   0.09696597 0.09609785 0.13564914 0.08115305\n",
            "  0.14957973 0.1242543  0.09592642]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09662253  0.123751    0.09696597  0.09609785  0.13564914  0.08115305\n",
            "   0.14957973  0.1242543  -0.90407358]]\n",
            "  Gradient Wy (dWy): [[-0.00028841]\n",
            " [-0.00036939]\n",
            " [-0.00028944]\n",
            " [-0.00028685]\n",
            " [-0.0004049 ]\n",
            " [-0.00024224]\n",
            " [-0.00044649]\n",
            " [-0.00037089]\n",
            " [ 0.0026986 ]]\n",
            "  Gradient Wh (dWh): [[1.81587764e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00608347  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00608347]]\n",
            "  Gradient by (dby): [[ 0.09662253  0.123751    0.09696597  0.09609785  0.13564914  0.08115305\n",
            "   0.14957973  0.1242543  -0.90407358]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[0.00017384]]\n",
            "  Raw Prediction (y_pred): [[-0.13103176  0.11382938 -0.12747547 -0.13633548  0.20442157 -0.30385396\n",
            "   0.30074119  0.11779728 -0.03809929]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09582698 0.1224136  0.09616838 0.09532009 0.13402115 0.08061801\n",
            "  0.14757216 0.12290029 0.10515934]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09582698  0.1224136   0.09616838 -0.90467991  0.13402115  0.08061801\n",
            "   0.14757216  0.12290029  0.10515934]]\n",
            "  Gradient Wy (dWy): [[ 1.66580841e-05]\n",
            " [ 2.12797682e-05]\n",
            " [ 1.67174305e-05]\n",
            " [-1.57265037e-04]\n",
            " [ 2.32975676e-05]\n",
            " [ 1.40142326e-05]\n",
            " [ 2.56532066e-05]\n",
            " [ 2.13643719e-05]\n",
            " [ 1.82803753e-05]]\n",
            "  Gradient Wh (dWh): [[-7.94193838e-07]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00456866]]\n",
            "  Gradient bh (dbh): [[-0.00456866]]\n",
            "  Gradient by (dby): [[ 0.09582698  0.1224136   0.09616838 -0.90467991  0.13402115  0.08061801\n",
            "   0.14757216  0.12290029  0.10515934]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00878848]]\n",
            "  Raw Prediction (y_pred): [[-0.14036724  0.10151032 -0.1369657  -0.04587233  0.19099358 -0.31196899\n",
            "   0.28608628  0.10554482 -0.04863051]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09507507 0.12109124 0.09539902 0.10449735 0.13242647 0.08008311\n",
            "  0.14563745 0.12158077 0.10420952]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09507507  0.12109124  0.09539902  0.10449735  0.13242647  0.08008311\n",
            "   0.14563745 -0.87841923  0.10420952]]\n",
            "  Gradient Wy (dWy): [[-0.00083557]\n",
            " [-0.00106421]\n",
            " [-0.00083841]\n",
            " [-0.00091837]\n",
            " [-0.00116383]\n",
            " [-0.00070381]\n",
            " [-0.00127993]\n",
            " [ 0.00771997]\n",
            " [-0.00091584]]\n",
            "  Gradient Wh (dWh): [[-1.68303988e-06]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.00019151 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00019151]]\n",
            "  Gradient by (dby): [[ 0.09507507  0.12109124  0.09539902  0.10449735  0.13242647  0.08008311\n",
            "   0.14563745 -0.87841923  0.10420952]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01273011]]\n",
            "  Raw Prediction (y_pred): [[-0.15046725  0.08958912 -0.14680852 -0.05630925  0.17781455 -0.31984861\n",
            "   0.27127852  0.19328672 -0.0590136 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09401054 0.11951756 0.09435513 0.10329251 0.13054118 0.07936247\n",
            "  0.14333043 0.13257664 0.10301355]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09401054 -0.88048244  0.09435513  0.10329251  0.13054118  0.07936247\n",
            "   0.14333043  0.13257664  0.10301355]]\n",
            "  Gradient Wy (dWy): [[ 0.00119676]\n",
            " [-0.01120864]\n",
            " [ 0.00120115]\n",
            " [ 0.00131493]\n",
            " [ 0.0016618 ]\n",
            " [ 0.00101029]\n",
            " [ 0.00182461]\n",
            " [ 0.00168772]\n",
            " [ 0.00131137]]\n",
            "  Gradient Wh (dWh): [[-0.00016285]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.01279246  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01279246]]\n",
            "  Gradient by (dby): [[ 0.09401054 -0.88048244  0.09435513  0.10329251  0.13054118  0.07936247\n",
            "   0.14333043  0.13257664  0.10301355]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00285106]]\n",
            "  Raw Prediction (y_pred): [[-0.15943948  0.17749742 -0.15602489 -0.06664799  0.1647141  -0.3278782\n",
            "   0.25712187  0.18010688 -0.06934258]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09307109 0.13036009 0.09338944 0.10212067 0.12870426 0.0786435\n",
            "  0.14116438 0.1307007  0.10184587]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.90692891  0.13036009  0.09338944  0.10212067  0.12870426  0.0786435\n",
            "   0.14116438  0.1307007   0.10184587]]\n",
            "  Gradient Wy (dWy): [[ 0.00258571]\n",
            " [-0.00037166]\n",
            " [-0.00026626]\n",
            " [-0.00029115]\n",
            " [-0.00036694]\n",
            " [-0.00022422]\n",
            " [-0.00040247]\n",
            " [-0.00037264]\n",
            " [-0.00029037]]\n",
            "  Gradient Wh (dWh): [[-6.77909037e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.02377743 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02377743]]\n",
            "  Gradient by (dby): [[-0.90692891  0.13036009  0.09338944  0.10212067  0.12870426  0.0786435\n",
            "   0.14116438  0.1307007   0.10184587]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00678831]]\n",
            "  Raw Prediction (y_pred): [[-0.06863608  0.16442219 -0.16530825 -0.07686223  0.15183225 -0.33576596\n",
            "   0.24305032  0.16705676 -0.07953392]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10209266 0.12888739 0.09268519 0.10125628 0.12727488 0.07815945\n",
            "  0.13943064 0.1292274  0.10098611]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10209266  0.12888739  0.09268519  0.10125628  0.12727488  0.07815945\n",
            "  -0.86056936  0.1292274   0.10098611]]\n",
            "  Gradient Wy (dWy): [[-0.00069304]\n",
            " [-0.00087493]\n",
            " [-0.00062918]\n",
            " [-0.00068736]\n",
            " [-0.00086398]\n",
            " [-0.00053057]\n",
            " [ 0.00584181]\n",
            " [-0.00087724]\n",
            " [-0.00068552]]\n",
            "  Gradient Wh (dWh): [[-4.99315197e-05]]\n",
            "  Gradient Wx (dWx): [[0.00735552 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00735552]]\n",
            "  Gradient by (dby): [[ 0.10209266  0.12888739  0.09268519  0.10125628  0.12727488  0.07815945\n",
            "  -0.86056936  0.1292274   0.10098611]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00316004]]\n",
            "  Raw Prediction (y_pred): [[-0.07894672  0.15156922 -0.17462826 -0.08698615  0.13911492 -0.34356056\n",
            "   0.32906763  0.15411526 -0.0896266 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.10073712 0.12685317 0.0915452  0.0999305  0.1252831  0.07731597\n",
            "  0.15149138 0.12717656 0.09966699]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.10073712  0.12685317 -0.9084548   0.0999305   0.1252831   0.07731597\n",
            "   0.15149138  0.12717656  0.09966699]]\n",
            "  Gradient Wy (dWy): [[-0.00031833]\n",
            " [-0.00040086]\n",
            " [ 0.00287075]\n",
            " [-0.00031578]\n",
            " [-0.0003959 ]\n",
            " [-0.00024432]\n",
            " [-0.00047872]\n",
            " [-0.00040188]\n",
            " [-0.00031495]]\n",
            "  Gradient Wh (dWh): [[-3.1181559e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.         0.\n",
            "  0.00986746 0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00986746]]\n",
            "  Gradient by (dby): [[ 0.10073712  0.12685317 -0.9084548   0.0999305   0.1252831   0.07731597\n",
            "   0.15149138  0.12717656  0.09966699]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00549254]]\n",
            "  Raw Prediction (y_pred): [[-0.08895574  0.1388603  -0.08374837 -0.09698077  0.12657948 -0.35130624\n",
            "   0.31394625  0.14140906 -0.09959759]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09992522 0.12549152 0.10044693 0.09912653 0.1239598  0.07686661\n",
            "  0.14950415 0.12581177 0.09886747]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09992522  0.12549152  0.10044693  0.09912653  0.1239598   0.07686661\n",
            "  -0.85049585  0.12581177  0.09886747]]\n",
            "  Gradient Wy (dWy): [[-0.00054884]\n",
            " [-0.00068927]\n",
            " [-0.00055171]\n",
            " [-0.00054446]\n",
            " [-0.00068085]\n",
            " [-0.00042219]\n",
            " [ 0.00467138]\n",
            " [-0.00069103]\n",
            " [-0.00054303]]\n",
            "  Gradient Wh (dWh): [[-4.21286712e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.00767017 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00767017]]\n",
            "  Gradient by (dby): [[ 0.09992522  0.12549152  0.10044693  0.09912653  0.1239598   0.07686661\n",
            "  -0.85049585  0.12581177  0.09886747]]\n",
            "\n",
            "Epoch 3 Loss: [28.0130072]\n",
            "Epoch 4/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.0101703]]\n",
            "  Raw Prediction (y_pred): [[-0.09938278  0.12646946 -0.09401743 -0.10688299  0.11423119 -0.35889837\n",
            "   0.39880367  0.12875078 -0.10945567]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09849468 0.12345229 0.09902456 0.09775871 0.12195066 0.07598128\n",
            "  0.16209604 0.12373425 0.09750753]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09849468 -0.87654771  0.09902456  0.09775871  0.12195066  0.07598128\n",
            "   0.16209604  0.12373425  0.09750753]]\n",
            "  Gradient Wy (dWy): [[ 0.00100172]\n",
            " [-0.00891475]\n",
            " [ 0.00100711]\n",
            " [ 0.00099424]\n",
            " [ 0.00124027]\n",
            " [ 0.00077275]\n",
            " [ 0.00164857]\n",
            " [ 0.00125841]\n",
            " [ 0.00099168]]\n",
            "  Gradient Wh (dWh): [[-0.00014814]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.01456641\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01456641]]\n",
            "  Gradient by (dby): [[ 0.09849468 -0.87654771  0.09902456  0.09775871  0.12195066  0.07598128\n",
            "   0.16209604  0.12373425  0.09750753]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00863032]]\n",
            "  Raw Prediction (y_pred): [[-0.10871018  0.21392606 -0.10365007 -0.11667089  0.10197951 -0.36660958\n",
            "   0.38282924  0.11647053 -0.11924033]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09744676 0.13455088 0.0979411  0.09667409 0.12030087 0.07529447\n",
            "  0.15930898 0.12205684 0.09642601]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09744676  0.13455088  0.0979411   0.09667409 -0.87969913  0.07529447\n",
            "   0.15930898  0.12205684  0.09642601]]\n",
            "  Gradient Wy (dWy): [[-0.000841  ]\n",
            " [-0.00116122]\n",
            " [-0.00084526]\n",
            " [-0.00083433]\n",
            " [ 0.00759208]\n",
            " [-0.00064982]\n",
            " [-0.00137489]\n",
            " [-0.00105339]\n",
            " [-0.00083219]]\n",
            "  Gradient Wh (dWh): [[6.19131064e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.00717391  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00717391]]\n",
            "  Gradient by (dby): [[ 0.09744676  0.13455088  0.0979411   0.09667409 -0.87969913  0.07529447\n",
            "   0.15930898  0.12205684  0.09642601]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00705495]]\n",
            "  Raw Prediction (y_pred): [[-0.11849928  0.20048752 -0.11346747 -0.12633796  0.18995941 -0.37413008\n",
            "   0.36687752  0.10425619 -0.12888076]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09640694 0.13263023 0.09689327 0.09565419 0.13124121 0.07466022\n",
            "  0.15664076 0.12046192 0.09541127]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09640694  0.13263023  0.09689327  0.09565419  0.13124121  0.07466022\n",
            "   0.15664076 -0.87953808  0.09541127]]\n",
            "  Gradient Wy (dWy): [[-0.00068015]\n",
            " [-0.0009357 ]\n",
            " [-0.00068358]\n",
            " [-0.00067484]\n",
            " [-0.0009259 ]\n",
            " [-0.00052672]\n",
            " [-0.00110509]\n",
            " [ 0.0062051 ]\n",
            " [-0.00067312]]\n",
            "  Gradient Wh (dWh): [[-5.40682429e-06]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00076639 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00076639]]\n",
            "  Gradient by (dby): [[ 0.09640694  0.13263023  0.09689327  0.09565419  0.13124121  0.07466022\n",
            "   0.15664076 -0.87953808  0.09541127]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01251297]]\n",
            "  Raw Prediction (y_pred): [[-0.12868191  0.18744363 -0.12343619 -0.13588944  0.17687921 -0.38147729\n",
            "   0.350971    0.19210598 -0.13838518]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09534739 0.13079778 0.09584887 0.09466264 0.12942326 0.07404934\n",
            "  0.154035   0.13140903 0.09442668]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09534739  0.13079778  0.09584887  0.09466264 -0.87057674  0.07404934\n",
            "   0.154035    0.13140903  0.09442668]]\n",
            "  Gradient Wy (dWy): [[ 0.00119308]\n",
            " [ 0.00163667]\n",
            " [ 0.00119935]\n",
            " [ 0.00118451]\n",
            " [-0.0108935 ]\n",
            " [ 0.00092658]\n",
            " [ 0.00192744]\n",
            " [ 0.00164432]\n",
            " [ 0.00118156]]\n",
            "  Gradient Wh (dWh): [[-8.06762694e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00644741  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00644741]]\n",
            "  Gradient by (dby): [[ 0.09534739  0.13079778  0.09584887  0.09466264 -0.87057674  0.07404934\n",
            "   0.154035    0.13140903  0.09442668]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00663181]]\n",
            "  Raw Prediction (y_pred): [[-0.13768611  0.17414989 -0.1327474  -0.14536902  0.26388605 -0.38899821\n",
            "   0.33580522  0.17907222 -0.14786345]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09431013 0.1288211  0.09477705 0.09358833 0.14091555 0.07335249\n",
            "  0.15142341 0.12945676 0.09335517]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09431013  0.1288211   0.09477705  0.09358833  0.14091555  0.07335249\n",
            "  -0.84857659  0.12945676  0.09335517]]\n",
            "  Gradient Wy (dWy): [[-0.00062545]\n",
            " [-0.00085432]\n",
            " [-0.00062854]\n",
            " [-0.00062066]\n",
            " [-0.00093452]\n",
            " [-0.00048646]\n",
            " [ 0.0056276 ]\n",
            " [-0.00085853]\n",
            " [-0.00061911]]\n",
            "  Gradient Wh (dWh): [[-5.62788613e-05]]\n",
            "  Gradient Wx (dWx): [[0.        0.        0.        0.        0.0084862 0.        0.\n",
            "  0.        0.       ]]\n",
            "  Gradient bh (dbh): [[0.0084862]]\n",
            "  Gradient by (dby): [[ 0.09431013  0.1288211   0.09477705  0.09358833  0.14091555  0.07335249\n",
            "  -0.84857659  0.12945676  0.09335517]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00400773]]\n",
            "  Raw Prediction (y_pred): [[-0.1471903   0.16129649 -0.14226307 -0.15472648  0.24980296 -0.39631792\n",
            "   0.42063222  0.16611123 -0.15719453]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09304038 0.12666177 0.09349994 0.09234184 0.13838322 0.07252316\n",
            "  0.16416238 0.12727309 0.09211422]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09304038  0.12666177  0.09349994  0.09234184  0.13838322  0.07252316\n",
            "   0.16416238  0.12727309 -0.90788578]]\n",
            "  Gradient Wy (dWy): [[-0.00037288]\n",
            " [-0.00050763]\n",
            " [-0.00037472]\n",
            " [-0.00037008]\n",
            " [-0.0005546 ]\n",
            " [-0.00029065]\n",
            " [-0.00065792]\n",
            " [-0.00051008]\n",
            " [ 0.00363856]]\n",
            "  Gradient Wh (dWh): [[2.43080231e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00606528  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00606528]]\n",
            "  Gradient by (dby): [[ 0.09304038  0.12666177  0.09349994  0.09234184  0.13838322  0.07252316\n",
            "   0.16416238  0.12727309 -0.90788578]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[-1.62218261e-05]]\n",
            "  Raw Prediction (y_pred): [[-0.15660501  0.14867483 -0.15167019 -0.16395795  0.23597845 -0.4035461\n",
            "   0.40416367  0.1533615  -0.06639859]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0923667  0.12534205 0.09282364 0.09169003 0.13677675 0.07215564\n",
            "  0.16182819 0.12593086 0.10108613]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0923667   0.12534205  0.09282364 -0.90830997  0.13677675  0.07215564\n",
            "   0.16182819  0.12593086  0.10108613]]\n",
            "  Gradient Wy (dWy): [[-1.49835659e-06]\n",
            " [-2.03327693e-06]\n",
            " [-1.50576899e-06]\n",
            " [ 1.47344465e-05]\n",
            " [-2.21876861e-06]\n",
            " [-1.17049626e-06]\n",
            " [-2.62514880e-06]\n",
            " [-2.04282859e-06]\n",
            " [-1.63980170e-06]]\n",
            "  Gradient Wh (dWh): [[7.90331095e-08]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00487202]]\n",
            "  Gradient bh (dbh): [[-0.00487202]]\n",
            "  Gradient by (dby): [[ 0.0923667   0.12534205  0.09282364 -0.90830997  0.13677675  0.07215564\n",
            "   0.16182819  0.12593086  0.10108613]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00942668]]\n",
            "  Raw Prediction (y_pred): [[-0.16558111  0.13603518 -0.16081824 -0.07313369  0.22226768 -0.41081884\n",
            "   0.38810356  0.1408208  -0.07652113]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09173198 0.12402553 0.09216993 0.10061673 0.13519523 0.07178197\n",
            "  0.15958166 0.12462049 0.10027647]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09173198  0.12402553  0.09216993  0.10061673  0.13519523  0.07178197\n",
            "   0.15958166 -0.87537951  0.10027647]]\n",
            "  Gradient Wy (dWy): [[-0.00086473]\n",
            " [-0.00116915]\n",
            " [-0.00086886]\n",
            " [-0.00094848]\n",
            " [-0.00127444]\n",
            " [-0.00067667]\n",
            " [-0.00150432]\n",
            " [ 0.00825192]\n",
            " [-0.00094527]]\n",
            "  Gradient Wh (dWh): [[-1.37452548e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.00145812 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00145812]]\n",
            "  Gradient by (dby): [[ 0.09173198  0.12402553  0.09216993  0.10061673  0.13519523  0.07178197\n",
            "   0.15958166 -0.87537951  0.10027647]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01390906]]\n",
            "  Raw Prediction (y_pred): [[-0.17539927  0.12389574 -0.17036709 -0.08317734  0.20883199 -0.41785431\n",
            "   0.3718432   0.22821737 -0.08651292]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09072425 0.12237863 0.09118194 0.09948896 0.13322721 0.07119123\n",
            "  0.15681507 0.13583506 0.09915766]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09072425 -0.87762137  0.09118194  0.09948896  0.13322721  0.07119123\n",
            "   0.15681507  0.13583506  0.09915766]]\n",
            "  Gradient Wy (dWy): [[ 0.00126189]\n",
            " [-0.01220689]\n",
            " [ 0.00126826]\n",
            " [ 0.0013838 ]\n",
            " [ 0.00185307]\n",
            " [ 0.0009902 ]\n",
            " [ 0.00218115]\n",
            " [ 0.00188934]\n",
            " [ 0.00137919]]\n",
            "  Gradient Wh (dWh): [[-0.00021505]]\n",
            "  Gradient Wx (dWx): [[ 0.         0.         0.         0.         0.         0.\n",
            "   0.        -0.0154611  0.       ]]\n",
            "  Gradient bh (dbh): [[-0.0154611]]\n",
            "  Gradient by (dby): [[ 0.09072425 -0.87762137  0.09118194  0.09948896  0.13322721  0.07119123\n",
            "   0.15681507  0.13583506  0.09915766]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00499387]]\n",
            "  Raw Prediction (y_pred): [[-0.18394928  0.21143776 -0.1792165  -0.09314087  0.19544132 -0.42508907\n",
            "   0.35640642  0.21475563 -0.09645777]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08984862 0.1334215  0.09027486 0.09838955 0.13130421 0.07059691\n",
            "  0.1542357  0.13386491 0.09806375]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.91015138  0.1334215   0.09027486  0.09838955  0.13130421  0.07059691\n",
            "   0.1542357   0.13386491  0.09806375]]\n",
            "  Gradient Wy (dWy): [[ 0.00454518]\n",
            " [-0.00066629]\n",
            " [-0.00045082]\n",
            " [-0.00049135]\n",
            " [-0.00065572]\n",
            " [-0.00035255]\n",
            " [-0.00077023]\n",
            " [-0.0006685 ]\n",
            " [-0.00048972]]\n",
            "  Gradient Wh (dWh): [[-0.00011888]]\n",
            "  Gradient Wx (dWx): [[0.         0.02380432 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02380432]]\n",
            "  Gradient by (dby): [[-0.91015138  0.1334215   0.09027486  0.09838955  0.13130421  0.07059691\n",
            "   0.1542357   0.13386491  0.09806375]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00799936]]\n",
            "  Raw Prediction (y_pred): [[-0.09284716  0.19805738 -0.18820133 -0.10298224  0.18229998 -0.43216721\n",
            "   0.34102163  0.20138838 -0.10626885]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.098643   0.13194851 0.08967151 0.09764829 0.12988564 0.07025891\n",
            "  0.15222749 0.13238877 0.09732789]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.098643    0.13194851  0.08967151  0.09764829  0.12988564  0.07025891\n",
            "  -0.84777251  0.13238877  0.09732789]]\n",
            "  Gradient Wy (dWy): [[-0.00078908]\n",
            " [-0.0010555 ]\n",
            " [-0.00071731]\n",
            " [-0.00078112]\n",
            " [-0.001039  ]\n",
            " [-0.00056203]\n",
            " [ 0.00678164]\n",
            " [-0.00105903]\n",
            " [-0.00077856]]\n",
            "  Gradient Wh (dWh): [[-7.10968836e-05]]\n",
            "  Gradient Wx (dWx): [[0.00888782 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00888782]]\n",
            "  Gradient by (dby): [[ 0.098643    0.13194851  0.08967151  0.09764829  0.12988564  0.07025891\n",
            "  -0.84777251  0.13238877  0.09732789]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.0041723]]\n",
            "  Raw Prediction (y_pred): [[-0.10281965  0.18491034 -0.19722338 -0.11274463  0.16932447 -0.43917007\n",
            "   0.42575184  0.18812414 -0.11599628]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09726026 0.12968656 0.0884986  0.09629973 0.12768095 0.06948008\n",
            "  0.16500272 0.13010401 0.0959871 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09726026  0.12968656 -0.9115014   0.09629973  0.12768095  0.06948008\n",
            "   0.16500272  0.13010401  0.0959871 ]]\n",
            "  Gradient Wy (dWy): [[-0.0004058 ]\n",
            " [-0.00054109]\n",
            " [ 0.00380306]\n",
            " [-0.00040179]\n",
            " [-0.00053272]\n",
            " [-0.00028989]\n",
            " [-0.00068844]\n",
            " [-0.00054283]\n",
            " [-0.00040049]]\n",
            "  Gradient Wh (dWh): [[-4.12054534e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.         0.\n",
            "  0.00987596 0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00987596]]\n",
            "  Gradient by (dby): [[ 0.09726026  0.12968656 -0.9115014   0.09629973  0.12768095  0.06948008\n",
            "   0.16500272  0.13010401  0.0959871 ]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00689279]]\n",
            "  Raw Prediction (y_pred): [[-0.1124695   0.17190672 -0.106032   -0.12237706  0.15654613 -0.44613497\n",
            "   0.40928839  0.17513079 -0.12559952]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09656547 0.12832902 0.09718912 0.09561347 0.12637287 0.06916921\n",
            "  0.16271157 0.12874343 0.09530585]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09656547  0.12832902  0.09718912  0.09561347  0.12637287  0.06916921\n",
            "  -0.83728843  0.12874343  0.09530585]]\n",
            "  Gradient Wy (dWy): [[-0.00066561]\n",
            " [-0.00088455]\n",
            " [-0.0006699 ]\n",
            " [-0.00065904]\n",
            " [-0.00087106]\n",
            " [-0.00047677]\n",
            " [ 0.00577125]\n",
            " [-0.0008874 ]\n",
            " [-0.00065692]]\n",
            "  Gradient Wh (dWh): [[-6.36783969e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.00923841 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00923841]]\n",
            "  Gradient by (dby): [[ 0.09656547  0.12832902  0.09718912  0.09561347  0.12637287  0.06916921\n",
            "  -0.83728843  0.12874343  0.09530585]]\n",
            "\n",
            "Epoch 4 Loss: [27.74816254]\n",
            "Epoch 5/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.01083376]]\n",
            "  Raw Prediction (y_pred): [[-0.12262281  0.15930113 -0.11600857 -0.1319228   0.14397505 -0.45294209\n",
            "   0.4927692   0.16214478 -0.13510096]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09510494 0.12607852 0.09573607 0.09422456 0.12416096 0.06835137\n",
            "  0.17598045 0.12643755 0.09392558]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09510494 -0.87392148  0.09573607  0.09422456  0.12416096  0.06835137\n",
            "   0.17598045  0.12643755  0.09392558]]\n",
            "  Gradient Wy (dWy): [[ 0.00103034]\n",
            " [-0.00946785]\n",
            " [ 0.00103718]\n",
            " [ 0.00102081]\n",
            " [ 0.00134513]\n",
            " [ 0.0007405 ]\n",
            " [ 0.00190653]\n",
            " [ 0.00136979]\n",
            " [ 0.00101757]]\n",
            "  Gradient Wh (dWh): [[-0.000189]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.01744552\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01744552]]\n",
            "  Gradient by (dby): [[ 0.09510494 -0.87392148  0.09573607  0.09422456  0.12416096  0.06835137\n",
            "   0.17598045  0.12643755  0.09392558]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01079966]]\n",
            "  Raw Prediction (y_pred): [[-0.1315265   0.2464049  -0.12526717 -0.14136375  0.13147888 -0.45991083\n",
            "   0.47548077  0.14963804 -0.14452854]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09413219 0.13736361 0.09472325 0.09321073 0.12245033 0.0677833\n",
            "  0.17272615 0.12469424 0.09291621]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09413219  0.13736361  0.09472325  0.09321073 -0.87754967  0.0677833\n",
            "   0.17272615  0.12469424  0.09291621]]\n",
            "  Gradient Wy (dWy): [[-0.0010166 ]\n",
            " [-0.00148348]\n",
            " [-0.00102298]\n",
            " [-0.00100664]\n",
            " [ 0.00947724]\n",
            " [-0.00073204]\n",
            " [-0.00186538]\n",
            " [-0.00134666]\n",
            " [-0.00100346]]\n",
            "  Gradient Wh (dWh): [[8.60868498e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.00797126  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00797126]]\n",
            "  Gradient by (dby): [[ 0.09413219  0.13736361  0.09472325  0.09321073 -0.87754967  0.0677833\n",
            "   0.17272615  0.12469424  0.09291621]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00839938]]\n",
            "  Raw Prediction (y_pred): [[-0.14100802  0.23270043 -0.13477543 -0.15068374  0.21925053 -0.46667504\n",
            "   0.45817201  0.13715212 -0.15381724]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09316124 0.13537385 0.09374369 0.09226419 0.13356528 0.06726666\n",
            "  0.16961172 0.12303784 0.09197553]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09316124  0.13537385  0.09374369  0.09226419  0.13356528  0.06726666\n",
            "   0.16961172 -0.87696216  0.09197553]]\n",
            "  Gradient Wy (dWy): [[-0.0007825 ]\n",
            " [-0.00113706]\n",
            " [-0.00078739]\n",
            " [-0.00077496]\n",
            " [-0.00112187]\n",
            " [-0.000565  ]\n",
            " [-0.00142463]\n",
            " [ 0.00736594]\n",
            " [-0.00077254]]\n",
            "  Gradient Wh (dWh): [[-1.69468286e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00201763 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00201763]]\n",
            "  Gradient by (dby): [[ 0.09316124  0.13537385  0.09374369  0.09226419  0.13356528  0.06726666\n",
            "   0.16961172 -0.87696216  0.09197553]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01415666]]\n",
            "  Raw Prediction (y_pred): [[-0.1509546   0.21947937 -0.14447599 -0.15988866  0.20595618 -0.47326079\n",
            "   0.44089209  0.22469654 -0.16297607]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09216025 0.13348149 0.09275925 0.09134055 0.13168855 0.06676792\n",
            "  0.16656332 0.13417971 0.09105898]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09216025  0.13348149  0.09275925  0.09134055 -0.86831145  0.06676792\n",
            "   0.16656332  0.13417971  0.09105898]]\n",
            "  Gradient Wy (dWy): [[ 0.00130468]\n",
            " [ 0.00188965]\n",
            " [ 0.00131316]\n",
            " [ 0.00129308]\n",
            " [-0.01229239]\n",
            " [ 0.00094521]\n",
            " [ 0.00235798]\n",
            " [ 0.00189954]\n",
            " [ 0.00128909]]\n",
            "  Gradient Wh (dWh): [[-0.00010023]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00708026  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00708026]]\n",
            "  Gradient by (dby): [[ 0.09216025  0.13348149  0.09275925  0.09134055 -0.86831145  0.06676792\n",
            "   0.16656332  0.13417971  0.09105898]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00817361]]\n",
            "  Raw Prediction (y_pred): [[-0.15954607  0.20581866 -0.15342857 -0.16904359  0.29271479 -0.48007678\n",
            "   0.42455205  0.21143652 -0.17211989]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09119607 0.13141714 0.09175567 0.09033404 0.14334763 0.0661868\n",
            "  0.16354856 0.1321575  0.09005657]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09119607  0.13141714  0.09175567  0.09033404  0.14334763  0.0661868\n",
            "  -0.83645144  0.1321575   0.09005657]]\n",
            "  Gradient Wy (dWy): [[-0.0007454 ]\n",
            " [-0.00107415]\n",
            " [-0.00074998]\n",
            " [-0.00073836]\n",
            " [-0.00117167]\n",
            " [-0.00054099]\n",
            " [ 0.00683683]\n",
            " [-0.0010802 ]\n",
            " [-0.00073609]]\n",
            "  Gradient Wh (dWh): [[-8.3038884e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.01015938 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01015938]]\n",
            "  Gradient by (dby): [[ 0.09119607  0.13141714  0.09175567  0.09033404  0.14334763  0.0661868\n",
            "  -0.83645144  0.1321575   0.09005657]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00503909]]\n",
            "  Raw Prediction (y_pred): [[-0.16875398  0.1927199  -0.16265017 -0.17807469  0.27839206 -0.48667638\n",
            "   0.50815578  0.19819768 -0.18112085]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08990691 0.1290563  0.09045737 0.08907281 0.14060027 0.0654216\n",
            "  0.17691769 0.12976518 0.08880189]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08990691  0.1290563   0.09045737  0.08907281  0.14060027  0.0654216\n",
            "   0.17691769  0.12976518 -0.91119811]]\n",
            "  Gradient Wy (dWy): [[-0.00045305]\n",
            " [-0.00065033]\n",
            " [-0.00045582]\n",
            " [-0.00044885]\n",
            " [-0.0007085 ]\n",
            " [-0.00032967]\n",
            " [-0.0008915 ]\n",
            " [-0.0006539 ]\n",
            " [ 0.00459161]]\n",
            "  Gradient Wh (dWh): [[3.05424974e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00606111  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00606111]]\n",
            "  Gradient by (dby): [[ 0.08990691  0.1290563   0.09045737  0.08907281  0.14060027  0.0654216\n",
            "   0.17691769  0.12976518 -0.91119811]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[-0.00018113]]\n",
            "  Raw Prediction (y_pred): [[-0.17788059  0.1798822  -0.1717663  -0.18697748  0.26435216 -0.4931883\n",
            "   0.49039113  0.18518673 -0.08999275]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0893383  0.12776506 0.08988622 0.08852929 0.13902629 0.06517804\n",
            "  0.17428677 0.1284446  0.09754543]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0893383   0.12776506  0.08988622 -0.91147071  0.13902629  0.06517804\n",
            "   0.17428677  0.1284446   0.09754543]]\n",
            "  Gradient Wy (dWy): [[-1.61821650e-05]\n",
            " [-2.31425403e-05]\n",
            " [-1.62814106e-05]\n",
            " [ 1.65097934e-04]\n",
            " [-2.51823274e-05]\n",
            " [-1.18059307e-05]\n",
            " [-3.15691832e-05]\n",
            " [-2.32656267e-05]\n",
            " [-1.76687501e-05]]\n",
            "  Gradient Wh (dWh): [[9.54518366e-07]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00526969]]\n",
            "  Gradient bh (dbh): [[-0.00526969]]\n",
            "  Gradient by (dby): [[ 0.0893383   0.12776506  0.08988622 -0.91147071  0.13902629  0.06517804\n",
            "   0.17428677  0.1284446   0.09754543]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01018717]]\n",
            "  Raw Prediction (y_pred): [[-0.18653495  0.16696509 -0.18061042 -0.09583996  0.25040732 -0.49976876\n",
            "   0.4731116   0.17241249 -0.09975963]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08880563 0.12646304 0.08933332 0.09723639 0.13746816 0.06492393\n",
            "  0.17175973 0.12715381 0.096856  ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08880563  0.12646304  0.08933332  0.09723639  0.13746816  0.06492393\n",
            "   0.17175973 -0.87284619  0.096856  ]]\n",
            "  Gradient Wy (dWy): [[-0.00090468]\n",
            " [-0.0012883 ]\n",
            " [-0.00091005]\n",
            " [-0.00099056]\n",
            " [-0.00140041]\n",
            " [-0.00066139]\n",
            " [-0.00174975]\n",
            " [ 0.00889183]\n",
            " [-0.00098669]]\n",
            "  Gradient Wh (dWh): [[-2.82246244e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.00277061 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00277061]]\n",
            "  Gradient by (dby): [[ 0.08880563  0.12646304  0.08933332  0.09723639  0.13746816  0.06492393\n",
            "   0.17175973 -0.87284619  0.096856  ]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01541798]]\n",
            "  Raw Prediction (y_pred): [[-0.19612927  0.15468058 -0.18991211 -0.10553762  0.23677067 -0.50609979\n",
            "   0.45555667  0.2595037  -0.10941214]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08785031 0.12476653 0.08839819 0.09618044 0.13544076 0.06443544\n",
            "  0.16856481 0.138555   0.09580851]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08785031 -0.87523347  0.08839819  0.09618044  0.13544076  0.06443544\n",
            "   0.16856481  0.138555    0.09580851]]\n",
            "  Gradient Wy (dWy): [[ 0.00135447]\n",
            " [-0.01349433]\n",
            " [ 0.00136292]\n",
            " [ 0.00148291]\n",
            " [ 0.00208822]\n",
            " [ 0.00099346]\n",
            " [ 0.00259893]\n",
            " [ 0.00213624]\n",
            " [ 0.00147717]]\n",
            "  Gradient Wh (dWh): [[-0.00028482]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.01847322  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01847322]]\n",
            "  Gradient by (dby): [[ 0.08785031 -0.87523347  0.08839819  0.09618044  0.13544076  0.06443544\n",
            "   0.16856481  0.138555    0.09580851]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00702589]]\n",
            "  Raw Prediction (y_pred): [[-0.20428852  0.24187617 -0.19842891 -0.11517828  0.22313025 -0.51268467\n",
            "   0.43903262  0.24582717 -0.11902184]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08703526 0.13597594 0.08754675 0.09514705 0.13345069 0.06393821\n",
            "  0.1656098  0.13651425 0.09478204]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.91296474  0.13597594  0.08754675  0.09514705  0.13345069  0.06393821\n",
            "   0.1656098   0.13651425  0.09478204]]\n",
            "  Gradient Wy (dWy): [[ 0.00641439]\n",
            " [-0.00095535]\n",
            " [-0.00061509]\n",
            " [-0.00066849]\n",
            " [-0.00093761]\n",
            " [-0.00044922]\n",
            " [-0.00116356]\n",
            " [-0.00095913]\n",
            " [-0.00066593]]\n",
            "  Gradient Wh (dWh): [[-0.00016821]]\n",
            "  Gradient Wx (dWx): [[0.         0.02394186 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02394186]]\n",
            "  Gradient by (dby): [[-0.91296474  0.13597594  0.08754675  0.09514705  0.13345069  0.06393821\n",
            "   0.1656098   0.13651425  0.09478204]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00934127]]\n",
            "  Raw Prediction (y_pred): [[-0.11292128  0.22824172 -0.20715061 -0.1246957   0.20977469 -0.51909333\n",
            "   0.42250526  0.23219365 -0.1285034 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.095631   0.1345129  0.08703129 0.09451161 0.13205164 0.06370895\n",
            "  0.16335465 0.13504553 0.09415242]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.095631    0.1345129   0.08703129  0.09451161  0.13205164  0.06370895\n",
            "  -0.83664535  0.13504553  0.09415242]]\n",
            "  Gradient Wy (dWy): [[-0.00089332]\n",
            " [-0.00125652]\n",
            " [-0.00081298]\n",
            " [-0.00088286]\n",
            " [-0.00123353]\n",
            " [-0.00059512]\n",
            " [ 0.00781533]\n",
            " [-0.0012615 ]\n",
            " [-0.0008795 ]]\n",
            "  Gradient Wh (dWh): [[-9.93817961e-05]]\n",
            "  Gradient Wx (dWx): [[0.010639 0.       0.       0.       0.       0.       0.       0.\n",
            "  0.      ]]\n",
            "  Gradient bh (dbh): [[0.010639]]\n",
            "  Gradient by (dby): [[ 0.095631    0.1345129   0.08703129  0.09451161  0.13205164  0.06370895\n",
            "  -0.83664535  0.13504553  0.09415242]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00518456]]\n",
            "  Raw Prediction (y_pred): [[-0.1226038   0.21485473 -0.21591412 -0.13414328  0.19658653 -0.52543845\n",
            "   0.50611202  0.21865509 -0.1379139 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09423312 0.13205656 0.08583797 0.09315197 0.12966602 0.06298755\n",
            "  0.17670604 0.13255937 0.09280139]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09423312  0.13205656 -0.91416203  0.09315197  0.12966602  0.06298755\n",
            "   0.17670604  0.13255937  0.09280139]]\n",
            "  Gradient Wy (dWy): [[-0.00048856]\n",
            " [-0.00068466]\n",
            " [ 0.00473953]\n",
            " [-0.00048295]\n",
            " [-0.00067226]\n",
            " [-0.00032656]\n",
            " [-0.00091614]\n",
            " [-0.00068726]\n",
            " [-0.00048113]]\n",
            "  Gradient Wh (dWh): [[-5.12159134e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.         0.\n",
            "  0.00987854 0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00987854]]\n",
            "  Gradient by (dby): [[ 0.09423312  0.13205656 -0.91416203  0.09315197  0.12966602  0.06298755\n",
            "   0.17670604  0.13255937  0.09280139]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00844826]]\n",
            "  Raw Prediction (y_pred): [[-0.13193442  0.20159709 -0.1244471  -0.14346235  0.18360511 -0.53175815\n",
            "   0.48849174  0.20542435 -0.14719882]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09364003 0.13071109 0.09434378 0.09256675 0.12838037 0.06277986\n",
            "  0.17414428 0.13121232 0.09222153]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09364003  0.13071109  0.09434378  0.09256675  0.12838037  0.06277986\n",
            "  -0.82585572  0.13121232  0.09222153]]\n",
            "  Gradient Wy (dWy): [[-0.0007911 ]\n",
            " [-0.00110428]\n",
            " [-0.00079704]\n",
            " [-0.00078203]\n",
            " [-0.00108459]\n",
            " [-0.00053038]\n",
            " [ 0.00697704]\n",
            " [-0.00110852]\n",
            " [-0.00077911]]\n",
            "  Gradient Wh (dWh): [[-9.31661412e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.01102785 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01102785]]\n",
            "  Gradient by (dby): [[ 0.09364003  0.13071109  0.09434378  0.09256675  0.12838037  0.06277986\n",
            "  -0.82585572  0.13121232  0.09222153]]\n",
            "\n",
            "Epoch 5 Loss: [27.55000119]\n",
            "Epoch 6/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.01176481]]\n",
            "  Raw Prediction (y_pred): [[-0.14187313  0.18884705 -0.13418005 -0.15269569  0.17085796 -0.53790684\n",
            "   0.57075451  0.19214615 -0.15639203]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09216109 0.1282855  0.09287283 0.09116905 0.12599839 0.06202294\n",
            "  0.18794807 0.12870943 0.09083268]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09216109 -0.8717145   0.09287283  0.09116905  0.12599839  0.06202294\n",
            "   0.18794807  0.12870943  0.09083268]]\n",
            "  Gradient Wy (dWy): [[ 0.00108426]\n",
            " [-0.01025556]\n",
            " [ 0.00109263]\n",
            " [ 0.00107259]\n",
            " [ 0.00148235]\n",
            " [ 0.00072969]\n",
            " [ 0.00221117]\n",
            " [ 0.00151424]\n",
            " [ 0.00106863]]\n",
            "  Gradient Wh (dWh): [[-0.00024363]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.02070827\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.02070827]]\n",
            "  Gradient by (dby): [[ 0.09216109 -0.8717145   0.09287283  0.09116905  0.12599839  0.06202294\n",
            "   0.18794807  0.12870943  0.09083268]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01288523]]\n",
            "  Raw Prediction (y_pred): [[-0.1503878   0.2756126  -0.14310264 -0.16184047  0.15814807 -0.54426642\n",
            "   0.5523634   0.17946744 -0.16551002]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09125777 0.13972672 0.09192503 0.09021859 0.1242411  0.06154752\n",
            "  0.18427687 0.12691828 0.08988813]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09125777  0.13972672  0.09192503  0.09021859 -0.8757589   0.06154752\n",
            "   0.18427687  0.12691828  0.08988813]]\n",
            "  Gradient Wy (dWy): [[-0.00117588]\n",
            " [-0.00180041]\n",
            " [-0.00118448]\n",
            " [-0.00116249]\n",
            " [ 0.01128436]\n",
            " [-0.00079305]\n",
            " [-0.00237445]\n",
            " [-0.00163537]\n",
            " [-0.00115823]]\n",
            "  Gradient Wh (dWh): [[0.00011429]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.00886994  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00886994]]\n",
            "  Gradient by (dby): [[ 0.09125777  0.13972672  0.09192503  0.09021859 -0.8757589   0.06154752\n",
            "   0.18427687  0.12691828  0.08988813]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.0100089]]\n",
            "  Raw Prediction (y_pred): [[-0.15959675  0.2616869  -0.15233903 -0.17086039  0.24574789 -0.55040371\n",
            "   0.53388593  0.16675134 -0.17449608]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09034966 0.13768534 0.09100778 0.08933771 0.13550817 0.06112252\n",
            "  0.18075996 0.12521539 0.08901349]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09034966  0.13768534  0.09100778  0.08933771  0.13550817  0.06112252\n",
            "   0.18075996 -0.87478461  0.08901349]]\n",
            "  Gradient Wy (dWy): [[-0.0009043 ]\n",
            " [-0.00137808]\n",
            " [-0.00091089]\n",
            " [-0.00089417]\n",
            " [-0.00135629]\n",
            " [-0.00061177]\n",
            " [-0.00180921]\n",
            " [ 0.00875563]\n",
            " [-0.00089093]]\n",
            "  Gradient Wh (dWh): [[-3.33541448e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00333245 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00333245]]\n",
            "  Gradient by (dby): [[ 0.09034966  0.13768534  0.09100778  0.08933771  0.13550817  0.06112252\n",
            "   0.18075996 -0.87478461  0.08901349]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01618686]]\n",
            "  Raw Prediction (y_pred): [[-0.16937395  0.24836949 -0.16182416 -0.17976138  0.23228481 -0.55634666\n",
            "   0.5153873   0.25401373 -0.1833574 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08939966 0.13575616 0.09007716 0.08847583 0.13359003 0.06071217\n",
            "  0.17730619 0.13652456 0.08815824]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08939966  0.13575616  0.09007716  0.08847583 -0.86640997  0.06071217\n",
            "   0.17730619  0.13652456  0.08815824]]\n",
            "  Gradient Wy (dWy): [[ 0.0014471 ]\n",
            " [ 0.00219747]\n",
            " [ 0.00145807]\n",
            " [ 0.00143215]\n",
            " [-0.01402446]\n",
            " [ 0.00098274]\n",
            " [ 0.00287003]\n",
            " [ 0.0022099 ]\n",
            " [ 0.00142701]]\n",
            "  Gradient Wh (dWh): [[-0.00012663]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00782289  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00782289]]\n",
            "  Gradient by (dby): [[ 0.08939966  0.13575616  0.09007716  0.08847583 -0.86640997  0.06071217\n",
            "   0.17730619  0.13652456  0.08815824]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00998177]]\n",
            "  Raw Prediction (y_pred): [[-0.17757191  0.23434403 -0.17044738 -0.18864117  0.31882281 -0.56258664\n",
            "   0.49807993  0.24058808 -0.19221268]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08850324 0.13361401 0.08913604 0.08752898 0.14539205 0.0602212\n",
            "  0.17393663 0.13445091 0.08721693]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08850324  0.13361401  0.08913604  0.08752898  0.14539205  0.0602212\n",
            "  -0.82606337  0.13445091  0.08721693]]\n",
            "  Gradient Wy (dWy): [[-0.00088342]\n",
            " [-0.0013337 ]\n",
            " [-0.00088974]\n",
            " [-0.00087369]\n",
            " [-0.00145127]\n",
            " [-0.00060111]\n",
            " [ 0.00824558]\n",
            " [-0.00134206]\n",
            " [-0.00087058]]\n",
            "  Gradient Wh (dWh): [[-0.00012051]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.01207277 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01207277]]\n",
            "  Gradient by (dby): [[ 0.08850324  0.13361401  0.08913604  0.08752898  0.14539205  0.0602212\n",
            "  -0.82606337  0.13445091  0.08721693]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00606202]]\n",
            "  Raw Prediction (y_pred): [[-0.18653426  0.22104867 -0.17941947 -0.19739012  0.30430155 -0.56858408\n",
            "   0.58062717  0.22710766 -0.20092934]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08720423 0.13108367 0.08782689 0.08626268 0.14246391 0.05951349\n",
            "  0.1878069  0.13188032 0.08595791]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08720423  0.13108367  0.08782689  0.08626268  0.14246391  0.05951349\n",
            "   0.1878069   0.13188032 -0.91404209]]\n",
            "  Gradient Wy (dWy): [[-0.00052863]\n",
            " [-0.00079463]\n",
            " [-0.00053241]\n",
            " [-0.00052293]\n",
            " [-0.00086362]\n",
            " [-0.00036077]\n",
            " [-0.00113849]\n",
            " [-0.00079946]\n",
            " [ 0.00554094]]\n",
            "  Gradient Wh (dWh): [[3.67449187e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.         0.         0.         0.         0.         0.\n",
            "  -0.0060615  0.         0.       ]]\n",
            "  Gradient bh (dbh): [[-0.0060615]]\n",
            "  Gradient by (dby): [[ 0.08720423  0.13108367  0.08782689  0.08626268  0.14246391  0.05951349\n",
            "   0.1878069   0.13188032 -0.91404209]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[-0.00029737]]\n",
            "  Raw Prediction (y_pred): [[-0.19541817  0.20803936 -0.18828688 -0.20600932  0.29008366 -0.57449827\n",
            "   0.56174743  0.21386963 -0.10951628]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08672453 0.1298259  0.0873452  0.08581087 0.14092652 0.05936214\n",
            "  0.18491614 0.13058503 0.09450367]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08672453  0.1298259   0.0873452  -0.91418913  0.14092652  0.05936214\n",
            "   0.18491614  0.13058503  0.09450367]]\n",
            "  Gradient Wy (dWy): [[-2.57891884e-05]\n",
            " [-3.86061995e-05]\n",
            " [-2.59737557e-05]\n",
            " [ 2.71851519e-04]\n",
            " [-4.19071803e-05]\n",
            " [-1.76524616e-05]\n",
            " [-5.49883283e-05]\n",
            " [-3.88319417e-05]\n",
            " [-2.81024633e-05]]\n",
            "  Gradient Wh (dWh): [[1.71163984e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00575595]]\n",
            "  Gradient bh (dbh): [[-0.00575595]]\n",
            "  Gradient by (dby): [[ 0.08672453  0.1298259   0.0873452  -0.91418913  0.14092652  0.05936214\n",
            "   0.18491614  0.13058503  0.09450367]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01106093]]\n",
            "  Raw Prediction (y_pred): [[-0.203786    0.19487087 -0.19686384 -0.1146039   0.27593677 -0.5805043\n",
            "   0.54343941  0.20090354 -0.11897693]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08627931 0.12854084 0.08687862 0.09432742 0.13939513 0.05919704\n",
            "  0.18214721 0.12931863 0.09391582]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08627931  0.12854084  0.08687862  0.09432742  0.13939513  0.05919704\n",
            "   0.18214721 -0.87068137  0.09391582]]\n",
            "  Gradient Wy (dWy): [[-0.00095433]\n",
            " [-0.00142178]\n",
            " [-0.00096096]\n",
            " [-0.00104335]\n",
            " [-0.00154184]\n",
            " [-0.00065477]\n",
            " [-0.00201472]\n",
            " [ 0.00963054]\n",
            " [-0.0010388 ]]\n",
            "  Gradient Wh (dWh): [[-4.61136765e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.00416906 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00416906]]\n",
            "  Gradient by (dby): [[ 0.08627931  0.12854084  0.08687862  0.09432742  0.13939513  0.05919704\n",
            "   0.18214721 -0.87068137  0.09391582]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01731162]]\n",
            "  Raw Prediction (y_pred): [[-0.21321524  0.18250927 -0.20596537 -0.12399923  0.2621429  -0.58623884\n",
            "   0.52474424  0.28771141 -0.12833961]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08536949 0.12681297 0.08599066 0.0933359  0.13732453 0.05878961\n",
            "  0.17856421 0.14088098 0.09293166]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08536949 -0.87318703  0.08599066  0.0933359   0.13732453  0.05878961\n",
            "   0.17856421  0.14088098  0.09293166]]\n",
            "  Gradient Wy (dWy): [[ 0.00147788]\n",
            " [-0.01511628]\n",
            " [ 0.00148864]\n",
            " [ 0.0016158 ]\n",
            " [ 0.00237731]\n",
            " [ 0.00101774]\n",
            " [ 0.00309124]\n",
            " [ 0.00243888]\n",
            " [ 0.0016088 ]]\n",
            "  Gradient Wh (dWh): [[-0.00037892]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.02188796  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.02188796]]\n",
            "  Gradient by (dby): [[ 0.08536949 -0.87318703  0.08599066  0.0933359   0.13732453  0.05878961\n",
            "   0.17856421  0.14088098  0.09293166]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00893226]]\n",
            "  Raw Prediction (y_pred): [[-0.22101066  0.26935748 -0.21418146 -0.13336705  0.24827628 -0.59228883\n",
            "   0.50733292  0.27387608 -0.13765914]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08460983 0.13816086 0.08518963 0.09236001 0.13527875 0.05836826\n",
            "  0.17528165 0.13878656 0.09196444]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.91539017  0.13816086  0.08518963  0.09236001  0.13527875  0.05836826\n",
            "   0.17528165  0.13878656  0.09196444]]\n",
            "  Gradient Wy (dWy): [[ 0.0081765 ]\n",
            " [-0.00123409]\n",
            " [-0.00076094]\n",
            " [-0.00082498]\n",
            " [-0.00120835]\n",
            " [-0.00052136]\n",
            " [-0.00156566]\n",
            " [-0.00123968]\n",
            " [-0.00082145]]\n",
            "  Gradient Wh (dWh): [[-0.00021605]]\n",
            "  Gradient Wx (dWx): [[0.         0.02418766 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02418766]]\n",
            "  Gradient by (dby): [[-0.91539017  0.13816086  0.08518963  0.09236001  0.13527875  0.05836826\n",
            "   0.17528165  0.13878656  0.09196444]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01082032]]\n",
            "  Raw Prediction (y_pred): [[-0.12940926  0.25550433 -0.22267351 -0.14260621  0.23473774 -0.5981384\n",
            "   0.48983547  0.26001458 -0.14685817]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0930339  0.13671214 0.08474949 0.09181421 0.13390237 0.05822034\n",
            "  0.17281276 0.13733014 0.09142464]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0930339   0.13671214  0.08474949  0.09181421  0.13390237  0.05822034\n",
            "  -0.82718724  0.13733014  0.09142464]]\n",
            "  Gradient Wy (dWy): [[-0.00100666]\n",
            " [-0.00147927]\n",
            " [-0.00091702]\n",
            " [-0.00099346]\n",
            " [-0.00144887]\n",
            " [-0.00062996]\n",
            " [ 0.00895043]\n",
            " [-0.00148596]\n",
            " [-0.00098924]]\n",
            "  Gradient Wh (dWh): [[-0.00013695]]\n",
            "  Gradient Wx (dWx): [[0.01265628 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01265628]]\n",
            "  Gradient by (dby): [[ 0.0930339   0.13671214  0.08474949  0.09181421  0.13390237  0.05822034\n",
            "  -0.82718724  0.13733014  0.09142464]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00618488]]\n",
            "  Raw Prediction (y_pred): [[-0.13884849  0.2419205  -0.23121677 -0.15178231  0.22137014 -0.60393069\n",
            "   0.57248089  0.24623582 -0.1559967 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09162974 0.13409183 0.08354518 0.09045224 0.13136431 0.05755112\n",
            "  0.18662201 0.13467173 0.09007185]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09162974  0.13409183 -0.91645482  0.09045224  0.13136431  0.05755112\n",
            "   0.18662201  0.13467173  0.09007185]]\n",
            "  Gradient Wy (dWy): [[-0.00056672]\n",
            " [-0.00082934]\n",
            " [ 0.00566816]\n",
            " [-0.00055944]\n",
            " [-0.00081247]\n",
            " [-0.00035595]\n",
            " [-0.00115423]\n",
            " [-0.00083293]\n",
            " [-0.00055708]]\n",
            "  Gradient Wh (dWh): [[-6.11232928e-05]]\n",
            "  Gradient Wx (dWx): [[0.        0.        0.        0.        0.        0.        0.0098827\n",
            "  0.        0.       ]]\n",
            "  Gradient bh (dbh): [[0.0098827]]\n",
            "  Gradient by (dby): [[ 0.09162974  0.13409183 -0.91645482  0.09045224  0.13136431  0.05755112\n",
            "   0.18662201  0.13467173  0.09007185]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01017085]]\n",
            "  Raw Prediction (y_pred): [[-0.14789617  0.22843396 -0.13950763 -0.1608336   0.20821207 -0.60971233\n",
            "   0.55388888  0.23280575 -0.16500875]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0911237  0.13276068 0.09189131 0.08995239 0.13010297 0.05742052\n",
            "  0.18382846 0.13334235 0.08957761]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0911237   0.13276068  0.09189131  0.08995239  0.13010297  0.05742052\n",
            "  -0.81617154  0.13334235  0.08957761]]\n",
            "  Gradient Wy (dWy): [[-0.00092681]\n",
            " [-0.00135029]\n",
            " [-0.00093461]\n",
            " [-0.00091489]\n",
            " [-0.00132326]\n",
            " [-0.00058402]\n",
            " [ 0.00830115]\n",
            " [-0.0013562 ]\n",
            " [-0.00091108]]\n",
            "  Gradient Wh (dWh): [[-0.0001331]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.01308646 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01308646]]\n",
            "  Gradient by (dby): [[ 0.0911237   0.13276068  0.09189131  0.08995239  0.13010297  0.05742052\n",
            "  -0.81617154  0.13334235  0.08957761]]\n",
            "\n",
            "Epoch 6 Loss: [27.40018848]\n",
            "Epoch 7/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.01300736]]\n",
            "  Raw Prediction (y_pred): [[-0.15767981  0.2156065  -0.14904531 -0.16979438  0.1953264  -0.61530063\n",
            "   0.63508296  0.21925455 -0.17393902]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0896344  0.13019398 0.0904117  0.08855507 0.12758022 0.05671952\n",
            "  0.19804652 0.1306698  0.0881888 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0896344  -0.86980602  0.0904117   0.08855507  0.12758022  0.05671952\n",
            "   0.19804652  0.1306698   0.0881888 ]]\n",
            "  Gradient Wy (dWy): [[ 0.00116591]\n",
            " [-0.01131388]\n",
            " [ 0.00117602]\n",
            " [ 0.00115187]\n",
            " [ 0.00165948]\n",
            " [ 0.00073777]\n",
            " [ 0.00257606]\n",
            " [ 0.00169967]\n",
            " [ 0.0011471 ]]\n",
            "  Gradient Wh (dWh): [[-0.00031762]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.02441847\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.02441847]]\n",
            "  Gradient by (dby): [[ 0.0896344  -0.86980602  0.0904117   0.08855507  0.12758022  0.05671952\n",
            "   0.19804652  0.1306698   0.0881888 ]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01487563]]\n",
            "  Raw Prediction (y_pred): [[-0.16583513  0.30202895 -0.15766658 -0.17869075  0.1824193  -0.62115716\n",
            "   0.61580124  0.20644945 -0.18279038]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0887932  0.14176548 0.08952148 0.08765901 0.12578379 0.05631653\n",
            "  0.19401713 0.128843   0.08730038]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0887932   0.14176548  0.08952148  0.08765901 -0.87421621  0.05631653\n",
            "   0.19401713  0.128843    0.08730038]]\n",
            "  Gradient Wy (dWy): [[-0.00132085]\n",
            " [-0.00210885]\n",
            " [-0.00133169]\n",
            " [-0.00130398]\n",
            " [ 0.01300451]\n",
            " [-0.00083774]\n",
            " [-0.00288613]\n",
            " [-0.00191662]\n",
            " [-0.00129865]]\n",
            "  Gradient Wh (dWh): [[0.00014747]]\n",
            "  Gradient Wx (dWx): [[ 0.        -0.0099135  0.         0.         0.         0.\n",
            "   0.         0.         0.       ]]\n",
            "  Gradient bh (dbh): [[-0.0099135]]\n",
            "  Gradient by (dby): [[ 0.0887932   0.14176548  0.08952148  0.08765901 -0.87421621  0.05631653\n",
            "   0.19401713  0.128843    0.08730038]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01189788]]\n",
            "  Raw Prediction (y_pred): [[-0.17480248  0.28791107 -0.16666532 -0.187454    0.26987209 -0.6267702\n",
            "   0.59633989  0.19353466 -0.19151865]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08794053 0.13968283 0.08865904 0.08683496 0.13718568 0.05596313\n",
            "  0.19014809 0.12710302 0.08648272]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08794053  0.13968283  0.08865904  0.08683496  0.13718568  0.05596313\n",
            "   0.19014809 -0.87289698  0.08648272]]\n",
            "  Gradient Wy (dWy): [[-0.00104631]\n",
            " [-0.00166193]\n",
            " [-0.00105485]\n",
            " [-0.00103315]\n",
            " [-0.00163222]\n",
            " [-0.00066584]\n",
            " [-0.00226236]\n",
            " [ 0.01038562]\n",
            " [-0.00102896]]\n",
            "  Gradient Wh (dWh): [[-5.65533286e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00475323 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00475323]]\n",
            "  Gradient by (dby): [[ 0.08794053  0.13968283  0.08865904  0.08683496  0.13718568  0.05596313\n",
            "   0.19014809 -0.87289698  0.08648272]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.0186665]]\n",
            "  Raw Prediction (y_pred): [[-0.18447805  0.27458029 -0.17598714 -0.19608843  0.25627786 -0.63216144\n",
            "   0.57676123  0.28052135 -0.20012706]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08703244 0.13773606 0.08777457 0.0860278  0.13523808 0.05562304\n",
            "  0.18633016 0.13855679 0.08568107]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08703244  0.13773606  0.08777457  0.0860278  -0.86476192  0.05562304\n",
            "   0.18633016  0.13855679  0.08568107]]\n",
            "  Gradient Wy (dWy): [[ 0.00162459]\n",
            " [ 0.00257105]\n",
            " [ 0.00163844]\n",
            " [ 0.00160584]\n",
            " [-0.01614208]\n",
            " [ 0.00103829]\n",
            " [ 0.00347813]\n",
            " [ 0.00258637]\n",
            " [ 0.00159937]]\n",
            "  Gradient Wh (dWh): [[-0.00016277]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00871976  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00871976]]\n",
            "  Gradient by (dby): [[ 0.08703244  0.13773606  0.08777457  0.0860278  -0.86476192  0.05562304\n",
            "   0.18633016  0.13855679  0.08568107]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01207407]]\n",
            "  Raw Prediction (y_pred): [[-0.19229399  0.26016662 -0.18430533 -0.20473985  0.34260755 -0.63792954\n",
            "   0.55869681  0.26698598 -0.20873456]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0861976  0.13551779 0.08688896 0.08513144 0.14716345 0.05520242\n",
            "  0.18266121 0.13644509 0.08479205]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0861976   0.13551779  0.08688896  0.08513144  0.14716345  0.05520242\n",
            "  -0.81733879  0.13644509  0.08479205]]\n",
            "  Gradient Wy (dWy): [[-0.00104076]\n",
            " [-0.00163625]\n",
            " [-0.0010491 ]\n",
            " [-0.00102788]\n",
            " [-0.00177686]\n",
            " [-0.00066652]\n",
            " [ 0.00986861]\n",
            " [-0.00164745]\n",
            " [-0.00102379]]\n",
            "  Gradient Wh (dWh): [[-0.00017243]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.01428121 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01428121]]\n",
            "  Gradient by (dby): [[ 0.0861976   0.13551779  0.08688896  0.08513144  0.14716345  0.05520242\n",
            "  -0.81733879  0.13644509  0.08479205]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.0070655]]\n",
            "  Raw Prediction (y_pred): [[-0.20105955  0.24671719 -0.1930703  -0.21324629  0.32791873 -0.64341704\n",
            "   0.64034396  0.25328733 -0.21720856]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08489541 0.13284682 0.08557637 0.08386709 0.14408426 0.05454699\n",
            "  0.19692511 0.13372251 0.08353544]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08489541  0.13284682  0.08557637  0.08386709  0.14408426  0.05454699\n",
            "   0.19692511  0.13372251 -0.91646456]]\n",
            "  Gradient Wy (dWy): [[-0.00059983]\n",
            " [-0.00093863]\n",
            " [-0.00060464]\n",
            " [-0.00059256]\n",
            " [-0.00101803]\n",
            " [-0.0003854 ]\n",
            " [-0.00139138]\n",
            " [-0.00094482]\n",
            " [ 0.00647528]]\n",
            "  Gradient Wh (dWh): [[4.28072569e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00605863  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00605863]]\n",
            "  Gradient by (dby): [[ 0.08489541  0.13284682  0.08557637  0.08386709  0.14408426  0.05454699\n",
            "   0.19692511  0.13372251 -0.91646456]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[-0.00034601]]\n",
            "  Raw Prediction (y_pred): [[-0.20974302  0.23357244 -0.20172831 -0.22162235  0.31355007 -0.64882674\n",
            "   0.62051906  0.23984508 -0.12555323]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08448996 0.13162384 0.08516984 0.08349221 0.14258322 0.05446449\n",
            "  0.1938132  0.13245207 0.09191116]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08448996  0.13162384  0.08516984 -0.91650779  0.14258322  0.05446449\n",
            "   0.1938132   0.13245207  0.09191116]]\n",
            "  Gradient Wy (dWy): [[-2.92347761e-05]\n",
            " [-4.55437965e-05]\n",
            " [-2.94700259e-05]\n",
            " [ 3.17125252e-04]\n",
            " [-4.93359023e-05]\n",
            " [-1.88455205e-05]\n",
            " [-6.70622338e-05]\n",
            " [-4.58303745e-05]\n",
            " [-3.18026225e-05]]\n",
            "  Gradient Wh (dWh): [[2.18937341e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.         0.         0.         0.         0.         0.\n",
            "   0.         0.        -0.0063274]]\n",
            "  Gradient bh (dbh): [[-0.0063274]]\n",
            "  Gradient by (dby): [[ 0.08448996  0.13162384  0.08516984 -0.91650779  0.14258322  0.05446449\n",
            "   0.1938132   0.13245207  0.09191116]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01204669]]\n",
            "  Raw Prediction (y_pred): [[-0.21785509  0.22016519 -0.21007129 -0.12999046  0.29922119 -0.65435204\n",
            "   0.60136648  0.22672054 -0.13475187]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08411901 0.13035387 0.08477633 0.09184452 0.14107742 0.05436582\n",
            "  0.19084358 0.13121119 0.09140825]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08411901  0.13035387  0.08477633  0.09184452  0.14107742  0.05436582\n",
            "   0.19084358 -0.86878881  0.09140825]]\n",
            "  Gradient Wy (dWy): [[-0.00101336]\n",
            " [-0.00157033]\n",
            " [-0.00102127]\n",
            " [-0.00110642]\n",
            " [-0.00169952]\n",
            " [-0.00065493]\n",
            " [-0.00229903]\n",
            " [ 0.01046603]\n",
            " [-0.00110117]]\n",
            "  Gradient Wh (dWh): [[-6.86323089e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.00569719 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00569719]]\n",
            "  Gradient by (dby): [[ 0.08411901  0.13035387  0.08477633  0.09184452  0.14107742  0.05436582\n",
            "   0.19084358 -0.86878881  0.09140825]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.01965086]]\n",
            "  Raw Prediction (y_pred): [[-0.22717776  0.20779623 -0.2190183  -0.13912154  0.28530793 -0.65957373\n",
            "   0.58166698  0.31325197 -0.14387017]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08324671 0.12860975 0.08392874 0.09090953 0.13897504 0.05402315\n",
            "  0.18691489 0.14291333 0.09047886]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08324671 -0.87139025  0.08392874  0.09090953  0.13897504  0.05402315\n",
            "   0.18691489  0.14291333  0.09047886]]\n",
            "  Gradient Wy (dWy): [[ 0.00163587]\n",
            " [-0.01712357]\n",
            " [ 0.00164927]\n",
            " [ 0.00178645]\n",
            " [ 0.00273098]\n",
            " [ 0.0010616 ]\n",
            " [ 0.00367304]\n",
            " [ 0.00280837]\n",
            " [ 0.00177799]]\n",
            "  Gradient Wh (dWh): [[-0.00050643]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.02577126  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.02577126]]\n",
            "  Gradient by (dby): [[ 0.08324671 -0.87139025  0.08392874  0.09090953  0.13897504  0.05402315\n",
            "   0.18691489  0.14291333  0.09047886]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01069686]]\n",
            "  Raw Prediction (y_pred): [[-0.23462987  0.29427708 -0.2269612  -0.14826295  0.27122518 -0.6651814\n",
            "   0.56356571  0.29930837 -0.15293899]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08253672 0.14007111 0.0831721  0.08998205 0.13687914 0.05366128\n",
            "  0.1833577  0.14077763 0.08956227]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.91746328  0.14007111  0.0831721   0.08998205  0.13687914  0.05366128\n",
            "   0.1833577   0.14077763  0.08956227]]\n",
            "  Gradient Wy (dWy): [[ 0.00981397]\n",
            " [-0.00149832]\n",
            " [-0.00088968]\n",
            " [-0.00096253]\n",
            " [-0.00146418]\n",
            " [-0.00057401]\n",
            " [-0.00196135]\n",
            " [-0.00150588]\n",
            " [-0.00095803]]\n",
            "  Gradient Wh (dWh): [[-0.00026248]]\n",
            "  Gradient Wx (dWx): [[0.         0.02453773 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02453773]]\n",
            "  Gradient by (dby): [[-0.91746328  0.14007111  0.0831721   0.08998205  0.13687914  0.05366128\n",
            "   0.1833577   0.14077763  0.08956227]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01245087]]\n",
            "  Raw Prediction (y_pred): [[-0.1428207   0.28022811 -0.23525332 -0.15726507  0.25752505 -0.67056\n",
            "   0.54526203  0.28524915 -0.16189742]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.09081373 0.13863704 0.08279584 0.08951141 0.13552502 0.05357439\n",
            "  0.18070994 0.13933489 0.08909772]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.09081373  0.13863704  0.08279584  0.08951141  0.13552502  0.05357439\n",
            "  -0.81929006  0.13933489  0.08909772]]\n",
            "  Gradient Wy (dWy): [[-0.00113071]\n",
            " [-0.00172615]\n",
            " [-0.00103088]\n",
            " [-0.00111449]\n",
            " [-0.0016874 ]\n",
            " [-0.00066705]\n",
            " [ 0.01020087]\n",
            " [-0.00173484]\n",
            " [-0.00110934]]\n",
            "  Gradient Wh (dWh): [[-0.00018675]]\n",
            "  Gradient Wx (dWx): [[0.01499907 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01499907]]\n",
            "  Gradient by (dby): [[ 0.09081373  0.13863704  0.08279584  0.08951141  0.13552502  0.05357439\n",
            "  -0.81929006  0.13933489  0.08909772]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00716744]]\n",
            "  Raw Prediction (y_pred): [[-0.15206054  0.26648441 -0.24361208 -0.16620833  0.24400343 -0.67588222\n",
            "   0.62709537  0.27125371 -0.17080444]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08940938 0.13587979 0.08158734 0.08815334 0.13285916 0.05295296\n",
            "  0.19487953 0.13652939 0.08774911]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08940938  0.13587979 -0.91841266  0.08815334  0.13285916  0.05295296\n",
            "   0.19487953  0.13652939  0.08774911]]\n",
            "  Gradient Wy (dWy): [[-0.00064084]\n",
            " [-0.00097391]\n",
            " [ 0.00658267]\n",
            " [-0.00063183]\n",
            " [-0.00095226]\n",
            " [-0.00037954]\n",
            " [-0.00139679]\n",
            " [-0.00097857]\n",
            " [-0.00062894]]\n",
            "  Gradient Wh (dWh): [[-7.09162817e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.         0.\n",
            "  0.00989423 0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00989423]]\n",
            "  Gradient by (dby): [[ 0.08940938  0.13587979 -0.91841266  0.08815334  0.13285916  0.05295296\n",
            "   0.19487953  0.13652939  0.08774911]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01208162]]\n",
            "  Raw Prediction (y_pred): [[-0.16085617  0.25278164 -0.15169042 -0.17503305  0.23068569 -0.6812115\n",
            "   0.60770652  0.2576552  -0.17958395]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08897733 0.13456124 0.08979662 0.08772481 0.13162059 0.05288006\n",
            "  0.19189423 0.13521863 0.08732649]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08897733  0.13456124  0.08979662  0.08772481  0.13162059  0.05288006\n",
            "  -0.80810577  0.13521863  0.08732649]]\n",
            "  Gradient Wy (dWy): [[-0.00107499]\n",
            " [-0.00162572]\n",
            " [-0.00108489]\n",
            " [-0.00105986]\n",
            " [-0.00159019]\n",
            " [-0.00063888]\n",
            " [ 0.00976322]\n",
            " [-0.00163366]\n",
            " [-0.00105505]]\n",
            "  Gradient Wh (dWh): [[-0.00018696]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.01547443 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01547443]]\n",
            "  Gradient by (dby): [[ 0.08897733  0.13456124  0.08979662  0.08772481  0.13162059  0.05288006\n",
            "  -0.80810577  0.13521863  0.08732649]]\n",
            "\n",
            "Epoch 7 Loss: [27.28519069]\n",
            "Epoch 8/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.01460609]]\n",
            "  Raw Prediction (y_pred): [[-0.17054396  0.23994749 -0.16107949 -0.18375546  0.21769509 -0.68631549\n",
            "   0.6879592   0.24383633 -0.18829252]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0874827  0.13188528 0.0883146  0.08633452 0.12898293 0.05223065\n",
            "  0.20642645 0.13239916 0.0859437 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0874827  -0.86811472  0.0883146   0.08633452  0.12898293  0.05223065\n",
            "   0.20642645  0.13239916  0.0859437 ]]\n",
            "  Gradient Wy (dWy): [[ 0.00127778]\n",
            " [-0.01267977]\n",
            " [ 0.00128993]\n",
            " [ 0.00126101]\n",
            " [ 0.00188394]\n",
            " [ 0.00076289]\n",
            " [ 0.00301508]\n",
            " [ 0.00193383]\n",
            " [ 0.0012553 ]]\n",
            "  Gradient Wh (dWh): [[-0.00041844]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.02864863\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.02864863]]\n",
            "  Gradient by (dby): [[ 0.0874827  -0.86811472  0.0883146   0.08633452  0.12898293  0.05223065\n",
            "   0.20642645  0.13239916  0.0859437 ]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01675924]]\n",
            "  Raw Prediction (y_pred): [[-0.17836308  0.32600442 -0.16942916 -0.19244715  0.20459618 -0.69175445\n",
            "   0.66799116  0.2309464  -0.19691458]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08669615 0.14356344 0.08747416 0.08548368 0.12715017 0.0518844\n",
            "  0.20210023 0.13054513 0.08510264]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08669615  0.14356344  0.08747416  0.08548368 -0.87284983  0.0518844\n",
            "   0.20210023  0.13054513  0.08510264]]\n",
            "  Gradient Wy (dWy): [[-0.00145296]\n",
            " [-0.00240601]\n",
            " [-0.001466  ]\n",
            " [-0.00143264]\n",
            " [ 0.0146283 ]\n",
            " [-0.00086954]\n",
            " [-0.00338705]\n",
            " [-0.00218784]\n",
            " [-0.00142626]]\n",
            "  Gradient Wh (dWh): [[0.00018689]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.01115134  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01115134]]\n",
            "  Gradient by (dby): [[ 0.08669615  0.14356344  0.08747416  0.08548368 -0.87284983  0.0518844\n",
            "   0.20210023  0.13054513  0.08510264]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01409458]]\n",
            "  Raw Prediction (y_pred): [[-0.18711384  0.31171036 -0.17821973 -0.20099275  0.29191859 -0.69692587\n",
            "   0.64771868  0.21785883 -0.20542466]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08589147 0.14144468 0.0866588  0.08470762 0.13867276 0.05158714\n",
            "  0.19793071 0.12877377 0.08433304]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08589147  0.14144468  0.0866588   0.08470762  0.13867276  0.05158714\n",
            "   0.19793071 -0.87122623  0.08433304]]\n",
            "  Gradient Wy (dWy): [[-0.0012106 ]\n",
            " [-0.0019936 ]\n",
            " [-0.00122142]\n",
            " [-0.00119392]\n",
            " [-0.00195453]\n",
            " [-0.0007271 ]\n",
            " [-0.00278975]\n",
            " [ 0.01227956]\n",
            " [-0.00118864]]\n",
            "  Gradient Wh (dWh): [[-8.91537752e-05]]\n",
            "  Gradient Wx (dWx): [[0.        0.        0.        0.        0.0063254 0.        0.\n",
            "  0.        0.       ]]\n",
            "  Gradient bh (dbh): [[0.0063254]]\n",
            "  Gradient by (dby): [[ 0.08589147  0.14144468  0.0866588   0.08470762  0.13867276  0.05158714\n",
            "   0.19793071 -0.87122623  0.08433304]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.02166489]]\n",
            "  Raw Prediction (y_pred): [[-0.19675661  0.29846018 -0.18742916 -0.2093915   0.27822882 -0.70183503\n",
            "   0.62716962  0.30456044 -0.21382081]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08501576 0.13949844 0.08581245 0.08394835 0.13670455 0.05130346\n",
            "  0.19378763 0.14035202 0.08357734]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08501576  0.13949844  0.08581245  0.08394835 -0.86329545  0.05130346\n",
            "   0.19378763  0.14035202  0.08357734]]\n",
            "  Gradient Wy (dWy): [[ 0.00184186]\n",
            " [ 0.00302222]\n",
            " [ 0.00185912]\n",
            " [ 0.00181873]\n",
            " [-0.0187032 ]\n",
            " [ 0.00111148]\n",
            " [ 0.00419839]\n",
            " [ 0.00304071]\n",
            " [ 0.00181069]]\n",
            "  Gradient Wh (dWh): [[-0.00021278]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.00982127  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00982127]]\n",
            "  Gradient by (dby): [[ 0.08501576  0.13949844  0.08581245  0.08394835 -0.86329545  0.05130346\n",
            "   0.19378763  0.14035202  0.08357734]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01448186]]\n",
            "  Raw Prediction (y_pred): [[-0.20419221  0.2836079  -0.19546002 -0.21785819  0.36434907 -0.70721706\n",
            "   0.60855713  0.29097271 -0.22221517]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08423664 0.13719868 0.08497543 0.08309329 0.14873575 0.05093779\n",
            "  0.18987753 0.13821285 0.08273204]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08423664  0.13719868  0.08497543  0.08309329  0.14873575  0.05093779\n",
            "  -0.81012247  0.13821285  0.08273204]]\n",
            "  Gradient Wy (dWy): [[-0.0012199 ]\n",
            " [-0.00198689]\n",
            " [-0.0012306 ]\n",
            " [-0.00120335]\n",
            " [-0.00215397]\n",
            " [-0.00073767]\n",
            " [ 0.01173208]\n",
            " [-0.00200158]\n",
            " [-0.00119811]]\n",
            "  Gradient Wh (dWh): [[-0.00024404]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.01685173 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01685173]]\n",
            "  Gradient by (dby): [[ 0.08423664  0.13719868  0.08497543  0.08309329  0.14873575  0.05093779\n",
            "  -0.81012247  0.13821285  0.08273204]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00804511]]\n",
            "  Raw Prediction (y_pred): [[-0.21280739  0.27004596 -0.20405728 -0.22615639  0.34951825 -0.71226704\n",
            "   0.68944075  0.27706896 -0.23048352]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08293552 0.13441295 0.0836644  0.08183577 0.14553099 0.05033012\n",
            "  0.20444757 0.13536025 0.08148242]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08293552  0.13441295  0.0836644   0.08183577  0.14553099  0.05033012\n",
            "   0.20444757  0.13536025 -0.91851758]]\n",
            "  Gradient Wy (dWy): [[-0.00066723]\n",
            " [-0.00108137]\n",
            " [-0.00067309]\n",
            " [-0.00065838]\n",
            " [-0.00117081]\n",
            " [-0.00040491]\n",
            " [-0.0016448 ]\n",
            " [-0.00108899]\n",
            " [ 0.00738958]]\n",
            "  Gradient Wh (dWh): [[4.86523515e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00604744  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00604744]]\n",
            "  Gradient by (dby): [[ 0.08293552  0.13441295  0.0836644   0.08183577  0.14553099  0.05033012\n",
            "   0.20444757  0.13536025 -0.91851758]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[-0.00031331]]\n",
            "  Raw Prediction (y_pred): [[-0.22132888  0.2567978  -0.21254138 -0.23432454  0.33502021 -0.71724618\n",
            "   0.66882102  0.26343732 -0.13862361]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08259152 0.13322423 0.08332049 0.08152513 0.14406378 0.05029923\n",
            "  0.20115121 0.13411172 0.08971269]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08259152  0.13322423  0.08332049 -0.91847487  0.14406378  0.05029923\n",
            "   0.20115121  0.13411172  0.08971269]]\n",
            "  Gradient Wy (dWy): [[-2.58765986e-05]\n",
            " [-4.17402427e-05]\n",
            " [-2.61049914e-05]\n",
            " [ 2.87765691e-04]\n",
            " [-4.51363597e-05]\n",
            " [-1.57591592e-05]\n",
            " [-6.30223187e-05]\n",
            " [-4.20182998e-05]\n",
            " [-2.81077210e-05]]\n",
            "  Gradient Wh (dWh): [[2.18818803e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00698414]]\n",
            "  Gradient bh (dbh): [[-0.00698414]]\n",
            "  Gradient by (dby): [[ 0.08259152  0.13322423  0.08332049 -0.91847487  0.14406378  0.05029923\n",
            "   0.20115121  0.13411172  0.08971269]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01315196]]\n",
            "  Raw Prediction (y_pred): [[-0.22921046  0.24315318 -0.22067899 -0.14250318  0.32052078 -0.72236613\n",
            "   0.64899416  0.2501834  -0.14759858]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08228344 0.13196457 0.08298844 0.08973646 0.14257969 0.05025018\n",
            "  0.19802127 0.13289558 0.08928038]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08228344  0.13196457  0.08298844  0.08973646  0.14257969  0.05025018\n",
            "   0.19802127 -0.86710442  0.08928038]]\n",
            "  Gradient Wy (dWy): [[-0.00108219]\n",
            " [-0.00173559]\n",
            " [-0.00109146]\n",
            " [-0.00118021]\n",
            " [-0.0018752 ]\n",
            " [-0.00066089]\n",
            " [-0.00260437]\n",
            " [ 0.01140413]\n",
            " [-0.00117421]]\n",
            "  Gradient Wh (dWh): [[-9.73565446e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.00740243 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00740243]]\n",
            "  Gradient by (dby): [[ 0.08228344  0.13196457  0.08298844  0.08973646  0.14257969  0.05025018\n",
            "   0.19802127 -0.86710442  0.08928038]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.02250325]]\n",
            "  Raw Prediction (y_pred): [[-0.23848496  0.23085543 -0.22951537 -0.15140157  0.30652546 -0.72713964\n",
            "   0.62839733  0.33643146 -0.15651369]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08144051 0.13021842 0.08217428 0.08885059 0.14045445 0.04995977\n",
            "  0.19378612 0.14471832 0.08839753]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08144051 -0.86978158  0.08217428  0.08885059  0.14045445  0.04995977\n",
            "   0.19378612  0.14471832  0.08839753]]\n",
            "  Gradient Wy (dWy): [[ 0.00183268]\n",
            " [-0.01957291]\n",
            " [ 0.00184919]\n",
            " [ 0.00199943]\n",
            " [ 0.00316068]\n",
            " [ 0.00112426]\n",
            " [ 0.00436082]\n",
            " [ 0.00325663]\n",
            " [ 0.00198923]]\n",
            "  Gradient Wh (dWh): [[-0.00067956]]\n",
            "  Gradient Wx (dWx): [[ 0.         0.         0.         0.         0.         0.\n",
            "   0.        -0.0301984  0.       ]]\n",
            "  Gradient bh (dbh): [[-0.0301984]]\n",
            "  Gradient by (dby): [[ 0.08144051 -0.86978158  0.08217428  0.08885059  0.14045445  0.04995977\n",
            "   0.19378612  0.14471832  0.08839753]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01230175]]\n",
            "  Raw Prediction (y_pred): [[-0.24560693  0.31693    -0.2372072  -0.16035915  0.29222511 -0.73238059\n",
            "   0.60979649  0.32242963 -0.16536512]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08077473 0.14176931 0.08145608 0.08796262 0.13830982 0.04964465\n",
            "  0.19000828 0.14255113 0.08752338]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.91922527  0.14176931  0.08145608  0.08796262  0.13830982  0.04964465\n",
            "   0.19000828  0.14255113  0.08752338]]\n",
            "  Gradient Wy (dWy): [[ 0.01130808]\n",
            " [-0.00174401]\n",
            " [-0.00100205]\n",
            " [-0.00108209]\n",
            " [-0.00170145]\n",
            " [-0.00061072]\n",
            " [-0.00233744]\n",
            " [-0.00175363]\n",
            " [-0.00107669]]\n",
            "  Gradient Wh (dWh): [[-0.00030736]]\n",
            "  Gradient Wx (dWx): [[0.         0.02498494 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02498494]]\n",
            "  Gradient by (dby): [[-0.91922527  0.14176931  0.08145608  0.08796262  0.13830982  0.04964465\n",
            "   0.19000828  0.14255113  0.08752338]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01425683]]\n",
            "  Raw Prediction (y_pred): [[-0.15361064  0.30269736 -0.24532448 -0.16916077  0.27837778 -0.73735954\n",
            "   0.59083657  0.30819883 -0.1741194 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08892672 0.14034739 0.08113374 0.0875546  0.13697537 0.04960362\n",
            "  0.18721542 0.14112163 0.08712152]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08892672  0.14034739  0.08113374  0.0875546   0.13697537  0.04960362\n",
            "  -0.81278458  0.14112163  0.08712152]]\n",
            "  Gradient Wy (dWy): [[-0.00126781]\n",
            " [-0.00200091]\n",
            " [-0.00115671]\n",
            " [-0.00124825]\n",
            " [-0.00195283]\n",
            " [-0.00070719]\n",
            " [ 0.01158773]\n",
            " [-0.00201195]\n",
            " [-0.00124208]]\n",
            "  Gradient Wh (dWh): [[-0.00025291]]\n",
            "  Gradient Wx (dWx): [[0.01773943 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01773943]]\n",
            "  Gradient by (dby): [[ 0.08892672  0.14034739  0.08113374  0.0875546   0.13697537  0.04960362\n",
            "  -0.81278458  0.14112163  0.08712152]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00813248]]\n",
            "  Raw Prediction (y_pred): [[-0.16269184  0.28882881 -0.2535314  -0.17790462  0.2647233  -0.74227745\n",
            "   0.6719873   0.29400211 -0.18283064]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08752607 0.13747709 0.07992567 0.08620463 0.13420276 0.04902601\n",
            "  0.20166659 0.13819015 0.08578103]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08752607  0.13747709 -0.92007433  0.08620463  0.13420276  0.04902601\n",
            "   0.20166659  0.13819015  0.08578103]]\n",
            "  Gradient Wy (dWy): [[-0.0007118 ]\n",
            " [-0.00111803]\n",
            " [ 0.00748249]\n",
            " [-0.00070106]\n",
            " [-0.0010914 ]\n",
            " [-0.0003987 ]\n",
            " [-0.00164005]\n",
            " [-0.00112383]\n",
            " [-0.00069761]]\n",
            "  Gradient Wh (dWh): [[-8.06457187e-05]]\n",
            "  Gradient Wx (dWx): [[0.        0.        0.        0.        0.        0.        0.0099165\n",
            "  0.        0.       ]]\n",
            "  Gradient bh (dbh): [[0.0099165]]\n",
            "  Gradient by (dby): [[ 0.08752607  0.13747709 -0.92007433  0.08620463  0.13420276  0.04902601\n",
            "   0.20166659  0.13819015  0.08578103]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01421227]]\n",
            "  Raw Prediction (y_pred): [[-0.1712601   0.2749117  -0.1614221  -0.18653937  0.25125596 -0.74722377\n",
            "   0.65196151  0.28026259 -0.1914124 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08715676 0.13616673 0.08801844 0.08583519 0.13298341 0.04899629\n",
            "  0.19852796 0.1368973  0.08541793]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08715676  0.13616673  0.08801844  0.08583519  0.13298341  0.04899629\n",
            "  -0.80147204  0.1368973   0.08541793]]\n",
            "  Gradient Wy (dWy): [[-0.0012387 ]\n",
            " [-0.00193524]\n",
            " [-0.00125094]\n",
            " [-0.00121991]\n",
            " [-0.00189   ]\n",
            " [-0.00069635]\n",
            " [ 0.01139074]\n",
            " [-0.00194562]\n",
            " [-0.00121398]]\n",
            "  Gradient Wh (dWh): [[-0.00025958]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.01826473 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01826473]]\n",
            "  Gradient by (dby): [[ 0.08715676  0.13616673  0.08801844  0.08583519  0.13298341  0.04899629\n",
            "  -0.80147204  0.1368973   0.08541793]]\n",
            "\n",
            "Epoch 8 Loss: [27.19523216]\n",
            "Epoch 9/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.01660708]]\n",
            "  Raw Prediction (y_pred): [[-0.18091115  0.26215234 -0.17070737 -0.1950513   0.23819481 -0.75190227\n",
            "   0.73136894  0.26616849 -0.1999365 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08565941 0.13341207 0.08653794 0.0844567  0.13025383 0.04839459\n",
            "  0.21329138 0.13394895 0.08404512]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08565941 -0.86658793  0.08653794  0.0844567   0.13025383  0.04839459\n",
            "   0.21329138  0.13394895  0.08404512]]\n",
            "  Gradient Wy (dWy): [[ 0.00142255]\n",
            " [-0.01439149]\n",
            " [ 0.00143714]\n",
            " [ 0.00140258]\n",
            " [ 0.00216314]\n",
            " [ 0.00080369]\n",
            " [ 0.00354215]\n",
            " [ 0.0022245 ]\n",
            " [ 0.00139574]]\n",
            "  Gradient Wh (dWh): [[-0.00055604]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.03348237\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.03348237]]\n",
            "  Gradient by (dby): [[ 0.08565941 -0.86658793  0.08653794  0.0844567   0.13025383  0.04839459\n",
            "   0.21329138  0.13394895  0.08404512]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01852328]]\n",
            "  Raw Prediction (y_pred): [[-0.18841025  0.3478041  -0.17880948 -0.20357796  0.22490001 -0.75699343\n",
            "   0.71090807  0.2532355  -0.20836056]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08492066 0.14517382 0.08573989 0.08364233 0.12838425 0.04809289\n",
            "  0.20872879 0.13207411 0.08324326]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08492066  0.14517382  0.08573989  0.08364233 -0.87161575  0.04809289\n",
            "   0.20872879  0.13207411  0.08324326]]\n",
            "  Gradient Wy (dWy): [[-0.00157301]\n",
            " [-0.0026891 ]\n",
            " [-0.00158818]\n",
            " [-0.00154933]\n",
            " [ 0.01614518]\n",
            " [-0.00089084]\n",
            " [-0.00386634]\n",
            " [-0.00244645]\n",
            " [-0.00154194]]\n",
            "  Gradient Wh (dWh): [[0.00023415]]\n",
            "  Gradient Wx (dWx): [[ 0.         -0.01264112  0.          0.          0.          0.\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01264112]]\n",
            "  Gradient by (dby): [[ 0.08492066  0.14517382  0.08573989  0.08364233 -0.87161575  0.04809289\n",
            "   0.20872879  0.13207411  0.08324326]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01664252]]\n",
            "  Raw Prediction (y_pred): [[-0.19696217  0.33333744 -0.18741578 -0.21194056  0.31210268 -0.76179079\n",
            "   0.68998196  0.23999909 -0.21668653]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08415721 0.14302024 0.08496445 0.08290606 0.14001526 0.0478398\n",
            "  0.20430842 0.13027503 0.08251352]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08415721  0.14302024  0.08496445  0.08290606  0.14001526  0.0478398\n",
            "   0.20430842 -0.86972497  0.08251352]]\n",
            "  Gradient Wy (dWy): [[-0.00140059]\n",
            " [-0.00238022]\n",
            " [-0.00141402]\n",
            " [-0.00137977]\n",
            " [-0.00233021]\n",
            " [-0.00079618]\n",
            " [-0.00340021]\n",
            " [ 0.01447442]\n",
            " [-0.00137323]]\n",
            "  Gradient Wh (dWh): [[-0.00013477]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.00809807 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00809807]]\n",
            "  Gradient by (dby): [[ 0.08415721  0.14302024  0.08496445  0.08290606  0.14001526  0.0478398\n",
            "   0.20430842 -0.86972497  0.08251352]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.02525928]]\n",
            "  Raw Prediction (y_pred): [[-0.20664307  0.32028233 -0.19656286 -0.22012736  0.29835646 -0.76627041\n",
            "   0.66853327  0.32638994 -0.2249074 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0833045  0.14109423 0.08414847 0.08218874 0.13803428 0.04760202\n",
            "  0.19987234 0.14195861 0.08179681]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0833045   0.14109423  0.08414847  0.08218874 -0.86196572  0.04760202\n",
            "   0.19987234  0.14195861  0.08179681]]\n",
            "  Gradient Wy (dWy): [[ 0.00210421]\n",
            " [ 0.00356394]\n",
            " [ 0.00212553]\n",
            " [ 0.00207603]\n",
            " [-0.02177264]\n",
            " [ 0.00120239]\n",
            " [ 0.00504863]\n",
            " [ 0.00358577]\n",
            " [ 0.00206613]]\n",
            "  Gradient Wh (dWh): [[-0.00028255]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.01118601  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01118601]]\n",
            "  Gradient by (dby): [[ 0.0833045   0.14109423  0.08414847  0.08218874 -0.86196572  0.04760202\n",
            "   0.19987234  0.14195861  0.08179681]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.0172511]]\n",
            "  Raw Prediction (y_pred): [[-0.2136887   0.30491001 -0.20431634 -0.22845029  0.38425253 -0.77133867\n",
            "   0.64958163  0.3128148  -0.23311676]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08257622 0.138701   0.0833538  0.08136622 0.15015424 0.04727927\n",
            "  0.19578011 0.13980174 0.08098741]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08257622  0.138701    0.0833538   0.08136622  0.15015424  0.04727927\n",
            "  -0.80421989  0.13980174  0.08098741]]\n",
            "  Gradient Wy (dWy): [[-0.00142453]\n",
            " [-0.00239274]\n",
            " [-0.00143794]\n",
            " [-0.00140366]\n",
            " [-0.00259033]\n",
            " [-0.00081562]\n",
            " [ 0.01387368]\n",
            " [-0.00241173]\n",
            " [-0.00139712]]\n",
            "  Gradient Wh (dWh): [[-0.00034268]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.01986437 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01986437]]\n",
            "  Gradient by (dby): [[ 0.08257622  0.138701    0.0833538   0.08136622  0.15015424  0.04727927\n",
            "  -0.80421989  0.13980174  0.08098741]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.0090023]]\n",
            "  Raw Prediction (y_pred): [[-0.22219795  0.29128107 -0.21278239 -0.236569    0.36930375 -0.77600814\n",
            "   0.72981269  0.29871025 -0.24121201]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0812785  0.13582408 0.08204739 0.08011879 0.14684582 0.04671528\n",
            "  0.21058558 0.1368369  0.07974766]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0812785   0.13582408  0.08204739  0.08011879  0.14684582  0.04671528\n",
            "   0.21058558  0.1368369  -0.92025234]]\n",
            "  Gradient Wy (dWy): [[-0.00073169]\n",
            " [-0.00122273]\n",
            " [-0.00073862]\n",
            " [-0.00072125]\n",
            " [-0.00132195]\n",
            " [-0.00042054]\n",
            " [-0.00189575]\n",
            " [-0.00123185]\n",
            " [ 0.00828439]]\n",
            "  Gradient Wh (dWh): [[5.42475705e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00602597  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00602597]]\n",
            "  Gradient by (dby): [[ 0.0812785   0.13582408  0.08204739  0.08011879  0.14684582  0.04671528\n",
            "   0.21058558  0.1368369  -0.92025234]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[-0.00019]]\n",
            "  Raw Prediction (y_pred): [[-0.230592    0.27796067 -0.22112409 -0.24455918  0.35469512 -0.78061573\n",
            "   0.70852455  0.28489811 -0.14918032]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.080985   0.13466856 0.0817554  0.07986173 0.14540909 0.04672317\n",
            "  0.20713703 0.13560606 0.08785394]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.080985    0.13466856  0.0817554  -0.92013827  0.14540909  0.04672317\n",
            "   0.20713703  0.13560606  0.08785394]]\n",
            "  Gradient Wy (dWy): [[-1.53875225e-05]\n",
            " [-2.55876452e-05]\n",
            " [-1.55339020e-05]\n",
            " [ 1.74830496e-04]\n",
            " [-2.76283957e-05]\n",
            " [-8.87761752e-06]\n",
            " [-3.93569862e-05]\n",
            " [-2.57657750e-05]\n",
            " [-1.66926522e-05]]\n",
            "  Gradient Wh (dWh): [[1.46875683e-06]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.00773011]]\n",
            "  Gradient bh (dbh): [[-0.00773011]]\n",
            "  Gradient by (dby): [[ 0.080985    0.13466856  0.0817554  -0.92013827  0.14540909  0.04672317\n",
            "   0.20713703  0.13560606  0.08785394]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.014393]]\n",
            "  Raw Prediction (y_pred): [[-0.23826254  0.26406971 -0.22907998 -0.15258113  0.34002984 -0.78539172\n",
            "   0.68817806  0.27154271 -0.15796413]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08073022 0.13341241 0.08147494 0.08795227 0.14394126 0.04671119\n",
            "  0.20388447 0.13441314 0.0874801 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08073022  0.13341241  0.08147494  0.08795227  0.14394126  0.04671119\n",
            "   0.20388447 -0.86558686  0.0874801 ]]\n",
            "  Gradient Wy (dWy): [[-0.00116195]\n",
            " [-0.00192021]\n",
            " [-0.00117267]\n",
            " [-0.0012659 ]\n",
            " [-0.00207175]\n",
            " [-0.00067231]\n",
            " [-0.00293451]\n",
            " [ 0.0124584 ]\n",
            " [-0.0012591 ]]\n",
            "  Gradient Wh (dWh): [[-0.00013439]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.00933692 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00933692]]\n",
            "  Gradient by (dby): [[ 0.08073022  0.13341241  0.08147494  0.08795227  0.14394126  0.04671119\n",
            "   0.20388447 -0.86558686  0.0874801 ]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.02594494]]\n",
            "  Raw Prediction (y_pred): [[-0.24754799  0.25193796 -0.23784826 -0.16127145  0.32599432 -0.78976664\n",
            "   0.66675429  0.35748627 -0.16671338]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.07990879 0.13167962 0.08068766 0.08710919 0.1418015  0.04646351\n",
            "  0.19937514 0.14633816 0.08663644]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.07990879 -0.86832038  0.08068766  0.08710919  0.1418015   0.04646351\n",
            "   0.19937514  0.14633816  0.08663644]]\n",
            "  Gradient Wy (dWy): [[ 0.00207323]\n",
            " [-0.02252852]\n",
            " [ 0.00209344]\n",
            " [ 0.00226004]\n",
            " [ 0.00367903]\n",
            " [ 0.00120549]\n",
            " [ 0.00517278]\n",
            " [ 0.00379674]\n",
            " [ 0.00224778]]\n",
            "  Gradient Wh (dWh): [[-0.00091472]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.03525603  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.03525603]]\n",
            "  Gradient by (dby): [[ 0.07990879 -0.86832038  0.08068766  0.08710919  0.1418015   0.04646351\n",
            "   0.19937514  0.14633816  0.08663644]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01372548]]\n",
            "  Raw Prediction (y_pred): [[-0.2543453   0.33754688 -0.2453053  -0.17008423  0.31146362 -0.79470359\n",
            "   0.64783791  0.34348024 -0.1753745 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.07928251 0.14329561 0.08000247 0.08625246 0.13960632 0.04618519\n",
            "  0.1954297  0.14414836 0.08579737]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.92071749  0.14329561  0.08000247  0.08625246  0.13960632  0.04618519\n",
            "   0.1954297   0.14414836  0.08579737]]\n",
            "  Gradient Wy (dWy): [[ 0.01263729]\n",
            " [-0.0019668 ]\n",
            " [-0.00109807]\n",
            " [-0.00118386]\n",
            " [-0.00191616]\n",
            " [-0.00063391]\n",
            " [-0.00268237]\n",
            " [-0.00197851]\n",
            " [-0.00117761]]\n",
            "  Gradient Wh (dWh): [[-0.00035025]]\n",
            "  Gradient Wx (dWx): [[0.        0.0255181 0.        0.        0.        0.        0.\n",
            "  0.        0.       ]]\n",
            "  Gradient bh (dbh): [[0.0255181]]\n",
            "  Gradient by (dby): [[-0.92071749  0.14329561  0.08000247  0.08625246  0.13960632  0.04618519\n",
            "   0.1954297   0.14414836  0.08579737]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01627243]]\n",
            "  Raw Prediction (y_pred): [[-0.16217602  0.32313184 -0.25326771 -0.17871757  0.29747797 -0.79934159\n",
            "   0.628357    0.32910312 -0.18395561]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08732899 0.14188123 0.0797256  0.08589631 0.13828772 0.04617862\n",
            "  0.19252297 0.14273098 0.08544756]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08732899  0.14188123  0.0797256   0.08589631  0.13828772  0.04617862\n",
            "  -0.80747703  0.14273098  0.08544756]]\n",
            "  Gradient Wy (dWy): [[-0.00142106]\n",
            " [-0.00230875]\n",
            " [-0.00129733]\n",
            " [-0.00139774]\n",
            " [-0.00225028]\n",
            " [-0.00075144]\n",
            " [ 0.01313961]\n",
            " [-0.00232258]\n",
            " [-0.00139044]]\n",
            "  Gradient Wh (dWh): [[-0.00034112]]\n",
            "  Gradient Wx (dWx): [[0.02096308 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02096308]]\n",
            "  Gradient by (dby): [[ 0.08732899  0.14188123  0.0797256   0.08589631  0.13828772  0.04617862\n",
            "  -0.80747703  0.14273098  0.08544756]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00908539]]\n",
            "  Raw Prediction (y_pred): [[-0.1711365   0.30917521 -0.26135247 -0.18729022  0.28371031 -0.8039076\n",
            "   0.70893114  0.31471382 -0.1925023 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08593414 0.13891926 0.07852093 0.08455714 0.13542636 0.04564117\n",
            "  0.20719261 0.13969082 0.08411757]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08593414  0.13891926 -0.92147907  0.08455714  0.13542636  0.04564117\n",
            "   0.20719261  0.13969082  0.08411757]]\n",
            "  Gradient Wy (dWy): [[-0.00078075]\n",
            " [-0.00126214]\n",
            " [ 0.008372  ]\n",
            " [-0.00076823]\n",
            " [-0.0012304 ]\n",
            " [-0.00041467]\n",
            " [-0.00188243]\n",
            " [-0.00126915]\n",
            " [-0.00076424]]\n",
            "  Gradient Wh (dWh): [[-9.04019181e-05]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.         0.\n",
            "  0.00995025 0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00995025]]\n",
            "  Gradient by (dby): [[ 0.08593414  0.13891926 -0.92147907  0.08455714  0.13542636  0.04564117\n",
            "   0.20719261  0.13969082  0.08411757]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01660597]]\n",
            "  Raw Prediction (y_pred): [[-0.17949548  0.29503502 -0.16907547 -0.19576735  0.27009785 -0.80852795\n",
            "   0.68841275  0.30086027 -0.20091567]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08561823 0.1376107  0.08651504 0.08423633 0.13422152 0.04564371\n",
            "  0.20393604 0.13841466 0.08380377]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08561823  0.1376107   0.08651504  0.08423633  0.13422152  0.04564371\n",
            "  -0.79606396  0.13841466  0.08380377]]\n",
            "  Gradient Wy (dWy): [[-0.00142177]\n",
            " [-0.00228516]\n",
            " [-0.00143667]\n",
            " [-0.00139883]\n",
            " [-0.00222888]\n",
            " [-0.00075796]\n",
            " [ 0.01321942]\n",
            " [-0.00229851]\n",
            " [-0.00139164]]\n",
            "  Gradient Wh (dWh): [[-0.00035776]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.02154411 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02154411]]\n",
            "  Gradient by (dby): [[ 0.08561823  0.1376107   0.08651504  0.08423633  0.13422152  0.04564371\n",
            "  -0.79606396  0.13841466  0.08380377]]\n",
            "\n",
            "Epoch 9 Loss: [27.12334735]\n",
            "Epoch 10/10\n",
            "\n",
            "Time Step 1/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[0.0190592]]\n",
            "  Raw Prediction (y_pred): [[-0.18916971  0.28245023 -0.17830039 -0.20409007  0.25700577 -0.81282601\n",
            "   0.76703326  0.2864698  -0.20928903]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08411888 0.13480793 0.08503819 0.08287312 0.13142109 0.04508614\n",
            "  0.21886039 0.13535089 0.08244338]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08411888 -0.86519207  0.08503819  0.08287312  0.13142109  0.04508614\n",
            "   0.21886039  0.13535089  0.08244338]]\n",
            "  Gradient Wy (dWy): [[ 0.00160324]\n",
            " [-0.01648987]\n",
            " [ 0.00162076]\n",
            " [ 0.0015795 ]\n",
            " [ 0.00250478]\n",
            " [ 0.00085931]\n",
            " [ 0.0041713 ]\n",
            " [ 0.00257968]\n",
            " [ 0.0015713 ]]\n",
            "  Gradient Wh (dWh): [[-0.00074362]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.         -0.03901656\n",
            "   0.          0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.03901656]]\n",
            "  Gradient by (dby): [[ 0.08411888 -0.86519207  0.08503819  0.08287312  0.13142109  0.04508614\n",
            "   0.21886039  0.13535089  0.08244338]]\n",
            "\n",
            "Time Step 2/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.02015183]]\n",
            "  Raw Prediction (y_pred): [[-0.19635796  0.36763881 -0.18617314 -0.2124877   0.24350176 -0.81762706\n",
            "   0.74626366  0.27353931 -0.21754046]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08342202 0.1466295  0.084276   0.08208723 0.1295118  0.04481949\n",
            "  0.21411942 0.13346103 0.08167351]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08342202  0.1466295   0.084276    0.08208723 -0.8704882   0.04481949\n",
            "   0.21411942  0.13346103  0.08167351]]\n",
            "  Gradient Wy (dWy): [[-0.00168111]\n",
            " [-0.00295485]\n",
            " [-0.00169832]\n",
            " [-0.00165421]\n",
            " [ 0.01754193]\n",
            " [-0.00090319]\n",
            " [-0.0043149 ]\n",
            " [-0.00268948]\n",
            " [-0.00164587]]\n",
            "  Gradient Wh (dWh): [[0.00029121]]\n",
            "  Gradient Wx (dWx): [[ 0.        -0.0144509  0.         0.         0.         0.\n",
            "   0.         0.         0.       ]]\n",
            "  Gradient bh (dbh): [[-0.0144509]]\n",
            "  Gradient by (dby): [[ 0.08342202  0.1466295   0.084276    0.08208723 -0.8704882   0.04481949\n",
            "   0.21411942  0.13346103  0.08167351]]\n",
            "\n",
            "Time Step 3/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01960142]]\n",
            "  Raw Prediction (y_pred): [[-0.20472068  0.35298919 -0.19461297 -0.22069816  0.33058998 -0.8221067\n",
            "   0.72482748  0.26017937 -0.22571098]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08269414 0.14443919 0.08353423 0.0813834  0.14123983 0.04460129\n",
            "  0.20949438 0.13163709 0.08097646]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08269414  0.14443919  0.08353423  0.0813834   0.14123983  0.04460129\n",
            "   0.20949438 -0.86836291  0.08097646]]\n",
            "  Gradient Wy (dWy): [[-0.00162092]\n",
            " [-0.00283121]\n",
            " [-0.00163739]\n",
            " [-0.00159523]\n",
            " [-0.0027685 ]\n",
            " [-0.00087425]\n",
            " [-0.00410639]\n",
            " [ 0.01702115]\n",
            " [-0.00158725]]\n",
            "  Gradient Wh (dWh): [[-0.00019847]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.01012533 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01012533]]\n",
            "  Gradient by (dby): [[ 0.08269414  0.14443919  0.08353423  0.0813834   0.14123983  0.04460129\n",
            "   0.20949438 -0.86836291  0.08097646]]\n",
            "\n",
            "Time Step 4/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.02953724]]\n",
            "  Raw Prediction (y_pred): [[-0.21451431  0.34027505 -0.20374793 -0.22868918  0.31683553 -0.82619538\n",
            "   0.7025023   0.34621477 -0.23379071]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08185542 0.14255728 0.08274147 0.08070331 0.13925466 0.04440151\n",
            "  0.20478715 0.14340655 0.08029265]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08185542  0.14255728  0.08274147  0.08070331 -0.86074534  0.04440151\n",
            "   0.20478715  0.14340655  0.08029265]]\n",
            "  Gradient Wy (dWy): [[ 0.00241778]\n",
            " [ 0.00421075]\n",
            " [ 0.00244395]\n",
            " [ 0.00238375]\n",
            " [-0.02542404]\n",
            " [ 0.0013115 ]\n",
            " [ 0.00604885]\n",
            " [ 0.00423583]\n",
            " [ 0.00237162]]\n",
            "  Gradient Wh (dWh): [[-0.00038052]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.01288269  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.01288269]]\n",
            "  Gradient by (dby): [[ 0.08185542  0.14255728  0.08274147  0.08070331 -0.86074534  0.04440151\n",
            "   0.20478715  0.14340655  0.08029265]]\n",
            "\n",
            "Time Step 5/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.02044302]]\n",
            "  Raw Prediction (y_pred): [[-0.22114782  0.32426288 -0.21122542 -0.23690767  0.4024767  -0.8310124\n",
            "   0.68342707  0.33273132 -0.24183652]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08117444 0.14005162 0.08198389 0.07990517 0.15144536 0.04411217\n",
            "  0.20057239 0.14124267 0.07951229]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08117444  0.14005162  0.08198389  0.07990517  0.15144536  0.04411217\n",
            "  -0.79942761  0.14124267  0.07951229]]\n",
            "  Gradient Wy (dWy): [[-0.00165945]\n",
            " [-0.00286308]\n",
            " [-0.001676  ]\n",
            " [-0.0016335 ]\n",
            " [-0.003096  ]\n",
            " [-0.00090179]\n",
            " [ 0.01634271]\n",
            " [-0.00288743]\n",
            " [-0.00162547]]\n",
            "  Gradient Wh (dWh): [[-0.00047864]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.02341337 0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02341337]]\n",
            "  Gradient by (dby): [[ 0.08117444  0.14005162  0.08198389  0.07990517  0.15144536  0.04411217\n",
            "  -0.79942761  0.14124267  0.07951229]]\n",
            "\n",
            "Time Step 6/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.00994329]]\n",
            "  Raw Prediction (y_pred): [[-0.22959446  0.31062124 -0.21959435 -0.24487016  0.3874359  -0.83534616\n",
            "   0.76308748  0.31842148 -0.24978736]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.0798809  0.13710575 0.08068373 0.07866994 0.14805253 0.04358814\n",
            "  0.21555557 0.13817939 0.07828405]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.0798809   0.13710575  0.08068373  0.07866994  0.14805253  0.04358814\n",
            "   0.21555557  0.13817939 -0.92171595]]\n",
            "  Gradient Wy (dWy): [[-0.00079428]\n",
            " [-0.00136328]\n",
            " [-0.00080226]\n",
            " [-0.00078224]\n",
            " [-0.00147213]\n",
            " [-0.00043341]\n",
            " [-0.00214333]\n",
            " [-0.00137396]\n",
            " [ 0.00916489]]\n",
            "  Gradient Wh (dWh): [[5.96121808e-05]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "  -0.00599522  0.          0.        ]]\n",
            "  Gradient bh (dbh): [[-0.00599522]]\n",
            "  Gradient by (dby): [[ 0.0798809   0.13710575  0.08068373  0.07866994  0.14805253  0.04358814\n",
            "   0.21555557  0.13817939 -0.92171595]]\n",
            "\n",
            "Time Step 7/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "  Hidden State (h_prev): [[2.96960914e-05]]\n",
            "  Raw Prediction (y_pred): [[-0.23789201  0.29726153 -0.22782145 -0.25270736  0.3727352  -0.83962965\n",
            "   0.74123202  0.30443288 -0.15761227]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.07962871 0.13598277 0.08043466 0.07845768 0.14664312 0.04362529\n",
            "  0.21198143 0.13696146 0.08628488]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.07962871  0.13598277  0.08043466 -0.92154232  0.14664312  0.04362529\n",
            "   0.21198143  0.13696146  0.08628488]]\n",
            "  Gradient Wy (dWy): [[ 2.36466136e-06]\n",
            " [ 4.03815683e-06]\n",
            " [ 2.38859512e-06]\n",
            " [-2.73662051e-05]\n",
            " [ 4.35472747e-06]\n",
            " [ 1.29550060e-06]\n",
            " [ 6.29502004e-06]\n",
            " [ 4.06721997e-06]\n",
            " [ 2.56232376e-06]]\n",
            "  Gradient Wh (dWh): [[-2.54587654e-07]]\n",
            "  Gradient Wx (dWx): [[ 0.         0.         0.         0.         0.         0.\n",
            "   0.         0.        -0.0085731]]\n",
            "  Gradient bh (dbh): [[-0.0085731]]\n",
            "  Gradient by (dby): [[ 0.07962871  0.13598277  0.08043466 -0.92154232  0.14664312  0.04362529\n",
            "   0.21198143  0.13696146  0.08628488]]\n",
            "\n",
            "Time Step 8/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01579456]]\n",
            "  Raw Prediction (y_pred): [[-0.24536511  0.2831044  -0.23561432 -0.16060168  0.35790268 -0.84411238\n",
            "   0.72050637  0.29100536 -0.16623184]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.07941936 0.13472176 0.08019754 0.08644476 0.14518516 0.0436409\n",
            "  0.20864069 0.13579041 0.08595942]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.07941936  0.13472176  0.08019754  0.08644476  0.14518516  0.0436409\n",
            "   0.20864069 -0.86420959  0.08595942]]\n",
            "  Gradient Wy (dWy): [[-0.00125439]\n",
            " [-0.00212787]\n",
            " [-0.00126669]\n",
            " [-0.00136536]\n",
            " [-0.00229314]\n",
            " [-0.00068929]\n",
            " [-0.00329539]\n",
            " [ 0.01364981]\n",
            " [-0.00135769]]\n",
            "  Gradient Wh (dWh): [[-0.00018257]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.01155897 0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.01155897]]\n",
            "  Gradient by (dby): [[ 0.07941936  0.13472176  0.08019754  0.08644476  0.14518516  0.0436409\n",
            "   0.20864069 -0.86420959  0.08595942]]\n",
            "\n",
            "Time Step 9/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[0.03006325]]\n",
            "  Raw Prediction (y_pred): [[-0.2547226   0.27125814 -0.24435649 -0.16910137  0.3438785  -0.84812605\n",
            "   0.69828295  0.37660682 -0.17484954]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.07861204 0.13302082 0.07943118 0.08563946 0.14304024 0.04342874\n",
            "  0.20387977 0.14779916 0.0851486 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.07861204 -0.86697918  0.07943118  0.08563946  0.14304024  0.04342874\n",
            "   0.20387977  0.14779916  0.0851486 ]]\n",
            "  Gradient Wy (dWy): [[ 0.00236333]\n",
            " [-0.02606421]\n",
            " [ 0.00238796]\n",
            " [ 0.0025746 ]\n",
            " [ 0.00430025]\n",
            " [ 0.00130561]\n",
            " [ 0.00612929]\n",
            " [ 0.00444332]\n",
            " [ 0.00255984]]\n",
            "  Gradient Wh (dWh): [[-0.00123391]]\n",
            "  Gradient Wx (dWx): [[ 0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.04104387  0.        ]]\n",
            "  Gradient bh (dbh): [[-0.04104387]]\n",
            "  Gradient by (dby): [[ 0.07861204 -0.86697918  0.07943118  0.08563946  0.14304024  0.04342874\n",
            "   0.20387977  0.14779916  0.0851486 ]]\n",
            "\n",
            "Time Step 10/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01494051]]\n",
            "  Raw Prediction (y_pred): [[-0.26119302  0.35631818 -0.25158905 -0.17780567  0.32909222 -0.85281193\n",
            "   0.67923306  0.36265894 -0.18334133]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.07802143 0.14467571 0.07877436 0.08480639 0.14078991 0.04317945\n",
            "  0.19981854 0.14559598 0.08433823]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[-0.92197857  0.14467571  0.07877436  0.08480639  0.14078991  0.04317945\n",
            "   0.19981854  0.14559598  0.08433823]]\n",
            "  Gradient Wy (dWy): [[ 0.01377483]\n",
            " [-0.00216153]\n",
            " [-0.00117693]\n",
            " [-0.00126705]\n",
            " [-0.00210347]\n",
            " [-0.00064512]\n",
            " [-0.00298539]\n",
            " [-0.00217528]\n",
            " [-0.00126006]]\n",
            "  Gradient Wh (dWh): [[-0.00039026]]\n",
            "  Gradient Wx (dWx): [[0.         0.02612117 0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02612117]]\n",
            "  Gradient by (dby): [[-0.92197857  0.14467571  0.07877436  0.08480639  0.14078991  0.04317945\n",
            "   0.19981854  0.14559598  0.08433823]]\n",
            "\n",
            "Time Step 11/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01854273]]\n",
            "  Raw Prediction (y_pred): [[-0.16885773  0.34170923 -0.25941121 -0.18629928  0.31497176 -0.85715822\n",
            "   0.65935425  0.34816297 -0.19177502]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08597961 0.14326229 0.07853597 0.084493   0.13948258 0.04319867\n",
            "  0.19682641 0.14418986 0.0840316 ]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08597961  0.14326229  0.07853597  0.084493    0.13948258  0.04319867\n",
            "  -0.80317359  0.14418986  0.0840316 ]]\n",
            "  Gradient Wy (dWy): [[-0.0015943 ]\n",
            " [-0.00265647]\n",
            " [-0.00145627]\n",
            " [-0.00156673]\n",
            " [-0.00258639]\n",
            " [-0.00080102]\n",
            " [ 0.01489303]\n",
            " [-0.00267367]\n",
            " [-0.00155818]]\n",
            "  Gradient Wh (dWh): [[-0.00045932]]\n",
            "  Gradient Wx (dWx): [[0.02477089 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02477089]]\n",
            "  Gradient by (dby): [[ 0.08597961  0.14326229  0.07853597  0.084493    0.13948258  0.04319867\n",
            "  -0.80317359  0.14418986  0.0840316 ]]\n",
            "\n",
            "Time Step 12/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.01003562]]\n",
            "  Raw Prediction (y_pred): [[-0.17773326  0.32770659 -0.26740095 -0.194724    0.30111142 -0.86141424\n",
            "   0.73943267  0.33358335 -0.20018449]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08459135 0.14022831 0.07733637 0.08316622 0.13654807 0.04269796\n",
            "  0.21166355 0.14105483 0.08271333]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08459135  0.14022831 -0.92266363  0.08316622  0.13654807  0.04269796\n",
            "   0.21166355  0.14105483  0.08271333]]\n",
            "  Gradient Wy (dWy): [[-0.00084893]\n",
            " [-0.00140728]\n",
            " [ 0.0092595 ]\n",
            " [-0.00083462]\n",
            " [-0.00137034]\n",
            " [-0.0004285 ]\n",
            " [-0.00212418]\n",
            " [-0.00141557]\n",
            " [-0.00083008]]\n",
            "  Gradient Wh (dWh): [[-0.00010029]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.         0.         0.         0.\n",
            "  0.00999375 0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.00999375]]\n",
            "  Gradient by (dby): [[ 0.08459135  0.14022831 -0.92266363  0.08316622  0.13654807  0.04269796\n",
            "   0.21166355  0.14105483  0.08271333]]\n",
            "\n",
            "Time Step 13/13\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-0.0193181]]\n",
            "  Raw Prediction (y_pred): [[-0.1858944   0.31332258 -0.1749711  -0.20307222  0.2873528  -0.86575615\n",
            "   0.71855306  0.319645   -0.2084537 ]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.08432125 0.13891342 0.08524736 0.08288516 0.13535231 0.04272448\n",
            "  0.20832124 0.13979447 0.08244031]]\n",
            "Backward Propagation:\n",
            "  Gradient of Loss wrt Output (dy): [[ 0.08432125  0.13891342  0.08524736  0.08288516  0.13535231  0.04272448\n",
            "  -0.79167876  0.13979447  0.08244031]]\n",
            "  Gradient Wy (dWy): [[-0.00162893]\n",
            " [-0.00268354]\n",
            " [-0.00164682]\n",
            " [-0.00160118]\n",
            " [-0.00261475]\n",
            " [-0.00082536]\n",
            " [ 0.01529373]\n",
            " [-0.00270056]\n",
            " [-0.00159259]]\n",
            "  Gradient Wh (dWh): [[-0.00049096]]\n",
            "  Gradient Wx (dWx): [[0.         0.         0.02541463 0.         0.         0.\n",
            "  0.         0.         0.        ]]\n",
            "  Gradient bh (dbh): [[0.02541463]]\n",
            "  Gradient by (dby): [[ 0.08432125  0.13891342  0.08524736  0.08288516  0.13535231  0.04272448\n",
            "  -0.79167876  0.13979447  0.08244031]]\n",
            "\n",
            "Epoch 10 Loss: [27.06459438]\n",
            "\n",
            "Testing the RNN Model:\n",
            "Input: p, Predicted: s\n",
            "Input: h, Predicted: s\n",
            "Input: o, Predicted: s\n",
            "Input: t, Predicted: s\n",
            "Input: o, Predicted: s\n",
            "Input: s, Predicted: s\n",
            "Input: y, Predicted: s\n",
            "Input: n, Predicted: s\n",
            "Input: t, Predicted: s\n",
            "Input: h, Predicted: s\n",
            "Input: e, Predicted: s\n",
            "Input: s, Predicted: s\n",
            "Input: i, Predicted: s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mutliple Neurons - A sentence"
      ],
      "metadata": {
        "id": "rJ93egf4XcIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the data\n",
        "sentence = \"I am CS student\"\n",
        "words = sentence.split()\n",
        "word_to_int = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "print(\"Encoded Sentence using One-Hot Encoding:\", encoded_sentence, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = len(word_to_int)  # Number of unique words\n",
        "output_size = len(word_to_int)\n",
        "hidden_size = 4  # Number of hidden neurons\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # Fewer epochs for demonstration\n",
        "\n",
        "# Initialize weights and biases\n",
        "Wx = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden weights\n",
        "Wh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden weights\n",
        "Wy = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output weights\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "# RNN step function\n",
        "def rnn_step_forward(x, h_prev):\n",
        "    h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + bh)\n",
        "    y = np.dot(Wy, h_next) + by\n",
        "    return h_next, y\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    loss = 0\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
        "\n",
        "    for t in range(len(encoded_sentence) - 1):\n",
        "        print(f\"\\nTime Step {t + 1}/{len(encoded_sentence) - 1}\")\n",
        "\n",
        "        # Input preparation\n",
        "        x_t = np.zeros((input_size, 1))\n",
        "        x_t[encoded_sentence[t]] = 1  # One-hot encoding\n",
        "        y_true = encoded_sentence[t + 1]  # Target word index\n",
        "\n",
        "        # Forward pass\n",
        "        h_prev, y_pred = rnn_step_forward(x_t, h_prev)\n",
        "        y_pred_softmax = np.exp(y_pred) / np.sum(np.exp(y_pred))  # Softmax activation\n",
        "\n",
        "        # Print forward propagation details\n",
        "        print(\"Forward Propagation:\")\n",
        "        print(f\"  Input (x_t): {x_t.T}\")\n",
        "        print(f\"  Hidden State (h_prev): {h_prev.T}\")\n",
        "        print(f\"  Raw Prediction (y_pred): {y_pred.T}\")\n",
        "        print(f\"  Softmax Prediction (y_pred_softmax): {y_pred_softmax.T}\")\n",
        "\n",
        "        # Loss calculation\n",
        "        loss += -np.log(y_pred_softmax[y_true])  # Cross-entropy loss\n",
        "\n",
        "        # Backward pass\n",
        "        dy = y_pred_softmax\n",
        "        dy[y_true] -= 1  # Gradient of softmax + loss\n",
        "\n",
        "        dWy = np.dot(dy, h_prev.T)\n",
        "        dby = dy\n",
        "        dh = np.dot(Wy.T, dy) * (1 - h_prev**2)  # Backprop\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01hUIQN7XkNi",
        "outputId": "d35d06aa-4aa3-4f5d-c846-42b3f4b6f7ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Sentence using One-Hot Encoding: [1, 2, 0, 3] \n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 2/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 3/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 4/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 5/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 6/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 7/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 8/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 9/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n",
            "\n",
            "Epoch 10/10\n",
            "\n",
            "Time Step 1/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 1. 0. 0.]]\n",
            "  Hidden State (h_prev): [[-4.12703871e-05  1.81937510e-03 -5.66316777e-03  1.05266405e-02]]\n",
            "  Raw Prediction (y_pred): [[-5.72544663e-05 -6.21467669e-05 -1.93818621e-05  9.38987306e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24998849 0.24998727 0.24999796 0.25002628]]\n",
            "\n",
            "Time Step 2/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[0. 0. 1. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00895886  0.0095831  -0.00256969 -0.00811158]]\n",
            "  Raw Prediction (y_pred): [[-1.56293039e-04  1.44680775e-04  2.75905553e-04 -4.93226762e-05]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.24994749 0.25002273 0.25005554 0.24997423]]\n",
            "\n",
            "Time Step 3/3\n",
            "Forward Propagation:\n",
            "  Input (x_t): [[1. 0. 0. 0.]]\n",
            "  Hidden State (h_prev): [[ 0.00677048 -0.01419261  0.00273955 -0.01016803]]\n",
            "  Raw Prediction (y_pred): [[ 0.00024798  0.00023421 -0.00016731 -0.00022872]]\n",
            "  Softmax Prediction (y_pred_softmax): [[0.25005661 0.25005317 0.24995279 0.24993744]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question#4 - Self Attention**"
      ],
      "metadata": {
        "id": "gBjzHN4_Yh9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a user example: sequence of words\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "words = sentence.split()\n",
        "vocab = sorted(set(words))\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "encoded_sentence = [word_to_int[word] for word in words]\n",
        "\n",
        "print(\"Input Sentence:\", sentence)\n",
        "print(\"Encoded Sentence:\", encoded_sentence, \"\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_size = 8  # Size of embedding vectors\n",
        "sequence_length = len(words)  # Number of tokens\n",
        "d_k = embedding_size  # Dimension of the key vectors (commonly the same as embedding size)\n",
        "\n",
        "# Random embeddings for demonstration\n",
        "np.random.seed(42)  # Seed for reproducibility\n",
        "word_embeddings = np.random.randn(len(vocab), embedding_size)  # Embedding matrix\n",
        "sequence_embeddings = np.array([word_embeddings[i] for i in encoded_sentence])  # Input embeddings\n",
        "\n",
        "# Initialize Query, Key, and Value weights\n",
        "Wq = np.random.randn(embedding_size, d_k)\n",
        "Wk = np.random.randn(embedding_size, d_k)\n",
        "Wv = np.random.randn(embedding_size, embedding_size)\n",
        "\n",
        "# Compute Query, Key, and Value matrices\n",
        "queries = np.dot(sequence_embeddings, Wq)  # (sequence_length x d_k)\n",
        "keys = np.dot(sequence_embeddings, Wk)     # (sequence_length x d_k)\n",
        "values = np.dot(sequence_embeddings, Wv)   # (sequence_length x embedding_size)\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Scale by sqrt(d_k)\n",
        "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # Softmax\n",
        "    output = np.dot(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Calculate self-attention\n",
        "attention_output, attention_weights = scaled_dot_product_attention(queries, keys, values)\n",
        "\n",
        "# Print results\n",
        "print(\"Self-Attention Mechanism:\\n\")\n",
        "print(\"Sequence Embeddings (Input):\")\n",
        "print(sequence_embeddings)\n",
        "print(\"\\nQuery Matrix:\")\n",
        "print(queries)\n",
        "print(\"\\nKey Matrix:\")\n",
        "print(keys)\n",
        "print(\"\\nValue Matrix:\")\n",
        "print(values)\n",
        "print(\"\\nAttention Weights (Softmax of Scores):\")\n",
        "print(attention_weights)\n",
        "print(\"\\nSelf-Attention Output:\")\n",
        "print(attention_output)\n",
        "\n",
        "# Reconstruct the sequence using the attention output\n",
        "reconstructed_sequence = [int_to_word[np.argmax(row)] for row in attention_output]\n",
        "print(\"\\nReconstructed Sequence (Using Attention Outputs):\", \" \".join(reconstructed_sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idwqEU3MY_My",
        "outputId": "480dcc06-03ac-4807-c6e2-41cb988e756c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence: The quick brown fox jumps over the lazy dog\n",
            "Encoded Sentence: [0, 7, 1, 3, 4, 6, 8, 5, 2] \n",
            "\n",
            "Self-Attention Mechanism:\n",
            "\n",
            "Sequence Embeddings (Input):\n",
            "[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
            "   1.57921282  0.76743473]\n",
            " [-0.83921752 -0.30921238  0.33126343  0.97554513 -0.47917424 -0.18565898\n",
            "  -1.10633497 -1.19620662]\n",
            " [-0.46947439  0.54256004 -0.46341769 -0.46572975  0.24196227 -1.91328024\n",
            "  -1.72491783 -0.56228753]\n",
            " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
            "  -0.60170661  1.85227818]\n",
            " [-0.01349722 -1.05771093  0.82254491 -1.22084365  0.2088636  -1.95967012\n",
            "  -1.32818605  0.19686124]\n",
            " [ 0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
            "   1.03099952  0.93128012]\n",
            " [ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
            "   0.36139561  1.53803657]\n",
            " [ 0.73846658  0.17136828 -0.11564828 -0.3011037  -1.47852199 -0.71984421\n",
            "  -0.46063877  1.05712223]\n",
            " [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877 -0.2257763\n",
            "   0.0675282  -1.42474819]]\n",
            "\n",
            "Query Matrix:\n",
            "[[ 1.46980507 -1.03600977 -0.80628069 -1.52267899 -2.39599583  3.18168486\n",
            "  -2.945073   -3.07718038]\n",
            " [-0.71686864 -0.04138778  1.21355483  0.89179012 -0.93187049 -2.85006322\n",
            "   2.07865128 -1.05554308]\n",
            " [-1.56953771 -3.66363421  1.26102058  0.83813456  0.21877208 -1.61076114\n",
            "   0.36966748  0.82357732]\n",
            " [ 0.50405745 -2.65143613 -3.3128751  -0.93027239 -2.32612782  0.27104413\n",
            "   0.08625936  5.5097923 ]\n",
            " [-1.74717337 -3.92939194 -2.85173523  2.94118094  1.1376885   0.2373488\n",
            "  -2.37929471 -0.96705638]\n",
            " [ 1.14363855 -0.19940866 -4.86857184  0.28522696  1.19300085  2.87261379\n",
            "  -1.45705061 -0.09046757]\n",
            " [ 0.35023764 -0.56531856 -1.19789526 -0.87282093 -3.7018412   0.92504787\n",
            "  -2.20286448 -0.18428634]\n",
            " [-0.15592639 -1.40521322 -6.62981621  0.71978753 -1.31602506  0.30073159\n",
            "   1.87179909  0.39666144]\n",
            " [-0.30847021 -1.61527496  8.13436281 -1.54812011  4.23264371  0.66468802\n",
            "  -0.79242298  2.12321079]]\n",
            "\n",
            "Key Matrix:\n",
            "[[-2.4840771   1.08881005  3.68000371 -2.40659043  0.57249945 -0.60969403\n",
            "  -0.82718866  3.5265519 ]\n",
            " [-0.49285828  1.66485063  0.67175784 -0.09044129  3.12049862  0.85855073\n",
            "   2.16949726  1.73964977]\n",
            " [ 1.45861489 -3.39333626 -2.35099266 -4.91818497 -0.3950089   3.38471286\n",
            "   3.91313062 -2.64386081]\n",
            " [ 1.54635178 -2.41790713  0.1653916   1.49156547 -3.11530758  0.17241238\n",
            "   3.10557165 -3.37512239]\n",
            " [ 0.62848878 -5.79190232 -0.13764482 -3.5008296  -0.48808469  3.3473029\n",
            "   0.53414062 -4.27975873]\n",
            " [-0.42932364 -0.74155865  3.19859393  4.48944567 -1.86962107 -2.25371257\n",
            "  -2.94511848 -1.51130177]\n",
            " [-1.06170996 -1.29644     0.41142576 -5.04665541 -0.3718256   2.21030604\n",
            "   0.66343098  1.01555779]\n",
            " [ 0.5209645  -1.9454029   1.47679056 -1.11654174 -1.12909971  1.81000996\n",
            "  -0.62490111 -3.6244079 ]\n",
            " [ 2.18628811 -0.36042156 -4.57381451  0.27566936 -2.17328067 -1.8715916\n",
            "   2.16178796 -0.56344152]]\n",
            "\n",
            "Value Matrix:\n",
            "[[ 2.82237615  1.5268746  -2.0282805   0.47086253  2.988742    0.88920701\n",
            "  -4.9891794  -2.16208922]\n",
            " [-4.51928965  0.05394407 -1.44053505 -4.6663449   0.88435252 -0.46824618\n",
            "   5.14353231  0.4850072 ]\n",
            " [-1.54088502  1.19086724  1.47420499 -2.93647597 -0.90798011  0.28018068\n",
            "   4.80116867  3.6846499 ]\n",
            " [ 2.06275769 -0.32586978  0.20225232 -0.69759245 -3.50444614  2.24140508\n",
            "  -5.93310585 -0.18547712]\n",
            " [-1.14197762 -6.19533136  0.12407822 -2.46300856  0.71483276 -3.73048628\n",
            "   3.95509461 -0.50065526]\n",
            " [ 1.63058178 -6.9665107  -2.84544304  0.56977706  0.83163954 -1.65931963\n",
            "  -3.19106746 -4.19745051]\n",
            " [ 3.68562612  5.68704534  2.4302126   2.5027518  -1.32679083  1.50269471\n",
            "  -6.25867648  0.75936445]\n",
            " [ 1.61310894  0.88521163 -1.54899105  0.12344986  0.9946352  -1.14301517\n",
            "  -0.9393221  -1.01908378]\n",
            " [-0.27235748 -0.57367539  1.74317039  0.42924295 -0.62960206  2.39360784\n",
            "   3.44031677  3.70047432]]\n",
            "\n",
            "Attention Weights (Softmax of Scores):\n",
            "[[1.15964209e-08 3.32695667e-09 1.20280984e-02 1.89168027e-04\n",
            "  9.68090580e-01 5.01031560e-06 1.05707809e-04 1.95761948e-02\n",
            "  5.22552918e-06]\n",
            " [5.40837451e-03 3.20468512e-03 5.79836393e-04 5.62082516e-01\n",
            "  4.92932794e-04 3.82398339e-01 2.54019215e-04 3.94284127e-03\n",
            "  4.16364554e-02]\n",
            " [8.32075061e-02 3.15082565e-03 2.94263609e-03 5.96752818e-02\n",
            "  1.72385030e-01 6.54884534e-01 9.64779684e-03 1.24111883e-02\n",
            "  1.69520148e-03]\n",
            " [4.40339743e-03 2.76551772e-04 3.58647945e-02 1.50421093e-04\n",
            "  5.51479622e-04 2.09681171e-06 1.30252079e-01 5.12245540e-06\n",
            "  8.28494057e-01]\n",
            " [4.55829037e-06 6.44831097e-05 9.39555569e-04 9.22651509e-03\n",
            "  5.93532845e-01 3.73501272e-01 5.26487060e-05 1.02210211e-02\n",
            "  1.24571016e-02]\n",
            " [9.26389705e-07 1.31164921e-03 6.37765724e-01 2.92036870e-04\n",
            "  7.67274349e-02 2.61138152e-06 2.45275923e-03 1.26054050e-03\n",
            "  2.80186317e-01]\n",
            " [1.30150671e-03 1.62797282e-05 7.07867164e-02 6.79100939e-02\n",
            "  4.50038355e-01 3.64503846e-02 7.19723079e-02 1.64660627e-01\n",
            "  1.36863729e-01]\n",
            " [9.73771495e-11 2.75537924e-07 1.22904430e-02 1.40766503e-04\n",
            "  2.99751465e-05 1.22584748e-09 1.26227184e-06 1.08337824e-07\n",
            "  9.87537168e-01]\n",
            " [9.99726034e-01 2.05812837e-04 1.51212343e-09 2.07440631e-10\n",
            "  1.13464080e-06 9.47023782e-06 5.53748802e-05 2.17165814e-06\n",
            "  3.81181616e-15]]\n",
            "\n",
            "Self-Attention Output:\n",
            "[[-1.09170658 -5.96548735  0.10781746 -2.41718125  0.69977049 -3.62986743\n",
            "   3.86646841 -0.46026663]\n",
            " [ 1.77825344 -2.86003136 -0.92199107 -0.17055241 -1.65557587  0.72250203\n",
            "  -4.42297125 -1.5673515 ]\n",
            " [ 1.26526479 -5.45408569 -1.99177053 -0.05083531  0.70600257 -1.5182793\n",
            "  -2.21305261 -3.01279939]\n",
            " [ 0.2100208   0.3114357   1.80437386  0.57561925 -0.71372425  2.19092417\n",
            "   2.18799961  3.28717457]\n",
            " [-0.03818221 -6.27880972 -0.97997252 -1.25181743  0.70403219 -2.79479292\n",
            "   1.13861483 -1.82741531]\n",
            " [-1.14090061  0.13843004  1.44029855 -1.9415242  -0.70249624  0.56539697\n",
            "   4.31788056  3.34949866]\n",
            " [ 0.07371194 -2.50135895  0.22597527 -1.08317598  0.03577518 -1.31860706\n",
            "   1.45989393  0.26042062]\n",
            " [-0.28764158 -0.55211378  1.73959888  0.38763263 -0.63338812  2.36742559\n",
            "   3.45572595  3.69960177]\n",
            " [ 2.82089453  1.52671123 -2.0279169   0.46991459  2.98804258  0.88892781\n",
            "  -4.98712827 -2.16139753]]\n",
            "\n",
            "Reconstructed Sequence (Using Attention Outputs): over The The quick over over over quick jumps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3cca74d"
      },
      "source": [
        "# **Question#5 - Encoding Techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "284a8cee"
      },
      "source": [
        "## Label Encoding\n",
        "Label encoding is a preprocessing technique used in machine learning to convert categorical data into numerical form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30790fc2",
        "outputId": "15bf13ed-960c-4590-82da-86b389df5862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded labels: [0 1 0 2]\n",
            "Original categories: ['Apple' 'Banana' 'Apple' 'Cherry']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample data\n",
        "categories = ['Apple', 'Banana', 'Apple', 'Cherry']\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "labels = encoder.fit_transform(categories)\n",
        "\n",
        "print(\"Encoded labels:\", labels)\n",
        "print(\"Original categories:\", encoder.inverse_transform(labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c87e897d"
      },
      "source": [
        "Here are the **disadvantages of Label Encoding**:\n",
        "\n",
        "### 1. **Introduces Ordinal Relationships**\n",
        "   - Label encoding assigns integer values to categories, which can create a false sense of order (e.g., `0 < 1 < 2`). This can mislead machine learning models into treating categories as ordinal when they are not.\n",
        "   - Example: For `['Red', 'Green', 'Blue']`, assigning `[0, 1, 2]` may cause the model to think `Red < Green < Blue`.\n",
        "\n",
        "### 2. **Model Bias**\n",
        "   - Some models, like linear regression or decision trees, may interpret the numeric labels as having meaning or weight, leading to biased predictions or poor performance.\n",
        "   - Example: If `['Dog', 'Cat', 'Elephant']` is encoded as `[0, 1, 2]`, a model might mistakenly assume `Elephant` is \"twice\" as important as `Cat`.\n",
        "\n",
        "### 3. **Not Suitable for High Cardinality**\n",
        "   - If the categorical variable has many unique values (e.g., a column of user IDs or product codes), label encoding can produce very large integers, making training difficult and leading to poor generalization.\n",
        "\n",
        "### 4. **Loss of Interpretability**\n",
        "   - Once categories are converted to integers, it can be challenging to interpret or explain the relationships inferred by the model between encoded values.\n",
        "\n",
        "### 5. **Inconsistent Behavior Across Datasets**\n",
        "   - When using label encoding for training and testing datasets separately, if a new category appears in the test set that wasn’t present during training, it can cause errors or unexpected behavior.\n",
        "\n",
        "### 6. **Risk of Data Leakage**\n",
        "   - If labels are based on the order in which data is encountered, the encoding may inadvertently include information about the dataset's structure or sequence, leading to data leakage and unreliable results.\n",
        "\n",
        "---\n",
        "\n",
        "### When to Avoid Label Encoding:\n",
        "- When categories do **not have an ordinal relationship** (use one-hot encoding instead).\n",
        "- When there are **many unique categories** (e.g., thousands of product IDs).\n",
        "- When the encoded values can mislead models due to the implied ordering.\n",
        "\n",
        "### Mitigation Strategies:\n",
        "- Use **One-Hot Encoding** for non-ordinal data.\n",
        "- For ordinal data, use **explicit ordinal encoding** that reflects the true order (e.g., `{'Low': 0, 'Medium': 1, 'High': 2}`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe29356a"
      },
      "source": [
        "## One Hot Encoding\n",
        "One-Hot Encoding is a method used to convert categorical data into a binary format that can be fed into machine learning models. It creates new binary columns (features), each representing one unique category.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2391d39b",
        "outputId": "215ca556-f14e-4cdd-e1ef-617034b6831b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Category_Apple  Category_Banana  Category_Cherry\n",
            "0               1                0                0\n",
            "1               0                1                0\n",
            "2               1                0                0\n",
            "3               0                0                1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'Category': ['Apple', 'Banana', 'Apple', 'Cherry']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# One-hot encoding\n",
        "encoded_df = pd.get_dummies(df, columns=['Category']).astype(int)\n",
        "print(encoded_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b519e74c"
      },
      "source": [
        "Here are the main **disadvantages of One-Hot Encoding**:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. High Dimensionality**\n",
        "   - **Description**: For categorical features with many unique values (high cardinality), one-hot encoding creates a separate column for each category, significantly increasing the number of features.\n",
        "   - **Impact**: This increases memory usage, computational cost, and can make training models slow.\n",
        "   - **Example**: A feature with 10,000 unique categories results in 10,000 new columns.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Sparse Representation**\n",
        "   - **Description**: The resulting matrix from one-hot encoding contains mostly `0`s, especially when the number of categories is large. This is inefficient for computation and storage.\n",
        "   - **Impact**: Models may struggle with sparse data, and additional resources may be required for processing.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Overfitting Risk**\n",
        "   - **Description**: When there are many categories but limited data for each, one-hot encoding can lead to overfitting. Models might learn noise or irrelevant patterns associated with specific categories.\n",
        "   - **Impact**: Poor generalization to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Not Suitable for Ordinal Data**\n",
        "   - **Description**: One-hot encoding ignores any natural order among categories, treating them as independent. This is unsuitable for ordinal data (e.g., `Low`, `Medium`, `High`), where the relationship matters.\n",
        "   - **Impact**: Loss of valuable information about the inherent order.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Inefficiency in High-Cardinality Features**\n",
        "   - **Description**: For features like postal codes or user IDs, one-hot encoding produces excessive columns, most of which contribute little useful information to the model.\n",
        "   - **Impact**: Increased computational complexity without significant performance gains.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Hard to Interpret**\n",
        "   - **Description**: One-hot encoding transforms a single feature into multiple binary columns, making the dataset harder to interpret for humans.\n",
        "   - **Impact**: Reduced clarity when analyzing or visualizing data.\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Avoid One-Hot Encoding:**\n",
        "- **High Cardinality**: For features with thousands of unique values.\n",
        "- **Ordinal Features**: Where the order of categories is meaningful.\n",
        "- **Limited Data**: When the dataset is small compared to the number of categories.\n",
        "\n",
        "---\n",
        "\n",
        "### **Alternatives to One-Hot Encoding:**\n",
        "1. **Label Encoding**: Suitable for ordinal features.\n",
        "2. **Binary Encoding**: Encodes categories as binary numbers to reduce dimensionality.\n",
        "3. **Embedding Layers**: Common in deep learning for representing categories in dense, lower-dimensional spaces.\n",
        "4. **Hash Encoding**: Reduces dimensionality by mapping categories to a fixed number of hash buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5a31997",
        "outputId": "10185997-0451-44c7-9a6d-a1d51a558e5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting category-encoders\n",
            "  Obtaining dependency information for category-encoders from https://files.pythonhosted.org/packages/98/47/598b4bf0ccf6f02915e71bdd23fe846a27adc2d3ba734f2ba5215d8e44f5/category_encoders-2.6.4-py2.py3-none-any.whl.metadata\n",
            "  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from category-encoders) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from category-encoders) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from category-encoders) (1.11.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from category-encoders) (0.14.0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from category-encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from category-encoders) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2023.3)\n",
            "Requirement already satisfied: six in c:\\users\\htc\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\htc\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category-encoders) (23.1)\n",
            "Downloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
            "   --------- ------------------------------ 20.5/82.0 kB ? eta -:--:--\n",
            "   --------- ------------------------------ 20.5/82.0 kB ? eta -:--:--\n",
            "   --------- ------------------------------ 20.5/82.0 kB ? eta -:--:--\n",
            "   --------- ------------------------------ 20.5/82.0 kB ? eta -:--:--\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   -------------- ------------------------- 30.7/82.0 kB 108.9 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ------------------- -------------------- 41.0/82.0 kB 49.2 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 61.4/82.0 kB 42.0 kB/s eta 0:00:01\n",
            "   ---------------------------------------  81.9/82.0 kB 43.3 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 82.0/82.0 kB 42.5 kB/s eta 0:00:00\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.6.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install category-encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d842a11"
      },
      "source": [
        "## Binary Encoding\n",
        "Binary Encoding is a data preprocessing technique that encodes categorical data into binary format. Each category is first assigned a unique integer value, which is then converted into its binary representation. The binary digits are then split into separate columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18238ea5",
        "outputId": "e2132e7c-4721-4b2f-8fee-613a884eeebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Category_0  Category_1\n",
            "0           0           1\n",
            "1           1           0\n",
            "2           1           1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from category_encoders import BinaryEncoder\n",
        "\n",
        "# Sample data\n",
        "data = {'Category': ['Apple', 'Banana', 'Cherry']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Binary Encoding\n",
        "encoder = BinaryEncoder(cols=['Category'])\n",
        "binary_encoded = encoder.fit_transform(df)\n",
        "\n",
        "print(binary_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c286372e"
      },
      "source": [
        "### **Advantages of Binary Encoding**\n",
        "1. **Reduced Dimensionality**: Compared to one-hot encoding, binary encoding requires fewer columns, making it more efficient for features with high cardinality.\n",
        "   - Example: One-hot encoding for 10 categories creates 10 columns, but binary encoding creates only 4 columns (`log2(10) ≈ 4`).\n",
        "   \n",
        "2. **Retains Some Ordinal Information**: Unlike one-hot encoding, binary encoding preserves the numeric order of categories to some extent.\n",
        "\n",
        "3. **Efficient for Large Data**: Handles high-cardinality features well without creating sparse matrices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Binary Encoding**\n",
        "1. **Not Fully Interpretable**: The binary representation may not be as intuitive to humans compared to one-hot encoding.\n",
        "2. **Loss of Ordinality**: While some order information is retained, it is not explicit or complete.\n",
        "3. **Risk of Overfitting**: If categories are highly specific and unique (e.g., user IDs), binary encoding can lead to overfitting in some models.\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Binary Encoding**\n",
        "- **High-Cardinality Features**: For categorical variables with many unique values.\n",
        "- **Efficiency Needs**: When one-hot encoding leads to too many columns, causing memory or computational inefficiencies.\n",
        "\n",
        "---\n",
        "\n",
        "### **Alternatives to Binary Encoding**\n",
        "1. **One-Hot Encoding**: For low cardinality and non-ordinal data.\n",
        "2. **Hash Encoding**: Uses a hash function to map categories to a fixed number of columns.\n",
        "3. **Embedding Layers**: Common in deep learning for dense, low-dimensional representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143189dc"
      },
      "source": [
        "## Count Vectorizer & BoW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc60183"
      },
      "source": [
        "CountVectorizer is a tool provided by the scikit-learn library to convert a collection of text documents into a matrix of token counts. It is used to implement the Bag of Words (BoW) model, where each document is represented as a vector of word counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9865189"
      },
      "source": [
        "The Bag of Words (BoW) model is a simple and commonly used technique in Natural Language Processing (NLP) to represent text data in numerical form. It disregards grammar, word order, and context, focusing only on word occurrences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6c985d8"
      },
      "source": [
        "### **How CountVectorizer Works**\n",
        "1. **Tokenization**: Splits the text into individual words (tokens).\n",
        "2. **Vocabulary Creation**: Builds a vocabulary of unique words from the corpus.\n",
        "3. **Vectorization**:\n",
        "   - Counts the occurrences of each word in the vocabulary for each document.\n",
        "   - Represents documents as vectors based on these counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10e962a0"
      },
      "source": [
        "### **How Bag of Words Works**\n",
        "\n",
        "1. **Text Preprocessing**:\n",
        "   - Tokenize the text into words (e.g., split sentences into words).\n",
        "   - Convert all words to lowercase to ensure consistency.\n",
        "   - Optionally, remove stop words (e.g., \"the,\" \"is,\" \"and\") and perform stemming or lemmatization.\n",
        "\n",
        "2. **Vocabulary Creation**:\n",
        "   - Create a vocabulary of unique words from the corpus (all text data).\n",
        "   - Assign an index to each unique word.\n",
        "\n",
        "3. **Encoding**:\n",
        "   - Represent each document (text) as a vector, where:\n",
        "     - Each element corresponds to a word in the vocabulary.\n",
        "     - The value is the frequency (or presence/absence) of the word in the document.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "#### Input:\n",
        "```text\n",
        "Document 1: \"I like apples\"\n",
        "Document 2: \"I like bananas\"\n",
        "Document 3: \"I eat apples and bananas\"\n",
        "```\n",
        "\n",
        "#### Step 1: Create Vocabulary\n",
        "Vocabulary: `['i', 'like', 'apples', 'bananas', 'eat', 'and']`\n",
        "\n",
        "#### Step 2: Encode Documents as Vectors\n",
        "Each document is converted into a vector based on word counts:\n",
        "\n",
        "| Word       | `I` | `like` | `apples` | `bananas` | `eat` | `and` |\n",
        "|------------|-----|--------|----------|-----------|-------|-------|\n",
        "| Document 1 | 1   | 1      | 1        | 0         | 0     | 0     |\n",
        "| Document 2 | 1   | 1      | 0        | 1         | 0     | 0     |\n",
        "| Document 3 | 1   | 0      | 1        | 1         | 1     | 1     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f1c2a6d",
        "outputId": "42d499e7-6ced-43d7-93b3-11ef6ac86a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['and' 'apples' 'bananas' 'eat' 'like']\n",
            "Bag of Words Matrix:\n",
            " [[0 1 0 0 1]\n",
            " [0 0 1 0 1]\n",
            " [1 1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"I like apples\",\n",
        "    \"I like bananas\",\n",
        "    \"I eat apples and bananas\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Display results\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4644840",
        "outputId": "b31bd504-9096-4ff6-8692-b34e2b54889f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['apples' 'bananas' 'eat' 'like']\n",
            "Bag of Words Matrix:\n",
            " [[1 0 0 1]\n",
            " [0 1 0 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')  # Remove common stop words\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f047ab4"
      },
      "source": [
        "### **Advantages of CoutnVectorizer**\n",
        "1. **Automatic Text Processing**: Automates tokenization, vocabulary creation, and counting.\n",
        "2. **Flexible**: Allows customization with parameters like stop word removal and n-grams.\n",
        "3. **Integration**: Easy to integrate with machine learning models in scikit-learn.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of CoutnVectorizer**\n",
        "1. **No Context**: Ignores word order and semantic meaning.\n",
        "2. **High Dimensionality**: For large vocabularies, results in sparse and high-dimensional matrices.\n",
        "3. **Sensitive to Rare Words**: Rare words may disproportionately affect the representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "470f9f0c"
      },
      "source": [
        "### **Advantages of Bag of Words**\n",
        "1. **Simple and Intuitive**: Easy to implement and understand.\n",
        "2. **Works Well for Small Datasets**: Effective for small-scale text classification or clustering tasks.\n",
        "3. **Foundation for Other Models**: Basis for more advanced techniques like TF-IDF and word embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Bag of Words**\n",
        "1. **High Dimensionality**:\n",
        "   - For a large vocabulary, the resulting vectors are high-dimensional and sparse.\n",
        "   - This increases memory and computational requirements.\n",
        "\n",
        "2. **No Context or Order Information**:\n",
        "   - Ignores the sequence of words.\n",
        "   - Loses the semantic meaning of words and phrases.\n",
        "\n",
        "3. **Ignores Synonyms and Polysemy**:\n",
        "   - Treats synonyms as separate words (e.g., \"happy\" and \"joyful\").\n",
        "   - The same word used in different contexts has the same representation (e.g., \"bank\" in \"river bank\" vs. \"money bank\").\n",
        "\n",
        "4. **Sparse Representation**:\n",
        "   - Vectors are filled mostly with zeros, making them inefficient to process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88bcd7ab"
      },
      "source": [
        "## TF-IDF (Term Frequency - Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72411fc0"
      },
      "source": [
        "**TF-IDF** is a statistical method used in Natural Language Processing (NLP) to evaluate the importance of a word in a document relative to a collection of documents (corpus). Unlike **Bag of Words (BoW)**, which counts word occurrences, TF-IDF assigns weights to words based on their frequency in a single document and their rarity across all documents, helping to identify key terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Components**\n",
        "1. **Term Frequency (TF)**:\n",
        "   - Measures how often a word occurs in a document.\n",
        "   - Formula:\n",
        "     \\[\n",
        "     TF = Number of occurrences of the word in the document \\ Total words in the document\n",
        "     \\]\n",
        "\n",
        "2. **Inverse Document Frequency (IDF)**:\n",
        "   - Measures how unique or rare a word is across all documents.\n",
        "   - Formula:\n",
        "     \\[\n",
        "     IDF = log(Total number of documents \\ Number of documents containing the word)\n",
        "     \\]\n",
        "   - A word appearing in many documents has a low IDF value, while a rare word has a high IDF value.\n",
        "\n",
        "3. **TF-IDF Score**:\n",
        "   - Combines TF and IDF to calculate the importance of a word in a document.\n",
        "   - Formula:\n",
        "     \\[\n",
        "     TF-IDF = TF* IDF\n",
        "     \\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "#### Documents:\n",
        "- Document 1: \"I love apples\"\n",
        "- Document 2: \"I love bananas\"\n",
        "- Document 3: \"Apples and bananas are great\"\n",
        "\n",
        "#### Vocabulary:\n",
        "`['i', 'love', 'apples', 'bananas', 'and', 'are', 'great']`\n",
        "\n",
        "#### Calculating TF-IDF:\n",
        "For the word **\"apples\"**:\n",
        "- **TF (Document 1)**: 1\\3 = 0.33\n",
        "- **IDF**: log(3/2) = 0.18\n",
        "- **TF-IDF (Document 1)**: 0.33 * 0.18 = 0.06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fa7f86b",
        "outputId": "86e30d49-8261-4fef-847b-48b05166f3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Matrix:\n",
            " [[0.         0.70710678 0.         0.         0.         0.70710678]\n",
            " [0.         0.         0.         0.70710678 0.         0.70710678]\n",
            " [0.49047908 0.37302199 0.49047908 0.37302199 0.49047908 0.        ]]\n",
            "Vocabulary: ['and' 'apples' 'are' 'bananas' 'great' 'love']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"I love apples\",\n",
        "    \"I love bananas\",\n",
        "    \"Apples and bananas are great\"\n",
        "]\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to array for viewing\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6066330"
      },
      "source": [
        "### **Advantages of TF-IDF**\n",
        "1. **Distinguishes Important Words**:\n",
        "   - Identifies words that are significant in a document but uncommon in the corpus.\n",
        "2. **Simple and Effective**:\n",
        "   - Works well for small to medium-sized datasets.\n",
        "3. **Reduces Noise**:\n",
        "   - Reduces the weight of common but uninformative words.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of TF-IDF**\n",
        "1. **No Contextual Understanding**:\n",
        "   - Fails to capture word meanings and relationships.\n",
        "2. **Sensitive to Data Sparsity**:\n",
        "   - High-dimensional representations can still be sparse for large corpora.\n",
        "3. **Static Weights**:\n",
        "   - Weights are fixed after computation and don't adapt to new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d6c395e"
      },
      "source": [
        "### **CBOW (Continuous Bag of Words) and Skip-Gram Models**\n",
        "\n",
        "CBOW and Skip-Gram are two key algorithms used for generating **word embeddings** in the **Word2Vec** model, developed by Google. These algorithms learn the relationships between words in a text corpus and encode semantic meaning into vector representations.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Continuous Bag of Words (CBOW)**\n",
        "\n",
        "CBOW predicts a **target word** (center word) based on the surrounding **context words**.\n",
        "\n",
        "#### **Key Characteristics:**\n",
        "- **Input**: The context words (surrounding words).\n",
        "- **Output**: The target word (center word).\n",
        "- **Objective**: Maximize the probability of predicting the correct target word given the context.\n",
        "\n",
        "#### **How It Works:**\n",
        "1. A window of size `n` is defined around the target word.\n",
        "2. The context words within this window are used as input.\n",
        "3. The algorithm learns to predict the center word from the context.\n",
        "\n",
        "#### **Example**:\n",
        "For the sentence: **\"I love playing football.\"**\n",
        "- If the window size is 2 and the target word is **\"playing\"**, the context words are **[\"I\", \"love\", \"football\"]**.\n",
        "- CBOW predicts **\"playing\"** based on these words.\n",
        "\n",
        "#### **Advantages of CBOW**:\n",
        "- Faster to train compared to Skip-Gram.\n",
        "- Performs well on frequent words.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Skip-Gram**\n",
        "\n",
        "Skip-Gram does the opposite of CBOW. It predicts the **context words** based on the **target word** (center word).\n",
        "\n",
        "#### **Key Characteristics:**\n",
        "- **Input**: The target word (center word).\n",
        "- **Output**: The context words (surrounding words).\n",
        "- **Objective**: Maximize the probability of predicting the correct context words given the center word.\n",
        "\n",
        "#### **How It Works:**\n",
        "1. A window of size `n` is defined around the target word.\n",
        "2. The center word is used as input.\n",
        "3. The algorithm learns to predict the context words.\n",
        "\n",
        "#### **Example**:\n",
        "For the sentence: **\"I love playing football.\"**\n",
        "- If the window size is 2 and the target word is **\"playing\"**, the algorithm tries to predict **[\"I\", \"love\", \"football\"]**.\n",
        "\n",
        "#### **Advantages of Skip-Gram**:\n",
        "- Handles infrequent words better than CBOW.\n",
        "- Produces more accurate word embeddings for rare words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb67579"
      },
      "source": [
        "### **GloVe, FastText, and BERT**  \n",
        "These are popular word embedding techniques, each with unique methodologies and applications. Here's an overview of their differences, functionalities, and strengths:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. GloVe (Global Vectors for Word Representation)**  \n",
        "**Approach**:  \n",
        "- **Count-based**: GloVe uses a co-occurrence matrix of word counts to learn embeddings by capturing statistical relationships between words in a corpus.\n",
        "- **Global Context**: It focuses on aggregating global word-to-word co-occurrence statistics.\n",
        "\n",
        "**Key Features**:  \n",
        "- Words with similar contexts have embeddings close in the vector space.\n",
        "- Computationally efficient after building the co-occurrence matrix.\n",
        "- Pre-trained embeddings are available (e.g., 50D, 100D, 300D trained on large corpora).\n",
        "\n",
        "**Strengths**:  \n",
        "- Efficient for capturing **semantic relationships** like analogies:\n",
        "  - Example: `king - man + woman ≈ queen`\n",
        "- Works well with static embeddings (one vector per word).\n",
        "\n",
        "**Limitations**:  \n",
        "- Cannot handle **out-of-vocabulary (OOV)** words effectively.\n",
        "- The embeddings are **static**, meaning polysemy (words with multiple meanings) cannot be captured.\n",
        "\n",
        "**Example Use Case**: Sentiment analysis, document classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. FastText**  \n",
        "**Approach**:  \n",
        "- **Subword-based**: FastText represents each word as a bag of character **n-grams** (e.g., \"playing\" includes \"pla\", \"lay\", \"ayi\", etc.).\n",
        "- Word embedding is the sum of the embeddings of its n-grams.\n",
        "\n",
        "**Key Features**:  \n",
        "- Captures **morphological features** (prefixes, suffixes, etc.) of words.\n",
        "- Handles **OOV words** by composing embeddings from subwords.\n",
        "\n",
        "**Strengths**:  \n",
        "- Works well for morphologically rich languages (e.g., Turkish, Finnish).\n",
        "- Handles **rare words** and unseen words (e.g., typos or compound words).\n",
        "- Better at capturing word similarities based on structure.\n",
        "\n",
        "**Limitations**:  \n",
        "- Still produces **static embeddings** (similar to GloVe).\n",
        "- Performance depends on the quality of the corpus.\n",
        "\n",
        "**Example Use Case**: Spell correction, machine translation, word similarity tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. BERT (Bidirectional Encoder Representations from Transformers)**  \n",
        "**Approach**:  \n",
        "- **Contextualized embeddings**: BERT generates different embeddings for the same word depending on its context in a sentence.\n",
        "- Based on the **Transformer architecture**, which uses self-attention mechanisms to understand relationships between words in both left-to-right and right-to-left directions.\n",
        "\n",
        "**Key Features**:  \n",
        "- Pre-trained on large corpora (e.g., BookCorpus, Wikipedia) using **masked language modeling (MLM)** and **next sentence prediction (NSP)** tasks.\n",
        "- Embeddings are dynamic and **context-sensitive**.\n",
        "\n",
        "**Strengths**:  \n",
        "- Handles **polysemy** by producing unique embeddings for the same word in different contexts.\n",
        "  - Example: \"bank\" in \"river bank\" vs. \"money bank.\"\n",
        "- Excels in downstream NLP tasks (e.g., question answering, text classification) via fine-tuning.\n",
        "- Pre-trained models like **BERT-base**, **BERT-large** can be fine-tuned for domain-specific tasks.\n",
        "\n",
        "**Limitations**:  \n",
        "- Computationally expensive and resource-intensive.\n",
        "- Requires fine-tuning for specific tasks.\n",
        "\n",
        "**Example Use Case**: Question answering (e.g., SQuAD), sentiment analysis, language translation, text summarization.\n"
      ]
    }
  ]
}